[["index.html", "Stat 255: Statistics for Data Science Notes Preface", " Stat 255: Statistics for Data Science Notes Andrew Sage - Lawrence University 2023-10-24 Preface These notes serve as the primary textual resource for Stat 255: Statistics for Data Science at Lawrence University. What is this course about? Stat 255 provides an introduction to essential statistical tasks including modeling, inference, prediction, and computation. The course employs a modern approach, intended to equip students with skills needed for working with today’s complex data. Traditional concepts, like interval estimation and hypothesis testing, are introduced through the lens of multivariate models and simulation. Data computation in R plays a central role throughout the course. The course’s overarching learning outcomes are: Visualize and wrangle data using statistical software R. Build and assess multivariate models to predict future outcomes. Use statistics from samples to draw inferences about larger populations or processes. Quantify uncertainty associated with estimates and predictions. Explain the assumptions associated with statistical models, and evaluate whether these assumptions are reasonably satisfied in context. Write reproducible analyses, using statistical software. Make ethical decisions based on data. More specific learning tasks, related to these outcomes are provided in each chapter. Who is this course intended for? This course is intended for students who are interested in learning statistical modeling and data computation skills that might prove useful in further courses, research, or career. Stat 255 can serve as either: a first course in statistics for students with a strong quantitative background, typically including calculus. a second course in statistics, building on introductory topics taught in courses like Lawrence’s Stat 107: Principles of Statistics or AP Statistics. At Lawrence, this course is required for the statistics track of the mathematics major, the economics and mathematics-economics majors, the business analytics track of the business and entrepreneurship major, and the statistics and data science minor. It also satisfies the statistics requirement for several other majors and minors. The prerequisite for the course is either 1) a prior college-level course in statistics (i.e. STAT 107, BIOL 170 or 280, ANTH 207, AP Stats) OR 2) Calculus. (Math 140, AP Calculus, or equivalent). The course does not assume any prior knowledge of statistics, but does move more rapidly than a typical introductory statistics course. Students engage rigorously in statistical thinking and computation, intended to equip them with essential skills for further study in statistics and data science. "],["exploratory-data-analysis.html", "Chapter 1 Exploratory Data Analysis 1.1 Getting Started in R 1.2 Data Visualization 1.3 Summary Tables", " Chapter 1 Exploratory Data Analysis Learning Outcomes: Interpret graphical summaries of data, including boxplots, histograms, violin plots, density plots, scatterplots, and correlation plots. Read data from a .csv file into R. Preview data in R. Create graphical summaries of data using R. Calculate summary statistics for entire datasets and grouped summaries. Create reproducible documents using R Markdown. 1.1 Getting Started in R This section provides examples of how to read data into R, create graphics, like those in the previous section, and calculate summary statistics. We’ll work with data on houses that sold in King County, WA, (home of Seattle) between 2014 and 2015. We begin by loading the tidyverse package which can be used to create professional data graphics and summaries. library(tidyverse) 1.1.1 Previewing the Data head() The head() function displays the first 5 rows of the dataset. head(Houses) ## # A tibble: 6 × 9 ## Id price bedrooms bathrooms sqft_living sqft_lot condition waterfront ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 1 1225 4 4.5 5420 101930 average No ## 2 2 885. 4 2.5 2830 5000 average No ## 3 3 385. 4 1.75 1620 4980 good No ## 4 4 253. 2 1.5 1070 9643 average No ## 5 5 468. 2 1 1160 6000 good No ## 6 6 310. 3 1 1430 19901 good No ## # … with 1 more variable: yr_built &lt;dbl&gt; The rows of the dataset are called observations. In this case, the observations are the houses. The columns of the dataset, which contain information about the houses, are called variables. glimpse The glimpse() command shows the number of observations (rows), and the number of variables, (columns). We also see the name of each variable and its type. Variable types include Categorical variables, which take on groups or categories, rather than numeric values. In R, these might be coded as logical &lt;logi&gt;, character &lt;chr&gt;, factor &lt;fct&gt; and ordered factor &lt;ord&gt;. Quantitative variables, which take on meaningful numeric values. These include numeric &lt;num&gt;, integer &lt;int&gt;, and double &lt;dbl&gt;. Date and time variables take on values that are dates and times, and are denoted &lt;dttm&gt; glimpse(Houses) ## Rows: 100 ## Columns: 9 ## $ Id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,… ## $ price &lt;dbl&gt; 1225.00, 885.00, 385.00, 252.70, 468.00, 310.00, 550.00, 4… ## $ bedrooms &lt;dbl&gt; 4, 4, 4, 2, 2, 3, 4, 4, 3, 3, 3, 4, 5, 3, 4, 4, 3, 4, 3, 3… ## $ bathrooms &lt;dbl&gt; 4.50, 2.50, 1.75, 1.50, 1.00, 1.00, 1.00, 1.00, 1.00, 2.25… ## $ sqft_living &lt;dbl&gt; 5420, 2830, 1620, 1070, 1160, 1430, 1660, 1600, 960, 1660,… ## $ sqft_lot &lt;dbl&gt; 101930, 5000, 4980, 9643, 6000, 19901, 34848, 4300, 6634, … ## $ condition &lt;fct&gt; average, average, good, average, good, good, poor, good, a… ## $ waterfront &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No… ## $ yr_built &lt;dbl&gt; 2001, 1995, 1947, 1985, 1942, 1927, 1933, 1916, 1952, 1979… There are 100 houses in the dataset, and 9 variables on each house. summary summary displays the mean, minimum, first quartile, median, third quartile, and maximum for each numeric variable, and the number of observations in each category, for categorical variables. summary(Houses) ## Id price bedrooms bathrooms ## Min. : 1.00 Min. : 180.0 Min. :1.00 Min. :0.750 ## 1st Qu.: 25.75 1st Qu.: 322.9 1st Qu.:3.00 1st Qu.:1.500 ## Median : 50.50 Median : 507.5 Median :3.00 Median :2.000 ## Mean : 50.50 Mean : 735.4 Mean :3.39 Mean :2.107 ## 3rd Qu.: 75.25 3rd Qu.: 733.8 3rd Qu.:4.00 3rd Qu.:2.500 ## Max. :100.00 Max. :5300.0 Max. :6.00 Max. :6.000 ## sqft_living sqft_lot condition waterfront yr_built ## Min. : 440 Min. : 1044 poor : 1 No :85 Min. :1900 ## 1st Qu.:1410 1st Qu.: 5090 fair : 1 Yes:15 1st Qu.:1948 ## Median :2000 Median : 7852 average :59 Median :1966 ## Mean :2291 Mean : 13205 good :30 Mean :1965 ## 3rd Qu.:2735 3rd Qu.: 12246 very_good: 9 3rd Qu.:1991 ## Max. :8010 Max. :101930 Max. :2014 1.1.2 Modifying the Data Next we’ll look at how to manipulate the data and create new variables. Adding a New Variable We can use the mutate() function to create a new variable based on variables already in the dataset. Let’s add a variable giving the age of the house, as of 2015. Houses &lt;- Houses %&gt;% mutate(age = 2015-yr_built) head(Houses) ## # A tibble: 6 × 10 ## Id price bedrooms bathrooms sqft_living sqft_lot condition waterfront ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 1 1225 4 4.5 5420 101930 average No ## 2 2 885. 4 2.5 2830 5000 average No ## 3 3 385. 4 1.75 1620 4980 good No ## 4 4 253. 2 1.5 1070 9643 average No ## 5 5 468. 2 1 1160 6000 good No ## 6 6 310. 3 1 1430 19901 good No ## # … with 2 more variables: yr_built &lt;dbl&gt;, age &lt;dbl&gt; Selecting Columns If the dataset contains a large number of variables, narrow down to the ones you are interested in working with. This can be done with the select() command. If there are not very many variables to begin with, or you are interested in all of them, then you may skip this step. Let’s create a smaller version of the dataset, with only the columns price, sqft_living, and waterfront. We’ll call this Houses_3var. Houses_3var &lt;- Houses %&gt;% select(price, sqft_living, waterfront) head(Houses_3var) ## # A tibble: 6 × 3 ## price sqft_living waterfront ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1225 5420 No ## 2 885. 2830 No ## 3 385. 1620 No ## 4 253. 1070 No ## 5 468. 1160 No ## 6 310. 1430 No 1.1.2.1 Filtering by Row The filter() command narrows a dataset down to rows that meet specified conditions. We’ll filter the data to include only houses built after 2000. New_Houses &lt;- Houses %&gt;% filter(yr_built&gt;=2000) head(New_Houses) ## # A tibble: 6 × 10 ## Id price bedrooms bathrooms sqft_living sqft_lot condition waterfront ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 1 1225 4 4.5 5420 101930 average No ## 2 16 3075 4 5 4550 18641 average Yes ## 3 23 862. 5 2.75 3595 5639 average No ## 4 24 360. 4 2.5 2380 5000 average No ## 5 25 625. 4 2.5 2570 5520 average No ## 6 27 488. 3 2.5 3160 13603 average No ## # … with 2 more variables: yr_built &lt;dbl&gt;, age &lt;dbl&gt; Now, we’ll filter the data to include only houses on the waterfront. New_Houses &lt;- Houses %&gt;% filter(waterfront == &quot;Yes&quot;) head(New_Houses) ## # A tibble: 6 × 10 ## Id price bedrooms bathrooms sqft_living sqft_lot condition waterfront ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 16 3075 4 5 4550 18641 average Yes ## 2 19 995. 3 4.5 4380 47044 average Yes ## 3 34 825. 2 1 1150 12775 good Yes ## 4 40 2400. 4 2.5 3650 8354 average Yes ## 5 42 290. 2 0.75 440 8313 good Yes ## 6 46 5111. 5 5.25 8010 45517 average Yes ## # … with 2 more variables: yr_built &lt;dbl&gt;, age &lt;dbl&gt; 1.2 Data Visualization 1.2.1 Histogram Next, we’ll create graphics to help us visualize the distributions and relationships between variables. We’ll use the ggplot() function, which is part of the tidyverse package. Histograms are useful for displaying the distribution of a single quantitative variable. In a histogram, the x-axis breaks the variable into ranges of values, and the y-axis displays the number of observations with a value falling in that category (frequency). General Template for Histogram ggplot(data=DatasetName, aes(x=VariableName)) + geom_histogram(fill=&quot;colorchoice&quot;, color=&quot;colorchoice&quot;) + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;x-axis label&quot;) + ylab(&quot;y-axis label&quot;) Histogram of House Prices ggplot(data=Houses, aes(x=price)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + ggtitle(&quot;Distribution of House Prices&quot;) + xlab(&quot;Price (in thousands)&quot;) + ylab(&quot;Frequency&quot;) We see that the distribution of house prices is right-skewed. Most houses cost less than $1,000,000, though there are a few houses that are much more expensive. The most common price range is around $400,000 to $500,000. 1.2.2 Density Plot Density plots show the distribution for a quantitative variable price. Scores can be compared across categories, like whether or not the house is on a waterfront. General Template for Density Plot ggplot(data=DatasetName, aes(x=QuantitativeVariable, color=CategoricalVariable, fill=CategoricalVariable)) + geom_density(alpha=0.2) + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Axis Label&quot;) + ylab(&quot;Frequency&quot;) alpha, ranging from 0 to 1 dictates transparency. Density Plot of House Prices ggplot(data=Houses, aes(x=price, color=waterfront, fill=waterfront)) + geom_density(alpha=0.2) + ggtitle(&quot;Distribution of Prices&quot;) + xlab(&quot;House price (in thousands)&quot;) + ylab(&quot;Frequency&quot;) We see that on average, houses on the waterfront tend to be more expensive and have a greater price range than houses not on the waterfront. 1.2.3 Boxplot Boxplots can be used to compare a quantitative variable with a categorical variable. The middle 50% of observations are contained in the “box”, with the upper and lower 25% of the observations in each tail. General Template for Boxplot ggplot(data=DatasetName, aes(x=CategoricalVariable, y=QuantitativeVariable)) + geom_boxplot() + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Variable Name&quot;) + ylab(&quot;Variable Name&quot;) You can make the plot horizontal by adding + coordflip(). You can turn the axis text vertical by adding theme(axis.text.x = element_text(angle = 90)). Boxplot Comparing Price by Waterfront Status ggplot(data=Houses, aes(x=waterfront, y=price)) + geom_boxplot() + ggtitle(&quot;House Price by Waterfront Status&quot;) + xlab(&quot;Waterfront&quot;) + ylab(&quot;Price (in thousands)&quot;) + coord_flip() For houses not on the waterfront, the median price is about $400,000, and the middle 50% of prices range from about $300,000 to $600,000. For waterfront houses, the median price is about $1,500,000, and the middle 50% of prices range from about $900,000 to $1,900,000. 1.2.4 Violin Plot Violin plots are an alternative to boxplots. The width of the violin tells us the density of observations in a given range. General Template for Violin Plot ggplot(data=DatasetName, aes(x=CategoricalVariable, y=QuantitativeVariable, fill=CategoricalVariable)) + geom_violin() + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Variable Name&quot;) + ylab(&quot;Variable Name&quot;) Violin Plot Comparing Prices by Waterfront ggplot(data=Houses, aes(x=waterfront, y=price, fill=waterfront)) + geom_violin() + ggtitle(&quot;Price by Waterfront Status&quot;) + xlab(&quot;Waterfront&quot;) + ylab(&quot;Price (in thousands)&quot;) + theme(axis.text.x = element_text(angle = 90)) Again, we see that houses on the waterfront tend to be more expensive than those not on the waterfront, and have a wider range in prices. 1.2.5 Scatterplot Scatterplots are used to visualize the relationship between two quantitative variables. Scatterplot Template ggplot(data=DatasetName, aes(x=CategoricalVariable, y=QuantitativeVariable)) + geom_point() + ggtitle(&quot;Plot Title&quot;) + ylab(&quot;Axis Label&quot;) + xlab(&quot;Axis Label&quot;) Scatterplot Comparing Price and Square Feet of Living Space ggplot(data=Houses, aes(x=sqft_living, y=price)) + geom_point() + ggtitle(&quot;Price and Living Space&quot;) + ylab(&quot;Price (in thousands)&quot;) + xlab(&quot;Living Space in sq. ft. &quot;) We see that there is an upward trend, indicating that houses with more living space tend to, on average, be higher priced than those with less living space. The relationship appears to be roughly linear, though there might be some curvature, as living space gets very large. There are some exceptions to this trend, most notably a house with more than 7,000 square feet, priced just over $1,000,000. We can also add color, size, and shape to the scatterplot to display information about other variables. We’ll use color to illustrate whether the house is on the waterfront, and size to represent the square footage of the entire lot (including the yard and the house). ggplot(data=Houses, aes(x=sqft_living, y=price, color=waterfront, size=sqft_lot)) + geom_point() + ggtitle(&quot;Price of King County Houses&quot;) + ylab(&quot;Price (in thousands)&quot;) + xlab(&quot;Living Space in sq. ft. &quot;) We notice that many of the largest and most expensive houses are on the waterfront. 1.2.6 Bar Graph Bar graphs can be used to visualize one or more categorical variables. A bar graph is similar to a histogram, in that the y-axis again displays frequency, but the x-axis displays categories, instead of ranges of values. Bar Graph Template ggplot(data=DatasetName, aes(x=CategoricalVariable)) + geom_bar(fill=&quot;colorchoice&quot;,color=&quot;colorchoice&quot;) + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Variable Name&quot;) + ylab(&quot;Frequency&quot;) Bar Graph by Condition ggplot(data=Houses, aes(x=condition)) + geom_bar(fill=&quot;lightblue&quot;,color=&quot;white&quot;) + ggtitle(&quot;Number of Houses by Condition&quot;) + xlab(&quot;Condition&quot;) + ylab(&quot;Frequency&quot;) + theme(axis.text.x = element_text(angle = 90)) We see that the majority of houses are in average condition. Some are in good or very good condition, while very few are in poor or very poor condition. 1.2.7 Stacked and Side-by-Side Bar Graphs Stacked Bar Graph Template ggplot(data = DatasetName, mapping = aes(x = CategoricalVariable1, fill = CategoricalVariable2)) + stat_count(position=&quot;fill&quot;) + theme_bw() + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Variable 1&quot;) + ylab(&quot;Proportion of Variable 2&quot;) + theme(axis.text.x = element_text(angle = 90)) Stacked Bar Graph Example The stat_count(position=\"fill\") command creates a stacked bar graph, comparing two categorical variables. Let’s explore whether waterfront status is related to condition. ggplot(data = Houses, mapping = aes(x = waterfront, fill = condition)) + stat_count(position=&quot;fill&quot;) + theme_bw() + ggtitle(&quot;Condition by Waterfront Status&quot;) + xlab(&quot;Waterfront Status&quot;) + ylab(&quot;Condition&quot;) + theme(axis.text.x = element_text(angle = 90)) We see that a higher proportion of waterfront houses are in good or excellent condition than non-waterfront houses. Side-by-side Bar Graph Template We can create a side-by-side bar graph, using position=dodge. ggplot(data = DatasetName, mapping = aes(x = CategoricalVariable1, fill = CategoricalVariable2)) + geom_bar(position = &quot;dodge&quot;) + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Genre&quot;) + ylab(&quot;Frequency&quot;) Side-by-side Bar Graph Example ggplot(data = Houses, mapping = aes(x = waterfront, fill = condition)) + geom_bar(position = &quot;dodge&quot;) + ggtitle(&quot;Condition by Waterfront Status&quot;) + xlab(&quot;Waterfront Status&quot;) + ylab(&quot;Condition&quot;) + theme(axis.text.x = element_text(angle = 90)) In this case, since there are so few waterfront houses, the graph is hard to read and not very useful. The stacked bar graph is a better way to convey information in this instance, though you may find that for a different dataset, the side-by-side bar graph could be a better choice. 1.2.8 Correlation Plot Correlation plots can be used to visualize relationships between quantitative variables. Correlation is a number between -1 and 1, describing the strength of the linear relationship between two variables. Variables with strong positive correlations will have correlation close to +1, while variables with strong negative correlations will have correlations close to -1. Variables with little to no relationship will have correlation close to 0. The cor() function calculates correlations between quantitative variables. We’ll use select_if to select only numeric variables. The `use=“complete.obs” command tells R to ignore observations with missing data. cor(select_if(Houses, is.numeric), use=&quot;complete.obs&quot;) %&gt;% round(2) ## Id price bedrooms bathrooms sqft_living sqft_lot yr_built age ## Id 1.00 0.03 -0.06 -0.01 -0.03 -0.07 -0.02 0.02 ## price 0.03 1.00 0.40 0.67 0.81 0.42 0.17 -0.17 ## bedrooms -0.06 0.40 1.00 0.58 0.58 0.15 0.26 -0.26 ## bathrooms -0.01 0.67 0.58 1.00 0.85 0.45 0.50 -0.50 ## sqft_living -0.03 0.81 0.58 0.85 1.00 0.54 0.36 -0.36 ## sqft_lot -0.07 0.42 0.15 0.45 0.54 1.00 0.14 -0.14 ## yr_built -0.02 0.17 0.26 0.50 0.36 0.14 1.00 -1.00 ## age 0.02 -0.17 -0.26 -0.50 -0.36 -0.14 -1.00 1.00 The corrplot() function in the corrplot() package provides a visualization of the correlations. Larger, thicker circles indicate stronger correlations. library(corrplot) Corr &lt;- cor(select_if(Houses, is.numeric), use=&quot;complete.obs&quot;) corrplot(Corr) We see that price has a strong positive correlation with square feet of living space, and is also positively correlated with number of bedrooms and bathrooms. Living space, bedrooms, and bathrooms are all positively correlated, which makes sense, since we would expect bigger houses to have more bedrooms and bathrooms. Price does not show much correlation with the other variables. We notice that bathrooms is negatively correlated with age, which means older houses tend to have fewer bathrooms than newer ones. Not surprisingly, age is very strongly correlated with year built. 1.2.9 Scatterplot Matrix A scatterplot matrix is a grid of plots. It can be created using the ggpairs() function in the GGally package. The scatterplot matrix shows us: Along the diagonal are density plots for quantitative variables, or bar graphs for categorical variables, showing the distribution of each variable. Under the diagonal are plots showing the relationships between the variables in the corresponding row and column. Scatterplots are used when both variables are quantitative, bar graphs are used when both variables are categorical, and boxplots are used when one variable is categorical, and the other is quantitative. Above the diagonal are correlations between quantitative variables. Including too many variables can make these hard to read, so it’s a good idea to use select to narrow down the number of variables. library(GGally) ggpairs(Houses %&gt;% select(price, sqft_living, condition, age)) The scatterplot matrix is useful for helping us notice key trends in our data. However, the plot can hard to read as it is quite dense, especially when there are a large number of variables. These can help us look for trends from a distance, but we should then focus in on more specific plots. 1.3 Summary Tables 1.3.1 Calculating Summary Statistics summarize() The summarize command calculates summary statistics for variables in the data. For a set of \\(n\\) values \\(y_i, \\ldots, y_n\\): mean (\\(\\bar{y}\\)) is calculated by \\(\\bar{y} =\\frac{1}{n}\\displaystyle\\sum_{i=1}^n y_i\\). standard deviation (\\(s\\)), a measure of the spread is calculated by \\(s =\\sqrt{\\displaystyle\\sum_{i=1}^n \\frac{(y_i-\\bar{y})^2}{n-1}}\\). The square of the standard deviation, called the variance is denoted \\(s^2\\). Let’s calculate the mean, median, and standard deviation, in prices. Houses_Summary &lt;- Houses %&gt;% summarize(Mean_Price = mean(price, na.rm=TRUE), Median_Price = median(price, na.rm=TRUE), StDev_Price = sd(price, na.rm = TRUE), Number_of_Houses = n()) Houses_Summary ## # A tibble: 1 × 4 ## Mean_Price Median_Price StDev_Price Number_of_Houses ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 735. 507. 835. 100 Notes: 1. The n() command calculates the number of observations. 2. The na.rm=TRUE command removes missing values, so that summary statistics can be calculated. It’s not needed here, since this dataset doesn’t include missing values, but if the dataset does include missing values, you will need to include this, in order to do the calculation. The kable() function in the knitr() package creates tables with professional appearance. library(knitr) kable(Houses_Summary) Mean_Price Median_Price StDev_Price Number_of_Houses 735.3525 507.5 835.1231 100 1.3.2 Grouped Summaries group_by() The group_by() command allows us to calculate summary statistics, with the data broken down by by category.We’ll compare waterfront houses to non-waterfront houses. Houses_Grouped_Summary &lt;- Houses %&gt;% group_by(waterfront) %&gt;% summarize(Mean_Price = mean(price, na.rm=TRUE), Median_Price = median(price, na.rm=TRUE), StDev_Price = sd(price, na.rm = TRUE), Number_of_Houses = n()) kable(Houses_Grouped_Summary) waterfront Mean_Price Median_Price StDev_Price Number_of_Houses No 523.7595 450 295.7991 85 Yes 1934.3800 1350 1610.7959 15 Note: arrange(desc(Mean_Gross)) arranges the table in descending order of Mean_Gross. To arrange in ascending order, use arrange(Mean_Gross). "],["introduction-to-statistical-models.html", "Chapter 2 Introduction to Statistical Models 2.1 Fitting Models to Data 2.2 Variability Explained by a Model 2.3 Models with Interaction 2.4 Least Squares Estimation (LSE) 2.5 ANalysis Of VAriance", " Chapter 2 Introduction to Statistical Models Learning Outcomes: Calculate sums of squares related to variability explained, including SST, SSR, and SSM., when given small datasets and/or summary statistics. Explain the meaning of SST, SSR, and SSM in a given context. Calculate \\(R^2\\) and ANOVA F-Statistics, when given small datasets and/or summary statistics. Interpret \\(R^2\\) and F-statistics in context. Explain the process for estimating least-squares regression coefficients. Calculate predictions from linear regression models. Interpret regression coefficients for models involving quantitative and/or categorical variables in context, or explain why it is inappropriate to do so. Explain the meaning of interaction between quantitative and categorical explanatory variables. Apply graphical methods, statistical summaries, and background knowledge to argue for whether or not interaction term(s) should be used in a statistical model. Determine slopes, intercepts, and other regression coefficients for specific categories or values of an explanatory variable in models that involve interaction. 2.1 Fitting Models to Data 2.1.1 Terminology In this section, we’ll use statistical models to predict the prices of houses in King County, WA. In a statistical model, The variable we are trying to predict (price) is called the response variable (denoted \\(Y\\)). Variable(s) we use to help us make the prediction is(are) called explanatory variables (denoted \\(X\\)). These are also referred to as predictor variables or covariates. In this section, we’ll attempt to predict the price of a house, using information about its size (in square feet), and whether or not it is on the waterfront. The price is our response variable, while size and waterfront location are explanatory variables. Categorical variables are variables that take on groups or categories, rather than numeric values, for example, whether or not the house is on the waterfront. Quantitative variables take on meaningful numeric values, for example the number of square feet in the house. 2.1.2 Model with Quantitative Explanatory Variable We’ll first predict the price of the house, using the number of square feet of living space as our explanatory variable. We’ll assume that price changes linearly with square feet, and fit a trend line to the data. ggplot(data=Houses, aes(x=sqft_living, y=price)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) + ggtitle(&quot;Price and Living Space&quot;) + ylab(&quot;Price&quot;) + xlab(&quot;Living Space in sq. ft. &quot;) The model equation is \\[ \\widehat{\\text{Price}} = b_0 + b_1\\times\\text{Sq.Ft.} \\] Note, the symbol over the response variable (Price) is read as “hat”, and means “predicted price”. We fit the model in R, using the lm (linear model) command. The output gives the estimates of \\(b_0\\) and \\(b_1\\). M_House_sqft &lt;- lm(data=Houses, price~sqft_living) M_House_sqft ## ## Call: ## lm(formula = price ~ sqft_living, data = Houses) ## ## Coefficients: ## (Intercept) sqft_living ## -484.9575 0.5328 The estimates are \\(b_0=-484.9575\\) and \\(b_1=0.5328\\). The model equation is \\[ \\widehat{\\text{Price}} = -484.9575 + 0.5328\\times\\text{Sq.Ft.} \\] Interpretations The intercept \\(b_0\\) represents the expected (or average) value of the response variable, when the explanatory variable is equal to 0. This is not always a meaningful interpretation in context. The slope \\(b_1\\) represents the expected (or average) change in the response variable for each one-unit increase in the explanatory variable. On average, a house with 0 square feet is expected to cost -485 thousand dollars. This is not a sensible interpretation, as there are no houses with 0 square feet. For each additional square foot in living space, the price of the house is expected to increase by 0.5328 thousand dollars (or $533). - Since a 1 square ft. increase is very small, it makes more sense to give the interpretation in terms of a 100-square foot increase. For each additional 100 square feet in living space, the price of the house is expected to increase by 53.28 thousand dollars. Prediction We can predict the price of a house with a given number of square feet by plugging the square feet into the model equation. The predicted price of a house with 1,500 square feet is \\[ \\widehat{\\text{Price}} = -484.9575 + 0.5328\\times 1500 = \\$314{ \\text{ thousand}} \\] We can calculate this directly in R using the predict command. predict(M_House_sqft, newdata=data.frame(sqft_living=1500)) ## 1 ## 314.1803 We should only try to make predictions on houses within the range of the observed data. Since the largest house in the dataset is 8,000 square feet we should not try to predict the price of house with 10,000 square feet. 2.1.3 Model with Categorical Variable Next, we’ll predict the price of a house based on whether or not it is on the waterfront. The boxplot shows the distribution of prices for waterfront and nonwaterfront houses. The red dots indicate the mean. ggplot(data=Houses, aes(x=waterfront, y=price)) + geom_boxplot() + ggtitle(&quot;House Price by Waterfront Status&quot;) + xlab(&quot;Waterfront&quot;) + ylab(&quot;Price&quot;) + coord_flip() + stat_summary(fun.y=mean, geom=&quot;point&quot;, shape=20, color=&quot;red&quot;, fill=&quot;red&quot;) The table displays the price summary by waterfront status. Houses_Grouped_Summary &lt;- Houses %&gt;% group_by(waterfront) %&gt;% summarize(Mean_Price = mean(price, na.rm=TRUE), Median_Price = median(price, na.rm=TRUE), StDev_Price = sd(price, na.rm = TRUE), Number_of_Houses = n()) kable(Houses_Grouped_Summary) waterfront Mean_Price Median_Price StDev_Price Number_of_Houses No 523.7595 450 295.7991 85 Yes 1934.3800 1350 1610.7959 15 The model equation is \\[ \\widehat{\\text{Price}} = b_0 + b_1\\times\\text{Waterfront} \\] The waterfront variable takes on value of 1 if the house is on the waterfront, and 0 otherwise. M_House_wf &lt;- lm(data=Houses, price~waterfront) M_House_wf ## ## Call: ## lm(formula = price ~ waterfront, data = Houses) ## ## Coefficients: ## (Intercept) waterfrontYes ## 523.8 1410.6 The estimates are \\(b_0=523.8\\) and \\(b_1=1410.6\\). The model equation is \\[ \\widehat{\\text{Price}} = 523.8 + 1410.6\\times \\text{Waterfront} \\] Interpretations The intercept \\(b_0\\) represents the expected (or average) value of the response variable in the “baseline” category (in this case non-waterfront). The coefficient \\(b_1\\) represents the expected (or average) difference in response between the a category and the “baseline” category. On average, a house that is not on the waterfront is expected to cost 523.8 thousand dollars. On average a house that is on the waterfront is expected to cost 1410.6 thousand (or 1.4 million) dollars more than a house that is not on the waterfront. Prediction We can predict the price of a house with a given number of square feet by plugging in either 1 or 0 for the waterfront variable. The predicted price of a house on the waterfront is: \\[ \\widehat{\\text{Price}} = 523.8 + 1410.6\\times 1 = \\$1934.6{ \\text{ thousand (or 1.9 million)}} \\] The predicted price of a house not on the waterfront is: \\[ \\widehat{\\text{Price}} = 523.8 + 1410.6\\times 0 = \\$523.8{ \\text{ thousand}} \\] Calculations in R: predict(M_House_wf, newdata=data.frame(waterfront=&quot;Yes&quot;)) ## 1 ## 1934.38 predict(M_House_wf, newdata=data.frame(waterfront=&quot;No&quot;)) ## 1 ## 523.7595 Notice that the predicted prices for each category correspond to the average price for that category. 2.1.4 Model with Multiple Explanatory Variables We’ve used square feet and waterfront status as explanatory variables individually. We can also build a model that uses both of these variables at the same time. A model with two or more explanatory variables is called a multiple regression model. The model equation is \\[ \\widehat{\\text{Price}} = b_0 + b_1\\times\\text{Sq. Ft} + b_2\\times\\text{Waterfront} \\] For a house not on the waterfront, \\(b_2=0\\), so the model equation is: \\[ \\widehat{\\text{Price}} = b_0 + b_1\\text{Sq. Ft} \\] For a house on the waterfront, \\(b_2=1\\), so the model equation is: \\[ \\widehat{\\text{Price}} = (b_0 + b_2) + b_1\\times\\text{Sq. Ft} \\] Notice that the slope is the same, regardless of whether the house is on the waterfront (\\(b_1\\)). The intercept, however, is different (\\(b_0\\) for houses not on the waterfront, and \\(b_0 + b_2\\) for houses on the waterfront). Thus, the model assumes that price increases at the same rate, with respect to square feet, regardless of whether or not it is on the waterfront, but allows the predicted price for a waterfront house to differ from a non-waterfront house of the same size. We fit the model in R. M_wf_sqft &lt;- lm(data=Houses, price~sqft_living+waterfront) M_wf_sqft ## ## Call: ## lm(formula = price ~ sqft_living + waterfront, data = Houses) ## ## Coefficients: ## (Intercept) sqft_living waterfrontYes ## -407.6549 0.4457 814.3613 The model equation is \\[ \\widehat{\\text{Price}} = -407.7 + 0.4457\\times\\text{Sq. Ft} + 814.36\\times\\text{Waterfront} \\] Interpretations The intercept \\(b_0\\) represents the expected (or average) value of the response variable, when all quantitative explanatory variables are equal to 0, and all categorical variables are in the “baseline” category. This interpretion is not always sensible. We interpret coefficients \\(b_j\\) for categorical or quantitative variables, the same way we would in a regression model with only one variable, but we need to state that all other explanatory variables are being held constant. On average, a house that is not on the waterfront with 0 square feet is expected to cost -407.7 thousand dollars. This is not a sensible interpretation, since there are no houses with 0 square feet. For each 1-square foot increase in size, the price of a house is expected to increase by 0.4457 thousand (or 446 hundred) dollars, assuming waterfront status is the same. Equivalently, for each 100-square foot increase in size, the price of a house is expected to increase by 44.57 thousand dollars, assuming waterfront status is the same. On average, a house on the waterfront is expected to cost 814 thousand dollars more than a house that is not on the waterfront, assuming square footage is the same. Prediction The predicted price of a 1,500 square foot house on the waterfront is: \\[ \\widehat{\\text{Price}} = -407.7 + 0.4457\\times1500 + 814.36\\times1 = \\$1075{ \\text{ thousand (or 1.075 million)}} \\] The predicted price of a 1,500 square foot not on the waterfront is: \\[ \\widehat{\\text{Price}} = -407.7 + 0.4457\\times1500 = \\$260.9{ \\text{ thousand}} \\] Calculations in R: predict(M_wf_sqft, newdata=data.frame(waterfront=&quot;Yes&quot;, sqft_living=1500)) ## 1 ## 1075.227 predict(M_wf_sqft, newdata=data.frame(waterfront=&quot;No&quot;, sqft_living=1500)) ## 1 ## 260.8657 2.1.5 Model with No Explanatory Variable Finally, we’ll consider a model that makes use of no explanatory variables at all. Although this might seem silly, its relevance will be seen in the next section. The histogram shows the distribution of prices, without any information about explanatory variables. The mean price is indicated in red. ggplot(data=Houses, aes(x=price)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + ggtitle(&quot;Distribution of House Prices&quot;) + xlab(&quot;Price&quot;) + ylab(&quot;Frequency&quot;) + geom_point(aes(x=mean(Houses$price), y=0), color=&quot;red&quot;, shape=24, fill=&quot;red&quot;) The mean, median, and standard deviation in prices is shown below. library(knitr) kable(Houses_Summary) Mean_Price Median_Price StDev_Price Number_of_Houses 735.3525 507.5 835.1231 100 Suppose we know that a house sold in King County during this time, and want to predict the price, without knowing anything else about the house. The best we can do is to use the mean price for our prediction. (We’ll define what we mean by “best” later in the chapter.) The model equation is \\[ \\widehat{\\text{Price}} = b_0 \\] We fit a statistical model in R using the lm command. # syntax for lm command # lm(data=DatasetName, ResponseVariable~ExplanatoryVariable(s)) M0_House &lt;- lm(data=Houses, price ~ 1) # when there are no explanatory variables, use ~1 M0_House ## ## Call: ## lm(formula = price ~ 1, data = Houses) ## ## Coefficients: ## (Intercept) ## 735.4 The model equation is \\[ \\widehat{\\text{Price}} = 735.4 \\] Interpretation The expected price of a house in King County is 735.4 thousand dollars. Predictions Without knowing anything about any explanatory variables, we would predict the price of any house sold in King County, WA to cost 735.4 thousand dollars. 2.2 Variability Explained by a Model 2.2.1 Quantifying Variability We’ve seen four different models for predicting house price. It would be nice to have a way to assess how well the models are predicting prices, and determine which model appears to be the best. Of course we won’t know the price of the house we are trying to predict, so we can’t be sure how close or far our prediction is. We do, however, know the prices of the original 100 houses in our dataset. We can assess the models by measuring how far the actual prices of the 100 houses differ from the predicted (mean) price, and by calculating the proportion of total variation in sale price explained by each model. 2.2.2 Total Variability Let’s start with our most basic model, which uses no explanatory variables and predicts the price of each simply using the average of all houses in the dataset. We measure the total variability in the response variable by calculating the square difference between each individual response value and the overall average. This quantity is called the total sum of squares (SST). \\[ \\text{SST} = \\displaystyle\\sum_{i=1}^n (y_i - \\bar{y})^2 \\] The plot below shows a horizontal line at the mean sale price (785 thousand). The points represent prices of individual houses, and the red lines represent the differences between the price of each house and the overall average. The first three houses in the dataset are shown below. First3Houses &lt;- Houses %&gt;% select(Id, price, waterfront, sqft_living) %&gt;% head(3) kable(First3Houses) Id price waterfront sqft_living 1 1225 No 5420 2 885 No 2830 3 385 No 1620 \\[ \\begin{aligned} \\text{SST} &amp; = \\displaystyle\\sum_{i=1}^{100} (y_i - \\bar{y})^2 \\\\ &amp; = (1225-785)^2 + (885-785)^2 + (385-785)^2 + \\ldots \\end{aligned} \\] We could calculate SST by hand for small datasets. For larger datasets, we’ll use R to perform the calculation. meanprice &lt;- mean(Houses$price) #calculate mean price SST &lt;- sum((Houses$price - meanprice)^2) ## calculate SST SST ## [1] 69045634 By itself, the size of SST does not have much meaning. We cannot say whether a SST value like the one we see here is large or small, since it depends on the size and scale of the variable being measured. An SST value that is very large in one context might be very small in another. SST does, however, give us a baseline measure of the total variability in the response variable. We’ll assess the performance of a model with a given explanatory variable by measuring how much of this variability the model accounts for. 2.2.3 Residuals Now let’s consider our model that uses the size of the house in square feet as the explanatory variable. The figure on the left shows difference between actual and predicted prices, using this linear model. We compare the size of the differences to those resulting from the basic model that does not use any explanatory variables, and predicts each price using the overall average (shown on the right). Residplot_sqft &lt;- ggplot(data=Houses, aes(x = sqft_living, y = price)) + geom_point() + geom_segment(aes(xend = sqft_living, yend = M_House_sqft$fitted.values), color=&quot;red&quot;) + geom_point(aes(y = M_House_sqft$fitted.values), shape = 1) + stat_smooth(method=&quot;lm&quot;, se=FALSE) + ylim(c(0,5500)) + theme_bw() Notice that the red lines are shorter in the figure on the left, indicating the predictions are closer to the actual values. The difference between the actual and predicted values is called the residual. The residual for the \\(ith\\) case is \\[ r_i = (y_i-\\hat{y}_i) \\] We’ll calculate the residuals for the first three houses in the dataset, shown below. kable(First3Houses) Id price waterfront sqft_living 1 1225 No 5420 2 885 No 2830 3 385 No 1620 The model equation is \\[ \\widehat{\\text{Price}} = -484.9575 + 0.5328\\times \\text{Sq. Ft} \\] The predicted prices for these three houses are: \\[ \\widehat{\\text{Price}_1} = -484.9575 + 0.5328\\times 5420 = 2402.6 \\text{ thousand dollars} \\] \\[ \\widehat{\\text{Price}_2} = -484.9575 + 0.5328\\times 2830 = 1022.7 \\text{ thousand dollars} \\] \\[ \\widehat{\\text{Price}_3} = -484.9575 + 0.5328\\times 1620 = 378.1 \\text{ thousand dollars} \\] To calculate the residuals, we subtract the predicted price from the actual price. \\[r_1 = y_1-\\hat{y}_1 = 1225 - 2402.6 = -1177.6 \\text{ thousand dollars}\\] \\[r_2 = y_2-\\hat{y}_2 = 885 - 1022.7 = -137.7 \\text{ thousand dollars}\\] \\[r_2 = y_2-\\hat{y}_2 = 385 - 378.1 = 6.9 \\text{ thousand dollars}\\] The fact that the first two residuals are negative indicates that these houses sold for less than the model predicts. The predicted values and residuals from a model can be calculated automatically in R. The predicted values and residuals for the first 5 houses are shown below. Predicted &lt;- predict(M_House_sqft) head(Predicted, 3) ## 1 2 3 ## 2402.5937 1022.7491 378.1113 Residual &lt;- M_House_sqft$residuals head(Residual, 3) ## 1 2 3 ## -1177.593665 -137.749128 6.888668 2.2.4 Variability Explained by Sq. Ft. Model The sum of squared residuals (SSR) measures the amount of unexplained variability in the response variable after accounting for all explanatory variables in the model. \\[ \\text{SSR} = \\displaystyle\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2. \\] Note that SSR is similar to SST, except we subtract the model’s predicted values, rather than the overall average. In the special case of a model with no explanatory variables, the predicted values are equal to the overall average, so SSR is equal to SST. We calculate SSR for the model using square feet as the explanatory variable. \\[ \\begin{aligned} \\text{SSR} &amp; = \\displaystyle\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2. \\\\ &amp; = (1225 - 2402.6)^2 + (885 - 1022.7)^2 + (385 - 378.1)^2 + \\ldots \\end{aligned} \\] We can calculate the model’s SSR directly in R. SSR_sqft &lt;- sum(M_House_sqft$residuals^2) SSR_sqft ## [1] 23767280 SSR represents the amount of total variability in saleprice remaining after accounting for the house’s size in square feet. The SSR=23,767,290 value is about one third of the SST value of 69,045,634. This means that about 2/3 of the total variability in sale price is explained by the model that accounts for sale price. The difference (SST-SSR) represents the variability in the response variable that is explained by the model. This quantity is called the model sum of squares (SSM). \\[ \\text{SSM} = \\text{SST} - \\text{SSR} \\] It can be shown that \\(\\text{SSM}=\\displaystyle\\sum_{i=1}^n(\\hat{y}_i-\\bar{y})^2\\). The proportion of total variability in the response variable explained by the model is called the coefficient of determination, denoted \\(R^2\\). We calculate this by dividing SSM by SST. \\[ R^2=\\frac{SSM}{SST}= \\frac{SST-SSR}{SST} \\] Example: For the model with square feet as the explanatory variable, \\[ SSM = SST-SSR = 69,045,634 - 23,767,290 =45,278,344. \\] \\[ R^2 = \\frac{45,278,344}{69,045,634}=0.6557. \\] Approximately 65.6% of the total variability in sale price is explained by the model using square feet as the explanatory variable. We calculate \\(R^2\\) directly in R. summary(M_House_sqft)$r.squared ## [1] 0.6557743 2.2.5 Linear Correlation Coefficient For models with a single quantiative explanatory varible, the coefficient of determination is equal to the square of the correlation coefficient \\(r\\), discussed in Chapter 1. ggplot(data=Houses, aes(x=sqft_living, y=price)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) For linear models with a single quantitative variable, the linear correlation coefficient \\(r=\\sqrt{R^2}\\), or \\(r=-\\sqrt{R^2}\\) (with sign matching the sign on the slope of the line), provides information about the strength and direction of the linear relationship between the variables. \\(-1 \\leq r \\leq 1\\), and \\(r\\) close to \\(\\pm1\\) provides evidence of strong linear relationship, while \\(r\\) close to 0 suggests linear relationship is weak. cor(Houses$price,Houses$sqft_living) ## [1] 0.8097989 \\(r\\) is only relevant for models with a single quantitative explanatory variable and a quantitative response variable, while \\(R^2\\) is relevant for any linear model with a quantitative response variable. 2.2.6 Variability Explained by Waterfront Model We can similarly calculate the proportion of variability explained by the model using waterfront as an explanatory variable. Recall that in this model, the predicted price of a house with a waterfront is given by the average price of all waterfront houses, and the predicted price of a non-waterfront house is given by the average price of all non-waterfront houses. We can calculate residuals using these predicted values, and compare them to the residuals resulting from a model with no explanatory variables, which uses the overall average price for all predictions. The left two figures show the residuals resulting from a model that accounts for waterfront status. The figure on the right shows the residuals resulting from the model with no explanatory variables. grid.arrange(arrangeGrob(M1aResid,M1bResid, Residplot_M0 + ggtitle(&quot;Model with no Exp. Vars&quot;), ncol=3, nrow=1, widths=c(3, 2,5))) Notice that after accounting for waterfront status, the differences between observed and predicted values are bigger than they were in the model that accounted for square feet, though not as big as for the model that doesn’t use any explanatory variables. We use R to calculate SSR for the waterfront model. SSR_wf &lt;- sum(M_House_wf$residuals^2) SSR_wf ## [1] 43675043 \\[ SSM = SST-SSR = 69,045,634 - 43,675,043 =25,370,591. \\] \\[ R^2 = \\frac{25,370,591}{69,045,634}=0.3674. \\] Approximately 36.7% of the total variability in sale price is explained by the model using waterfront status as the explanatory variable. We calculate \\(R^2\\) directly in R. summary(M_House_wf)$r.squared ## [1] 0.3674467 2.2.7 Variability Explained by Multiple Regression Model We’ve seen at the model using square feet accounts for about 2/3 of the total variability in house prices, while the model using waterfront status accounts for about 1/3 of the total variability. Let’s see if we can do better by using both variables together. The left figure shows the residuals resulting from a model that accounts for both waterfront status and square feet. The figure on the right shows the residuals resulting from the model with no explanatory variables. grid.arrange(Residplot_MR + ggtitle(&quot;Multiple Regression Model&quot;) , Residplot_M0 + ggtitle(&quot;Model with no Exp. Vars&quot;), ncol=2) We use R to calculate SSR for the waterfront model. SSR_wf_sqft &lt;- sum(M_wf_sqft$residuals^2) SSR_wf_sqft ## [1] 16521296 \\[ SSM = SST-SSR = 69,045,634 - 16,521,296 =52,524,338. \\] \\[ R^2 = \\frac{52,524,338}{69,045,634}=0.761. \\] Approximately 76.1% of the total variability in sale price is explained by the model using square feet and waterfront status as the explanatory variables. We calculate \\(R^2\\) directly in R. summary(M_wf_sqft)$r.squared ## [1] 0.7607192 Including both square feet and waterfront status allows us to explain more variability in sale price than models that include one but not both of these variables. 2.2.8 Summary: SST, SSR, SSM, \\(R^2\\) the total variability in the response variable is the sum of the squared differences between the observed values and the overall average. \\[\\text{Total Variability in Response Var.}= \\text{SST} =\\displaystyle\\sum_{i=1}^n(y_i-\\bar{y})^2\\] the variability remaining unexplained even after accounting for explanatory variable(s) in a model is given by the sum of squared residuals. We abbreviate this SSR, for sum of squared residuals. \\[ \\text{SSR} = \\text{Variability Remaining}=\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 \\] the variability explained by the model, abbreviated SSM, is given by \\[ \\text{SSM} = \\text{SST} - \\text{SSR} \\] The coefficient of determination (abbreviated \\(R^2\\)) is defined as \\[R^2=\\frac{\\text{Variability Explained by Model}}{\\text{Total Variability}}=\\frac{\\text{SSM}}{\\text{SST}} =\\frac{\\displaystyle\\sum_{i=1}^n(\\hat{y}_i-\\bar{y})^2}{\\displaystyle\\sum_{i=1}^n(y_i-\\bar{y})^2}\\] Note that some texts use different abbreviations than the ones used here. When working with resources outside this class, be sure to carefully check the notation being used. 2.2.9 \\(R^2\\) Visually Model with a single quantitative explanatory variable: Model with a single categorical explanatory variable with 3 categories: Blue Area = Total Variability (SST) Red Area = Variability Remaining Unexplained by Model (SSR) Blue Area - Red Area = Variability Explained by Model (SSM) \\(R^2 = \\frac{\\text{Area of Blue Squares} - \\text{Area of Red Squares}}{\\text{Area of Blue Squares}} = \\frac{\\text{SST}-\\text{SSR}}{\\text{SST}}= \\frac{\\text{SSM}}{\\text{SST}}\\) 2.2.10 Model Comparison Summary Model Variables Unexplained Variability Variability Explained \\(R^2\\) 0 None 69045634.1341748 0 0 1 Sq. Ft. 23767280.3817707 45278353.752404 0.6557743 2 Waterfront 43675043.0897012 25370591.0444736 0.3674467 3 Sq. Ft. and Waterfront 16521296.4889025 52524337.6452723 0.7607192 Comments on \\(R^2\\): \\(R^2\\) will never decrease when a new variable is added to a model. This does not mean that adding more variables to a model always improves its ability to make predictions on new data. \\(R^2\\) measures how well a model fits the data on which it was built. It is possible for a model with high \\(R^2\\) to “overfit” the data it was built from, and thus perform poorly on new data. We will discuss this idea extensively later in the course. On some datasets, there is a lot of “natural” variability in the response variable, and no model will achieve a high \\(R^2\\). That’s okay. Even a model with \\(R^2 = 0.10\\) or less can provide useful information. The goal is not to achieve a model that makes perfect predictions, but rather to be able to quantify the amount of uncertainty associated with the predictions we make. 2.3 Models with Interaction 2.3.1 Definition of Interaction We previously used a multiple regression model of the form \\[ \\widehat{Price} = b_0 + b_1\\times\\text{SqFt} + b_2\\times\\text{Waterfront} \\] Recall that this model assumes the slope relating price and square footage is the same (\\(b_1\\)) for houses on the waterfront as for houses not on the waterfront. An illustration of the model is shown below. PM3 This assumption of the rate of change in price with respect to living space being the same for waterfront houses, as for non-waterfront houses might be unrealistic. Let’s fit separate lines waterfront and non-waterfront houses, without requiring them to have the same slope. ggplot(data=Houses, aes(x=sqft_living, y=price, color=waterfront)) + geom_point()+stat_smooth(method=&quot;lm&quot;, se=FALSE) + ylim(c(0,5500)) + theme_bw() It appears that the prices of the houses on the waterfront are increasing more rapidly, with respect to square feet of living space, than the non-waterfront houses. The effect of additional square feet on the price of the house appears to depend on whether or not the house is on the waterfront. This is an example of an interaction between square footage and waterfront status. An interaction between two explanatory variables occurs when the effect of one explanatory variable on the response depends on the other explanatory variable. 2.3.2 Interaction Term If we want to allow for different slopes between waterfront and non-waterfront houses, we’ll need to change the mathematical equation of our model. To do that, we’ll add a coefficient \\(b_3\\), multiplied by the product of our two explanatory variables. The model equation is \\[ \\widehat{Price} = b_0 + b_1\\times\\text{Sq. Ft.} + b_2\\times\\text{waterfront} + b_3\\times\\text{Sq.Ft}\\times\\text{Waterfront} \\] The last term is called an interaction term. For a house on the waterfront (\\(\\text{waterfront}=1\\)), the equation relating price to square feet is \\[ \\begin{aligned} \\widehat{Price} &amp; = b_0 + b_1\\times\\text{Sq. Ft.} + b_2\\times\\text{1} + b_3\\times\\text{Sq.Ft}\\times\\text{1} \\\\ &amp; = (b_0+b_2) + (b_1+b_3)\\times{\\text{Sq. Ft.}} \\end{aligned} \\] For a house not on the waterfront (\\(\\text{waterfront}=0\\)), the equation relating price to square feet is \\[ \\begin{aligned} \\widehat{Price} &amp; = b_0 + b_1\\times\\text{Sq. Ft.} + b_2\\times\\text{0} + b_3\\times\\text{Sq.Ft}\\times\\text{0} \\\\ &amp; = b_0 + b_1\\times{\\text{Sq. Ft}} \\end{aligned} \\] The intercept is \\(b_0\\) for non-waterfront houses, and \\(b_0 + b_2\\) for waterfront houses. The slope is \\(b_1\\) for non-waterfront houses, and \\(b_1 + b_3\\) for waterfront houses. Thus, the model allows both the slope and intercept to differ between waterfront and non-waterfront houses. 2.3.3 Interaction Models in R To fit an interaction model in R, use * instead of + M_House_Int &lt;- lm(data=Houses, price~sqft_living*waterfront) M_House_Int ## ## Call: ## lm(formula = price ~ sqft_living * waterfront, data = Houses) ## ## Coefficients: ## (Intercept) sqft_living ## 67.3959 0.2184 ## waterfrontYes sqft_living:waterfrontYes ## -364.5950 0.4327 The regression equation is \\[ \\widehat{Price} = 67.4 + 0.2184\\times\\text{Sq. Ft.} -364.6\\times\\text{waterfront} + 0.4327\\times\\text{Sq.Ft}\\times\\text{Waterfront} \\] For a house on the waterfront (\\(\\text{waterfront}=1\\)), the equation is \\[ \\begin{aligned} \\widehat{Price} &amp; = 67.4 + 0.2184\\times\\text{Sq. Ft.} -364.6 \\times\\text{1} + 0.4327\\times\\text{Sq.Ft}\\times\\text{1} \\\\ &amp; = (67.4 - 364.6) + (0.2184+0.4327)\\times{\\text{Sq. Ft.}} \\\\ &amp; = -297.2 + 0.6511\\times{\\text{Sq. Ft.}} \\end{aligned} \\] For a house not on the waterfront (\\(\\text{waterfront}=0\\)), the equation is \\[ \\begin{aligned} \\widehat{Price} &amp; = 67.4 + 0.2184\\times\\text{Sq. Ft.} -364.6 \\times\\text{0} + 0.4327\\times\\text{Sq.Ft}\\times\\text{0} \\\\ &amp; = 67.4 0 + 0.2184\\times{\\text{Sq. Ft.}} \\end{aligned} \\] Interpretation When interpreting \\(b_0\\) and \\(b_1\\), we need to state that the interpretations apply only to the “baseline” category (in this case non-waterfront houses). In a model with interaction, it does not make sense to talk about holding one variable constant when interpreting the effect of the other, since the effect of one variable depends on the value or category of the other. Instead, we must state the value or category of one variable when interpreting the effect of the other. Interpretations: \\(b_0\\) - On average, a house with 0 square feet that is not on the waterfront is expected to cost 67 thousand dollars. This is not a sensible interpretation since there are no houses with 0 square feet. \\(b_1\\) - For each additional square foot in size, the price of a non-waterfront house is expected to increase by 0.2184 thousand dollars. \\(b_2\\) - On average, the price of a waterfront house with 0 square feet is expected to be 364.6 thousand dollars less than the price of a non-waterfront house with 0 square feet. This is not a sensible interpretation in this case. \\(b_3\\) - For each additional square foot in size, the price of a waterfront house is expected to increase by 0.4327 thousand dollars more than a non-waterfront house. Alternatively, we could interpret \\(b_0+b_2\\) and \\(b_1+b_3\\) together. \\(b_0 + b_2\\) - On average, a house with 0 square feet that is on the waterfront is expected to cost -297.2 thousand dollars. This is not a sensible interpretation since there are no houses with 0 square feet. \\(b_1\\) - For each additional square foot in size, the price of a waterfront house is expected to increase by 0.6511 thousand dollars. Prediction We calculate predicted prices for the following houses: Houses[c(1,16), ] %&gt;% select(Id, price, sqft_living, waterfront) ## # A tibble: 2 × 4 ## Id price sqft_living waterfront ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1 1225 5420 No ## 2 16 3075 4550 Yes \\[ \\widehat{Price}_1 = 67.4 + 0.2184\\times5420 -364.6\\times0 + 0.4327\\times5420 \\times 0 = 1191 \\text{ thousand dollars} \\] \\[ \\widehat{Price}_{16} = 67.4 + 0.2184\\times4450 -364.6\\times1 + 0.4327\\times4450 \\times 1 = 2600 \\text{ thousand dollars} \\] 2.3.4 \\(R^2\\) for Interaction Model We can calculate residuals, as well as SSR, SSM, SST, and \\(R^2\\), in the same manner we’ve seen previously. We’ll perform these calculations using R. SSR_int &lt;- sum(M_House_Int$residuals^2) SSR_int ## [1] 10139974 \\[ SSM = SST-SSR = 69,045,634 - 10,139,974 =58,905,660. \\] \\[ R^2 = \\frac{58,905,660}{69,045,634}=0.8531. \\] Approximately 85.3% of the total variability in sale price is explained by the model using square feet and waterfront status, as well as an interaction between them as the explanatory variables. We calculate \\(R^2\\) directly in R. summary(M_House_Int)$r.squared ## [1] 0.853141 We see that adding an interaction term improved the proportion of variability in house price explained by the model from 0.76 to 0.85. This is a fairly notable increase. 2.3.5 Considerations for Using Interactions It might be tempting to think we should always add an interaction term to a model when using two or more explanatory variables. Afterall, an interaction term is just another term added to the model, meaning that \\(R^2\\) will never go down. Adding an interaction term is not always a good idea, though. We saw that doing so makes interpretations more complicated. Increasing the complexity of a model also increases the risk of overfitting, potentially hurting predictive performance on new data. We should only add an interaction term if we have strong reason to believe that the rate of change in the response variable with respect to one explanatory variable really does depend on the other variable. This might come from background knowledge about the subject, or consultation with an expert in the area. It could also come from data visualization, and the increase in variability in the response variable explained when an interaction term is added to the model. In the house price dataset, we might expect that the price of waterfront houses might increase more rapidly as they get bigger than the price of non-waterfront houses. The fact that the lines shown in the scatterplot are not close to being parallel provides further evidence of a difference in rate of increase, providing justification for the use of an interaction term in the model. Furthermore, \\(R^2\\) increases notably (from 0.76 to 0.85), when an interaction term is added. All of these reasons support using an interaction term in this context. When examining a scatterplot, we should note that even if there is truly no interaction among all houses, the lines probably won’t be exactly parallel, due to random deviations among the sample of houses chosen. If the lines are reasonably close to parallel, then an interaction term is likely not needed. We’ll look more at criteria for determining whether to add an interaction term to a model in the coming sections. 2.3.6 Interaction vs Correlation It is easy to confuse the concept of interaction with that of correlation. These are, in fact, very different concepts. A correlation between two variables means that as one increases, the other is more likely to increase or decrease. We only use the word correlation to describe two quantitative variables, but we could discuss the similar notion of a relationship between categorical variables. An interaction between two explanatory variables means that the effect of one on the response depends on the other. Examples of Correlations (or relationships) Houses on the waterfront tend to be bigger than houses not on the waterfront, so there is a relationship between square feet and waterfront status. Houses with large amounts of living space in square feet are likely to have more bedrooms, so there is a correlation between living space and bedrooms. Suppose that some genres of movies (drama, comedy, action, etc.) tend to be longer than others. This is an example of a relationship between genre and length. The fact that there is a correlation between explanatory variables is NOT a reason to add an interaction term involving those variables in a model. Correlation is something entirely different than interaction! Examples of Interactions As houses on the waterfront increase in size, their price increases more rapidly than for houses not on the waterfront. This means there is an interaction between size and waterfront location. Suppose that the effect of additional bedrooms on price is different for houses with lots of living space than for houses with little living space. This would be an example of an interaction between living space and number of bedrooms. Suppose that audiences become more favorable to dramas as they get longer, but less favorable to comedies as they get longer. In this scenario, the effect of movie length on audience rating depends on the genre of the movie, indicating an interaction between length and genre. 2.4 Least Squares Estimation (LSE) 2.4.1 Estimating Regression Coefficients We’ve already used R to determine the estimates of \\(b_0\\), \\(b_1\\), \\(b_2\\), and \\(b_3\\) in various kinds of linear models. At this point, it is natural to wonder where these estimates are come from. Regression coefficients \\(b_0, b_1, \\ldots, b_p\\) are chosen in a way that minimizes the sum of the squared differences between the observed and predicted values. That is, we minimize \\[ \\text{SSR} = \\displaystyle\\sum_{i=1}^{n} (y_i-\\hat{y}_i)^2 \\] Because \\(\\hat{y}_i\\) is a function of \\(b_0, b_1, \\ldots, b_p\\), we can choose the values of \\(b_0, b_1, \\ldots, b_p\\) in a way that minimizes SSR. \\[\\text{SSR} = \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2 = \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 \\] The process of estimating regression coefficients \\(b_0, b_1, \\ldots, b_p\\) in a way that minimizes SSR is called least-squares estimation. Example: Model with one quantitative variable We start with an example of estimating the regression coefficients for a model with a single explanatory variable. This is easy to illustrate, since we can draw a scatter plot displaying our explanatory and response variable. The figure below illustrates four possible trend lines that could be fit to a set of 10 points in a scatter plot. The first line is the line of best fit, in that it makes the sum of the squared residuals the smallest of all possible lines that could be drawn. The second through fourth plots all show examples of other trend lines that are not the line of best fit. The sum of squared residuals for each of these models is bigger than for the first one. In the illustration, SSR is represented by the total area of the squares. The line of best fit is the one that make the intercept the smallest. This Rossman-Chance applet provides an illustration of the line of best fit. Returning to the model for predicting price of a house, using only size in square feet as an explanatory variable, the scatter plot, along with the slope and intercept of the regression line are shown below. ggplot(data=Houses, aes(x=sqft_living, y=price)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) + theme_bw() M_House_sqft ## ## Call: ## lm(formula = price ~ sqft_living, data = Houses) ## ## Coefficients: ## (Intercept) sqft_living ## -484.9575 0.5328 The line \\(\\text{Price} = -485 + 0.5328 \\times \\text{Square Feet}\\) is the “line of best fit” in the sense that it minimizes the sum of the squared residuals (SSR). Any other choices for the slope or intercept of the regression line would result in larger SSR than this line. 2.4.2 Mathematics of LSE for SLR Consider a simple linear regression(SLR) model, which is one with a singe quantitative explanatory variable \\(x\\). \\(\\hat{y}_i = b_0+b_1x_i\\) we need to choose the values of \\(b_0\\) and \\(b_1\\) that minimize: \\[ \\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 =\\displaystyle\\sum_{i=1}^n(y_i-(b_0+b_1x_i))^2 \\] We setup the equation by substituting in the values of \\(y_i\\) and \\(x_i\\) seen in the data. Recall the first 3 houses in the dataset: kable(First3Houses) Id price waterfront sqft_living 1 1225 No 5420 2 885 No 2830 3 385 No 1620 \\[ \\begin{aligned} \\displaystyle\\sum_{i=1}^{100}(y_i-\\hat{y}_i)^2 &amp; =\\displaystyle\\sum_{i=1}^n(y_i-(b_0+b_1x_i))^2 \\\\ &amp; = (1225-(b_0+b_1(5420)))^2 + (885-(b_0+b_1(2830)))^2 + (385-(b_0+b_1(1620)))^2 + \\ldots \\end{aligned} \\] We need to find the values of \\(b_0\\) and \\(b_1\\) that minimize this expression. This is a 2-dimensional optimization problem that can be solved using multivariable calculus or numerical or graphical methods. Using calculus, it can be shown that this quantity is minimized when \\(b_1=\\frac{\\displaystyle\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\displaystyle\\sum_{i=1}^{n}(x_i-\\bar{x})^2}=\\frac{\\displaystyle\\sum_{i=1}^{n} x_i y_i-\\frac{\\displaystyle\\sum_{i=1}^{n} x_i \\displaystyle\\sum_{i=1}^{n} y_i }{n}}{\\left(\\displaystyle\\sum_{i=1}^{n} x_i^2 -\\frac{\\left(\\displaystyle\\sum_{i=1}^{n} x_i\\right)^2}{n}\\right)}\\) \\(b_0=\\bar{y}-b_1\\bar{x}\\) (where \\(\\bar{y}=\\frac{\\displaystyle\\sum_{i=1}^{n}{y_i}}{n}\\), and \\(\\bar{x}=\\frac{\\displaystyle\\sum_{i=1}^{n}{x_i}}{n}\\)). 2.4.3 LSE for Categorical Variable Consider a model with a single categorical variable (such as waterfront), with G+1 categories, numbered \\(g=0,2, \\ldots, G\\) Then \\(\\hat{y}_i = b_0 + b_1x_{i1} + \\ldots +b_{G}x_{iG}\\). we need to minimize \\[ \\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 =\\displaystyle\\sum_{i=1}^n(y_i-(b_0 + b_1x_{i1} + \\ldots +b_{G}x_{iG}))^2. \\] It can be shown that this is achieved when \\(b_0 = \\bar{y_0}\\) (i.e. the average response in the “baseline group”), and \\(b_j = \\bar{y_j} - \\bar{y}_0\\) 2.4.4 LSE More Generally For multiple regression models, including those involving interaction, the logic is the same. We need to choose \\(b_0, b_1, \\ldots, b_p\\) in order to minimize \\[ \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2 = \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 \\] The mathematics, however, are more complicated and require inverting a matrix. This goes beyond the scope of this class, so we will let R do the estimation and use the results. More on least squares estimation in multiple regression can be found here. 2.5 ANalysis Of VAriance 2.5.1 Submodels We’ve seen 5 different models for predicting house price using some combination of square feet and waterfront status. A model A is defined to be a submodel of another model B, if every term in model A is also included in model B. Model Variables Unexplained Variability Variability Explained \\(R^2\\) 0 None 69045634 0 0 1 Sq. Ft. 23767280 45278354 0.656 2 Waterfront 43675043 25370591 0.367 3 Sq. Ft. and Waterfront 16521296 52524338 0.761 4 Sq. Ft., Waterfront, and Interaction 10139974 58905661 0.853 Model 1 is a submodel of Model 3, since all variables used in Model 1 are also used in Model 3. Model 2 is also a submodel of Model 3. Models 1, 2, and 3 are all submodels of Model 4. Model 0 is a submodel of Models 1, 2, 3, and 4. Models 1 and 2 are not submodels of each other, since Model 1 contains a variable used in Model 2 and Model 2 contains a variable not used in Model 1. 2.5.2 F-Statistics When one model is a submodel of another, we can compare the amount of variability explained by the models, using a technique known as ANalysis Of VAriance (ANOVA). Reduced Model: \\(\\hat{y}_i = b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_qx_{iq}\\) Full Model: \\(\\hat{y}_i = b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_qx_{iq} + b_{q+1}x_{i{q+1}} \\ldots + b_px_{ip}\\) p = # terms in Full Model, not including the intercept q = # terms in Reduced Model, not including the intercept n = number of observations We calculate a statistic called F that measures the amount of variability explained by adding additional variable(s) to the model, relative to the total amount of unexplained variability. \\[ \\begin{aligned} F &amp;= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\end{aligned} \\] Large values of F indicate that adding the additional explanatory variables is helpful in explaining variability in the response variable Small values of F indicate that adding new explanatory variables variables does not make much of a difference in explaining variability in the response variable What counts as “large” is depends on \\(n, p,\\) and \\(q\\). We will revisit this later in the course. Example 1 Let’s Calculate an ANOVA F-Statistic to compare Models 1 and 3. Reduced Model: \\(\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{sqft_living}\\) Full Model: \\(\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{sqft_living}+ b_2\\times\\text{Waterfront}\\) \\[ \\begin{aligned} F &amp;= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\\\ &amp;=\\frac{\\frac{23,767,280-16,521,296}{2-1}}{\\frac{16,521,296}{100-(2+1)}} \\\\ \\end{aligned} \\] ((SSR_sqft-SSR_wf_sqft)/(2-1))/((SSR_wf_sqft)/(100-(2+1))) ## [1] 42.54269 We can calculate the statistic directly in R, using the anova command. anova(M_House_sqft, M_wf_SqFt)$F[2] ## [1] 42.54269 In the coming chapters, we’ll talk about what to conclude from an F-statistic of 42.5 Is this big enough to say that adding waterfront status to a model already including square feet helps better explain variability in sale price? (Spoiler alert: YES - an F-statistic of 42.5 is quite large and indicative that the full model is a better choice than the reduced model.) We previously saw that the model including both square feet and waterfront status had a \\(R^2\\) value considerably higher than the one including only square feet. This large F-statistic is further evidence to the benefit of considering both variables in our model. Example 2 We’ll calculate an F-statistic to compare Models 3 and 4. This can help us determine whether it is worthwhile to include an interaction term in our model. Reduced Model: \\(\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{sqft_living} + b_2\\times\\text{Waterfront}\\) Full Model: \\(\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{sqft_living}+ b_2\\times\\text{Waterfront} + b_3\\times\\text{sqft_living}\\times\\text{Waterfront}\\) \\[ \\begin{aligned} F &amp;= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\\\ &amp;=\\frac{\\frac{16,521,296-10,139,974}{3-2}}{\\frac{10,139,974}{100-(3+1)}} \\\\ \\end{aligned} \\] ((SSR_wf_sqft-SSR_int)/(3-2))/((SSR_int)/(100-(3+1))) ## [1] 60.41505 We can calculate the statistic directly in R, using the anova command. anova(M_wf_SqFt, M_House_Int)$F[2] ## [1] 60.41505 We observe an F-statistic of 60, which is even bigger than the one seen previously! This suggests that adding the interaction term does indeed improve the model’s ability to account for variability in prices. 2.5.3 Comparing 3 or More Categories F-statistics are commonly used when making comparisons involving categorical variables with 3 or more categories. One variable in the houses dataset, which we haven’t looked at yet, is the condition of the house at the time of sale. The table shows the number of houses in each condition listed. summary(Houses$condition) ## poor fair average good very_good ## 1 1 59 30 9 We notice that there is only one house in poor condition and one house in fair condition. These sample sizes are too small to analyze. We’ll combine these two houses with those in the “average” category, creating a new category called \"average or below). Houses$condition &lt;- fct_collapse(Houses$condition, &quot;average or below&quot; = c(&quot;poor&quot;,&quot;fair&quot;, &quot;average&quot;)) The boxplot shows the distribution of houses in each category, and the table below it provides a numerical summary. ggplot(data=Houses, aes(x=condition, y=price)) + geom_boxplot() +coord_flip() Cond_Tab &lt;- Houses %&gt;% group_by(condition) %&gt;% summarize(Mean_Price = mean(price), SD_Price= sd (price), N= n()) kable(Cond_Tab) condition Mean_Price SD_Price N average or below 700.6349 768.1179 61 good 861.0000 1048.9521 30 very_good 551.8361 332.8597 9 It can be helpful to calculate a single statistic that quantifies the size of the differences between the conditions. If we were just comparing two different categories, we could simply find the difference in mean prices between them. But, with three or more categories, we need a way to represent the size of the differences with a single number. An F-statistic can serve this purpose. We’ll calculate an F-statistic for a model that includes condition, compared to a model with only an intercept term. Reduced Model: \\(\\widehat{\\text{Price}}= b_0\\) Full Model: \\(\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{good condition}+ b_2\\times\\text{very good condition}\\) Notice that the equation includes separate variables for the “good” and “very” good conditions. These variables take on value 0 if the house is not in that condition, and 1 if the house is in that condition. Here, houses in “average or below” condition are considered the “baseline” category. We’ll fit the model in R. The coefficient estimates for \\(b_0\\), \\(b_1\\) and \\(b_2\\) are shown below. M_House_Cond &lt;- lm(data=Houses, price~condition) M_House_Cond ## ## Call: ## lm(formula = price ~ condition, data = Houses) ## ## Coefficients: ## (Intercept) conditiongood conditionvery_good ## 700.6 160.4 -148.8 The model equation is \\[ \\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{good condition}+ b_2\\times\\text{very good condition} \\] Interpretations On average, houses in average or below condition cost 700.6 thousand dollars. On average, houses in good condition cost 160.4 thousand dollars more than those in average or below condition. On average, houses in very good condition cost 148.8 thousand dollars less than those in average or below condition. This last sentence is surprising and merits further investigation. We’ll leave that for future consideration. For now, we’ll calculate an F-statistic based on the models. Note that in this case, the reduced model does not include any explanatory variables, so SSR is equal to SST, which we calculate previously. SST ## [1] 69045634 We calculate SSR for the full model. SSR_cond &lt;- sum(M_House_Cond$residuals^2) SSR_cond ## [1] 68195387 \\[ \\begin{aligned} F &amp;= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\\\ &amp;=\\frac{\\frac{69,045,634-68,195,387}{2-0}}{\\frac{68,195,387}{100-(2+1)}} \\\\ \\end{aligned} \\] ((SST - SSR_cond)/(2-0))/(SSR_cond/(100-(2+1))) ## [1] 0.6046888 We perform the calculation directly in R. anova(M_House_Cond, M0_House)$F[2] ## [1] 0.6046888 Notice that the F-statistic of 0.6 is considerably smaller than the F-statistics we’ve seen previously. This indicates that adding condition to a model with no other explanatory variables doesn’t seem to help improve the model’s ability to account for variation in price. Put another way, there doesn’t appear to be much evidence of difference in price between houses in the different conditions. 2.5.4 F-Statistic Illustration The figure below gives an illustration of data that would produce a large F-statistic (Scenario 1), and also data that would produce a small F-statistic (Scenario 2), like the one seen in the house condition data. An F-statistic compares the amount of variability between groups to the amount of variability within groups. In scenario 1, we notice considerable differences between the groups, relative to the amount of variability within groups. In this scenario, knowing the group an observation is in will help us predict the response for that group, so we should include account for the groups in our model. We would obtain a large F-statistic when comparing a model that includes group to one that contains only an intercept term. In scenario 2, there is little difference between the overall averages in each group, and more variability between individual observations within each group. In a scenario like this, knowing the group an observation lies in does little to help us predict the response. In this scenario, predictions from a model that includes group as an explantory variable would not be much better than those from a model that does not. Hence, we would obtain a small F-statistic. Scenario 1 Scenario 2 variation between groups High Low variation within groups Low High F Statistic Large Small Result Evidence of Group Differences No evidence of differences 2.5.5 Alternative F-Statistic Formula The above illustration suggests alternative (and mathematically equivalent) way to calculate the F-statistic. We calculate the ratio of variability between different groups, relative to the amount of variability within each group For a categorical variable with \\(g\\) groups, let \\(\\bar{y}_{1\\cdot}, \\ldots, \\bar{y}_{g\\cdot}\\) represent the mean response for each group. let \\(n_1, \\ldots, n_g\\) represent the sample size for each group Then \\(\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}n_i(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1}\\) gives a measure of how much the group means differ, and \\(\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g}\\) gives a measure of how much individual observations differ within groups An alternative formula for this F-statistic is: \\[ F= \\frac{\\text{Variability between groups}}{\\text{Variability within groups}}= \\frac{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}n_i(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1}}{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g}} \\] It can be shown that this statistic is equivalent to the one we saw previously. Example Let’s recalculate the F-statistic for the conditions of the houses, using this alternate formula. The first 3 houses are shown. kable(head(Houses %&gt;% select(Id, price, condition),3)) Id price condition 1 1225 average or below 2 885 average or below 3 385 good We have seen previously that: \\(\\bar{y}_{\\cdot\\cdot}=735.3526\\) (overall average price), and \\(n=10\\) \\(\\bar{y}_{1\\cdot}=700.6349\\) (average price for average or below houses), and \\(n_1=61\\) \\(\\bar{y}_{2\\cdot}=861.0\\) (average price for good houses), and \\(n_2=30\\) \\(\\bar{y}_{3\\cdot}=551.8361\\) (average price for very good houses), and \\(n_3=9\\) Then, \\(\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1} = \\frac{61(700.6349-735.3526)^2+30(861.0-735.3526)^2+9(551.8361-735.3526)^2}{3-1} = \\frac{850247.3}{2}\\), and \\(\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g} = \\frac{(1225.0-700.6349)^2+ (885.0 - 700.6349)^2 + (385.0-861.0)^2+\\ldots}{100-3} = \\frac{68195387}{97}\\) \\[ F= \\frac{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}n_i(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1}}{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g}} = \\frac{\\frac{61(700.6349-735.3526)^2+30(861.0-735.3526)^2+9(551.8361-735.3526)^2}{3-1}}{\\frac{(1225.0-700.6349)^2+ (885.0 - 700.6349)^2 + (385.0-861.0)^2+\\ldots}{100-3}} = \\frac{\\frac{850247.3}{2}}{\\frac{68195387}{97}} \\] Note that the quantity in the the quantity in the third line is equivalent to the sum of the squared residuals using M2. Thus, we can calculate F using: ((61*(700.6349-735.3526)^2+30*(861.0-735.3526)^2+9*(551.8361-735.3526)^2)/(3-1))/((SSR_cond)/(100-3)) ## [1] 0.6046889 For models with only one categorical explanatory variable, “variability within vs variability between” interpretation of an F-statistic is very popular. This statistic is often relevant in studies in the natural and social sciences. Such studies are often referred to as One-Way ANOVA’s. In fact, these are just a special case of the full vs reduced model interpretation of the F-statistic, which can be applied to any two models, as long as one is a submodel of the other. "],["hypothesis-testing-via-permutation.html", "Chapter 3 Hypothesis Testing via Permutation 3.1 Test for Difference in Means 3.2 General Permutation Tests 3.3 Responsible Hypothesis Testing", " Chapter 3 Hypothesis Testing via Permutation Learning Outcomes: State null and alternative hypotheses associated with models involving categorical and quantitative explanatory variables. Explain how to use permutation tests for hypotheses involving means, medians, F-statistics, slopes, proportions, and other regression coefficients, as well as functions of these statistics. Interpret p-values in context. Explain the conclusions we should draw from from a hypothesis test, while accounting for other information available in a dataset. Distinguish between statistical significance and practical importance. 3.1 Test for Difference in Means 3.1.1 Mercury Levels in Florida Lakes A 2004 study by Lange, T., Royals, H. and Connor, L. examined Mercury accumulation in large-mouth bass, taken from a sample of 53 Florida Lakes. If Mercury accumulation exceeds 0.5 ppm, then there are environmental concerns. In fact, the legal safety limit in Canada is 0.5 ppm, although it is 1 ppm in the United States. In our sample, we have data on 53 lakes, out of more than 30,000 lakes in the the state of Florida. We’ll attempt to draw conclusions about the entire population, consisting of all lakes in Florida, using data from our sample of 53. It is not clear how the lakes in this sample of 53 were selected, or how representative they are of all lakes in the state of Florida. Let’s assume for our purposes that the lakes in the sample can be reasonably thought of as being representative of all lakes in Florida. Figure 3.1: https://www.maine.gov/ifw/fish-wildlife/fisheries/species-information/largemouth-bass.html data(&quot;FloridaLakes&quot;) glimpse(FloridaLakes) ## Rows: 53 ## Columns: 12 ## $ ID &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1… ## $ Lake &lt;chr&gt; &quot;Alligator&quot;, &quot;Annie&quot;, &quot;Apopka&quot;, &quot;Blue Cypress&quot;, &quot;Bri… ## $ Alkalinity &lt;dbl&gt; 5.9, 3.5, 116.0, 39.4, 2.5, 19.6, 5.2, 71.4, 26.4, 4… ## $ pH &lt;dbl&gt; 6.1, 5.1, 9.1, 6.9, 4.6, 7.3, 5.4, 8.1, 5.8, 6.4, 5.… ## $ Calcium &lt;dbl&gt; 3.0, 1.9, 44.1, 16.4, 2.9, 4.5, 2.8, 55.2, 9.2, 4.6,… ## $ Chlorophyll &lt;dbl&gt; 0.7, 3.2, 128.3, 3.5, 1.8, 44.1, 3.4, 33.7, 1.6, 22.… ## $ AvgMercury &lt;dbl&gt; 1.23, 1.33, 0.04, 0.44, 1.20, 0.27, 0.48, 0.19, 0.83… ## $ NumSamples &lt;int&gt; 5, 7, 6, 12, 12, 14, 10, 12, 24, 12, 12, 12, 7, 43, … ## $ MinMercury &lt;dbl&gt; 0.85, 0.92, 0.04, 0.13, 0.69, 0.04, 0.30, 0.08, 0.26… ## $ MaxMercury &lt;dbl&gt; 1.43, 1.90, 0.06, 0.84, 1.50, 0.48, 0.72, 0.38, 1.40… ## $ ThreeYrStdMercury &lt;dbl&gt; 1.53, 1.33, 0.04, 0.44, 1.33, 0.25, 0.45, 0.16, 0.72… ## $ AgeData &lt;int&gt; 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1… We are interested in whether mercury levels are higher or lower, on average, in Northern Florida compared to Southern Florida. We’ll divide the state along route 50, which runs East-West, passing through Northern Orlando. Figure 3.2: from Google Maps We add a variable indicating whether each lake lies in the northern or southern part of the state. library(Lock5Data) data(FloridaLakes) #Location relative to rt. 50 FloridaLakes$Location &lt;- as.factor(c(&quot;S&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;S&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;S&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;S&quot;,&quot;S&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;)) FloridaLakes &lt;- FloridaLakes %&gt;% rename(Mercury = AvgMercury) print.data.frame(data.frame(FloridaLakes%&gt;% select(Lake, Location, Mercury)), row.names = FALSE) ## Lake Location Mercury ## Alligator S 1.23 ## Annie S 1.33 ## Apopka N 0.04 ## Blue Cypress S 0.44 ## Brick S 1.20 ## Bryant N 0.27 ## Cherry N 0.48 ## Crescent N 0.19 ## Deer Point N 0.83 ## Dias N 0.81 ## Dorr N 0.71 ## Down S 0.50 ## Eaton N 0.49 ## East Tohopekaliga S 1.16 ## Farm-13 N 0.05 ## George N 0.15 ## Griffin N 0.19 ## Harney N 0.77 ## Hart S 1.08 ## Hatchineha S 0.98 ## Iamonia N 0.63 ## Istokpoga S 0.56 ## Jackson N 0.41 ## Josephine S 0.73 ## Kingsley N 0.34 ## Kissimmee S 0.59 ## Lochloosa N 0.34 ## Louisa S 0.84 ## Miccasukee N 0.50 ## Minneola N 0.34 ## Monroe N 0.28 ## Newmans N 0.34 ## Ocean Pond N 0.87 ## Ocheese Pond N 0.56 ## Okeechobee S 0.17 ## Orange N 0.18 ## Panasoffkee N 0.19 ## Parker S 0.04 ## Placid S 0.49 ## Puzzle N 1.10 ## Rodman N 0.16 ## Rousseau N 0.10 ## Sampson N 0.48 ## Shipp S 0.21 ## Talquin N 0.86 ## Tarpon S 0.52 ## Tohopekaliga S 0.65 ## Trafford S 0.27 ## Trout S 0.94 ## Tsala Apopka N 0.40 ## Weir N 0.43 ## Wildcat N 0.25 ## Yale N 0.27 We are interested in investigating whether average mercury levels are higher in either Northern Florida or Southern Florida than the other. The boxplot and table below show the distribution of mercury levels among the 33 northern and 20 southern lakes in the sample. LakesBP &lt;- ggplot(data=FloridaLakes, aes(x=Location, y=Mercury, fill=Location)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip() LakesBP LakesTable &lt;- FloridaLakes %&gt;% group_by(Location) %&gt;% summarize(MeanHg=mean(Mercury), StDevHg=sd(Mercury), N=n()) kable(LakesTable) Location MeanHg StDevHg N N 0.4245455 0.2696652 33 S 0.6965000 0.3838760 20 We see that on average mercury levels were higher among the southern lakes than the northern ones, a difference of \\(0.697-0.445= 0.272\\) ppm. 3.1.2 Model for Mercury Level We can use a statistical model to estimate a lake’s mercury level, using its location (N or S) as our explanatory variable. The model equation is \\(\\widehat{\\text{Hg}} = b_0 +b_1\\times\\text{South}\\) \\(b_0\\) represents the mean mercury level for lakes in North Florida, and \\(b_1\\) represents the mean difference in mercury level for lakes in South Florida, compared to North Florida Fitting the model in R, we obtain the estimates for \\(b_0\\) and \\(b_1\\). Lakes_M &lt;- lm(data=FloridaLakes, Mercury ~ Location) Lakes_M ## ## Call: ## lm(formula = Mercury ~ Location, data = FloridaLakes) ## ## Coefficients: ## (Intercept) LocationS ## 0.4245 0.2720 \\(\\widehat{\\text{Hg}} = 0.4245455 +0.2719545\\times\\text{South}\\) \\(b_1 = 0.272= 0.6965 - 0.4245\\) is equal to the difference in mean mercury levels between Northern and Southern lakes. (We’ve already seen that for categorical variables, the least-squares estimate is the mean, so this makes sense.) We can use \\(b_1\\) to assess the size of the difference in mean mercury concentration levels. 3.1.3 Hypotheses and Key Question Since the lakes we observed are only a sample of 53 lakes out of more than 30,000, we cannot assume the difference in mercury concentration for all Northern vs Southern Florida lakes is exactly 0.272. Instead, we need to determine whether a difference of this size in our sample is large enough to provide evidence of a difference in average mercury level between all Northern and Southern lakes in Florida. One possible explanation for us getting the results we did in our sample is that there really is no difference in average mercury levels between all lakes in Northern and Southern Florida, and we just happened, by chance, to select more lakes with higher mercury concentrations in Southern Florida than in Northern Florida. A different possible explanation is that there really is a difference in average mercury level between lakes in Northern and Southern Florida. In a statistical investigation, the null hypothesis is the one that says there is no difference between groups , or no relationship between variables in the larger population, and that any difference/relationship observed in our sample occurred merely by chance. The alternative hypothesis contradicts the null hypothesis, stating that there is a difference/relationship. Stated formally, the hypotheses are: Null Hypothesis: There is no difference in average mercury level between all lakes in Northern Florida and all lakes in Southern Florida. Alternative Hypothesis: There is a difference in average mercury level between all lakes in Northern Florida and all lakes in Southern Florida. A statistician’s job is to determine whether the data provide strong enough evidence to rule out the null hypothesis. The question we need to investigate is: “How likely is it that we would have observed a difference in means (i.e. a value of \\(b_1\\)) as extreme as 0.6965-0.4245 = 0.272 ppm, merely by chance, if there is really no relationship between location and mercury level?” 3.1.4 Permutation Test for Difference in Means We can answer the key question using a procedure known as a permutation test. In a permutation test, we randomly permute (or shuffle) the values of our explanatory variable to simulate a situation where there is no relationship between our explanatory and response variable. We observe whether it is plausible to observe values of a statistic (in this case the difference in means) as extreme or more extreme than what we saw in the actual data. We’ll simulate situations where there is no relationship between location and mercury level, and see how often we observe a difference in means (\\(b_1\\)) as extreme as 0.272. Procedure: Randomly shuffle the locations of the lakes, so that any relationship between location and mercury level is due only to chance. Calculate the difference in mean mercury levels (i.e. value of \\(b_1\\)) in “Northern” and “Southern” lakes, using the shuffled data. The statistic used to measure the size of the difference or relationship in the sample is called the test statistic. Repeat steps 1 and 2 many (say 10,000) times, recording the test statistic (difference in means, \\(b_1\\)) each time. Analyze the distribution of the test statistic (mean difference), simulated under the assumption that there is no relationship between location and mercury level. Look whether the value of the test statistic we observed in the sample (0.272) is consistent with values simulated under the assumption that the null hypothesis is true. This simulation can be performed using this Rossman-Chance App. 3.1.5 Five Permutations in R We’ll use R to perform permutation test. First Permutation Recall these groups were randomly assigned, so the only differences in averages are due to random chance. ShuffledLakes &lt;- FloridaLakes # create copy of dataset ShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] Shuffle1df &lt;- data.frame(FloridaLakes$Lake, FloridaLakes$Location, FloridaLakes$Mercury, ShuffledLakes$Location) names(Shuffle1df) &lt;- c(&quot;Lake&quot;, &quot;Location&quot;, &quot;Mercury&quot;, &quot;Shuffled Location&quot;) kable(head(Shuffle1df)) Lake Location Mercury Shuffled Location Alligator S 1.23 N Annie S 1.33 S Apopka N 0.04 S Blue Cypress S 0.44 N Brick S 1.20 N Bryant N 0.27 N Notice that the locations of the lakes have now been mixed up and assigned randomly. So, any relationship between location and mercury level will have occurred merely by chance. We create a boxplot and calculate the difference in mean mercury levels for the shuffled data. LakesPerm &lt;- ggplot(data=Shuffle1df, aes(x=`Shuffled Location`, y=Mercury, fill=`Shuffled Location`)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip() LakesPerm LakesPermTable &lt;- Shuffle1df %&gt;% group_by(`Shuffled Location`) %&gt;% summarize(MeanHg=mean(Mercury), StDevHg=sd(Mercury), N=n()) kable(LakesPermTable) Shuffled Location MeanHg StDevHg N N 0.5460606 0.3552986 33 S 0.4960000 0.3225784 20 Notice that the sample means are not identical. We observe a difference of 0.0500606 just by chance associated with the assignment of the lakes to their random location groups. This difference is considerably smaller than the difference of 0.272 that we saw in the actual data, suggesting that perhaps a difference as big as 0.272 would not be likely to occur by chance. Before we can be sure of this, however, we should repeat our simulation many times to get a better sense for how big of a difference we might reasonable expect to occur just by chance. Second Permutation ShuffledLakes &lt;- FloridaLakes ## create copy of dataset ShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] kable(head(Shuffle1df)) Lake Location Mercury Shuffled Location Alligator S 1.23 N Annie S 1.33 S Apopka N 0.04 S Blue Cypress S 0.44 N Brick S 1.20 N Bryant N 0.27 N Shuffle1df &lt;- data.frame(FloridaLakes$Lake, FloridaLakes$Location, FloridaLakes$Mercury, ShuffledLakes$Location) names(Shuffle1df) &lt;- c(&quot;Lake&quot;, &quot;Location&quot;, &quot;Mercury&quot;, &quot;Shuffled Location&quot;) LakesPerm &lt;- ggplot(data=Shuffle1df, aes(x=`Shuffled Location`, y=Mercury, fill=`Shuffled Location`)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip() LakesPerm LakesPermTable &lt;- Shuffle1df %&gt;% group_by(`Shuffled Location`) %&gt;% summarize(MeanHg=mean(Mercury), StDevHg=sd(Mercury), N=n()) kable(LakesPermTable) Shuffled Location MeanHg StDevHg N N 0.4839394 0.3316431 33 S 0.5985000 0.3527975 20 Third Permutation ShuffledLakes &lt;- FloridaLakes ## create copy of dataset ShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] kable(head(Shuffle1df)) Lake Location Mercury Shuffled Location Alligator S 1.23 S Annie S 1.33 S Apopka N 0.04 N Blue Cypress S 0.44 S Brick S 1.20 N Bryant N 0.27 S Shuffle1df &lt;- data.frame(FloridaLakes$Lake, FloridaLakes$Location, FloridaLakes$Mercury, ShuffledLakes$Location) names(Shuffle1df) &lt;- c(&quot;Lake&quot;, &quot;Location&quot;, &quot;Mercury&quot;, &quot;Shuffled Location&quot;) LakesPerm &lt;- ggplot(data=Shuffle1df, aes(x=`Shuffled Location`, y=Mercury, fill=`Shuffled Location`)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip() LakesPerm LakesPermTable &lt;- Shuffle1df %&gt;% group_by(`Shuffled Location`) %&gt;% summarize(MeanHg=mean(Mercury), StDevHg=sd(Mercury), N=n()) kable(LakesPermTable) Shuffled Location MeanHg StDevHg N N 0.5324242 0.3863777 33 S 0.5185000 0.2583607 20 Fourth Permutation ShuffledLakes &lt;- FloridaLakes ## create copy of dataset ShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] kable(head(Shuffle1df)) Lake Location Mercury Shuffled Location Alligator S 1.23 N Annie S 1.33 N Apopka N 0.04 N Blue Cypress S 0.44 S Brick S 1.20 N Bryant N 0.27 S Shuffle1df &lt;- data.frame(FloridaLakes$Lake, FloridaLakes$Location, FloridaLakes$Mercury, ShuffledLakes$Location) names(Shuffle1df) &lt;- c(&quot;Lake&quot;, &quot;Location&quot;, &quot;Mercury&quot;, &quot;Shuffled Location&quot;) LakesPerm &lt;- ggplot(data=Shuffle1df, aes(x=`Shuffled Location`, y=Mercury, fill=`Shuffled Location`)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip() LakesPerm LakesPermTable &lt;- Shuffle1df %&gt;% group_by(`Shuffled Location`) %&gt;% summarize(MeanHg=mean(Mercury), StDevHg=sd(Mercury), N=n()) kable(LakesPermTable) Shuffled Location MeanHg StDevHg N N 0.4833333 0.3314803 33 S 0.5995000 0.3527109 20 Fifth Permutation ShuffledLakes &lt;- FloridaLakes ## create copy of dataset ShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] kable(head(Shuffle1df)) Lake Location Mercury Shuffled Location Alligator S 1.23 N Annie S 1.33 S Apopka N 0.04 N Blue Cypress S 0.44 N Brick S 1.20 N Bryant N 0.27 S Shuffle1df &lt;- data.frame(FloridaLakes$Lake, FloridaLakes$Location, FloridaLakes$Mercury, ShuffledLakes$Location) names(Shuffle1df) &lt;- c(&quot;Lake&quot;, &quot;Location&quot;, &quot;Mercury&quot;, &quot;Shuffled Location&quot;) LakesPerm &lt;- ggplot(data=Shuffle1df, aes(x=`Shuffled Location`, y=Mercury, fill=`Shuffled Location`)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip() LakesPerm LakesPermTable &lt;- Shuffle1df %&gt;% group_by(`Shuffled Location`) %&gt;% summarize(MeanHg=mean(Mercury), StDevHg=sd(Mercury), N=n()) kable(LakesPermTable) Shuffled Location MeanHg StDevHg N N 0.5642424 0.3259508 33 S 0.4660000 0.3647551 20 3.1.6 R Code for Permutation Test We’ll write a for loop to perform 10,000 permutations and record the value of \\(b_1\\) (the difference in sample means) for each simulation. b1 &lt;- Lakes_M$coef[2] ## record value of b1 from actual data ## perform simulation b1Sim &lt;- rep(NA, 10000) ## vector to hold results ShuffledLakes &lt;- FloridaLakes ## create copy of dataset for (i in 1:10000){ #randomly shuffle locations ShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] ShuffledLakes_M&lt;- lm(data=ShuffledLakes, Mercury ~ Location) #fit model to shuffled data b1Sim[i] &lt;- ShuffledLakes_M$coef[2] ## record b1 from shuffled model } NSLakes_SimulationResults &lt;- data.frame(b1Sim) #save results in dataframe The histogram shows the distribution of differences in the group means observed in our simulation. The red lines indicate the difference we actually observed in the data (0.272), as well as an equally large difference in the opposite direction (-0.272). NSLakes_SimulationResultsPlot &lt;- ggplot(data=NSLakes_SimulationResults, aes(x=b1Sim)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + geom_vline(xintercept=c(b1, -1*b1), color=&quot;red&quot;) + xlab(&quot;Lakes: Simulated Value of b1&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Distribution of b1 under assumption of no relationship&quot;) NSLakes_SimulationResultsPlot The red lines are quite extreme, relative to the simulated values shown in the histogram. Based on the simulation, it is rare to obtain a difference as extreme as the 0.272 value we saw in the actual data, by chance when there is actually no difference in average mercury levels between Northern and Southern Florida lakes. We calculate the precise number of simulations (out of 10,000) resulting in difference in means more extreme than 0.27195. sum(abs(b1Sim) &gt; abs(b1)) ## [1] 39 The proportion of simulations resulting in difference in means more extreme than 0.272 is: sum(abs(b1Sim) &gt; abs(b1))/10000 ## [1] 0.0039 We only observed a difference between the groups as extreme or more extreme than the 0.272 difference we saw in the sample in a proportion of 0.0039 of our simulations (less than 1%). It is very unlikely that we would have gotten a difference as extreme as 0.272 by chance when there is no relationship between location and mercury level is very low. Thus, we have strong evidence that there is a difference in average mercury level between lakes in Northern and Southern Florida. In this case, there is strong evidence that mercury level is higher in Southern Florida lakes than Northern Florida lakes. 3.1.7 p-values The p-value represents the probability of getting a test statistic as extreme or more extreme than we did in our sample when the null hypothesis is true. In this situation, the p-value represents the probability of observing a difference in sample means as extreme or more extreme than 0.272 if there is actually no difference in average mercury level among all lakes in Northern Florida, compared to Southern Florida. In our study, the p-value was 0.0039, which is very low. This provides strong evidence against the null hypothesis that there is no difference in average mercury levels between all Northern and Southern Florida lakes. A low p-value tells us that the difference in average Mercury levels that we saw in our sample is unlikely to have occurred by chance, providing evidence that there is indeed a difference in average Mercury levels between Northern and Southern lakes. The p-value does not tell us anything about the size of the difference! If the difference is really small (say 0.001 ppm), perhaps there is no need to worry about it. It is possible to get a small p-value even when the true difference is very small (especially when our sample size is large). In addition to a p-value, we should consider whether a difference is big enough to be meaningful in a practical way, before making any policy decisions. For now, we can use the difference in sample means of 0.272 ppm as an estimate of the size of the difference. Based on our limited knowledge of mercury levels, this does seem big enough to merit further investigation, and possible action. At this point, a reasonable question is “how small must a p-value be in order to provide evidence against the null hypothesis?” While it is sometimes common to establish strict cutoffs for what counts as a small p-value (such as \\(&lt;0.05\\)), the American Statistical Association does not recommend this. In reality, a p-value of 0.04 is practically no different than a p-value of 0.06. Rather than using strict cutoffs for what counts as small, it is better to interpreting p-values on a sliding scale, as illustrated in the diagram below. A p-value of 0.10 or less provides at least some evidence against a null hypothesis, and the smaller the p-value is, the stronger the evidence gets. knitr::include_graphics(&quot;pvals.png&quot;) 3.2 General Permutation Tests 3.2.1 Other Test Statistics The permutation test procedure can be used to test hypotheses involving lots of different test statistics, in addition to testing for a difference in means, as we saw seen in the previous section. For example we could test whether there is evidence of: a difference in the median mercury level between lakes in Northern Florida, compared to southern Florida a difference in the amount of variability in mercury levels between lakes in Northern Florida, compared to southern Florida a difference in the proportion of lakes whose mercury level exceeds 1 ppm between lakes in Northern Florida, compared to southern Florida a difference in mean price between King County houses in very good, good, and average or below conditions a relationship between mercury level and pH level among all Florida lakes For each of these investigations, the null hypothesis will be that there is no difference or relationship among all lakes (that is, whatever difference or relationship occurred in the sample occurred just by random chance). We’ll need to find a test statistic that measures the quantity we’re interested in (for example, difference in means). Then, we use the permutation procedure to simulate a scenario where our null hypothesis is true, and see if test statistic we saw in our data is consistent with the ones we simulate under the null hypothesis. 3.2.2 General Permutation Test Procedure Procedure: Randomly shuffle the values or categories of the explanatory variable, so that any relationship between the explanatory and response variable occurs just by chance. Calculate the test statistic on the shuffled data. Repeat steps 1 and 2 many (say 10,000) times, recording the test statistic each time. Analyze the distribution of the test statistic, simulated under the assumption that the null hypothesis is true. Look whether the value of the test statistic we observed in the sample is consistent with values simulated under the assumption that the null hypothesis is true. (We might calculate a p-value, which represents the proportion of simulations in which we observed a test statistic as extreme or more extreme than the one we saw in our actual sample.) Next, we’ll apply these steps to questions 2, 4, and 5 from the previous subsection. 3.2.3 Difference in Standard Deviation We’ll test whether there is evidence of a difference in variability between lakes in Northern Florida, compared to Southern Florida. Since standard deviation is a measure of variability, we’ll use the difference in standard deviation in Northern vs Southern lakes as our test statistic. Recall that the standard deviation among the 53 Northern Florida Lakes in our sample was 0.270 ppm, which is lower than the 0.384 ppm in Southern Florida. kable(LakesTable) Location MeanHg StDevHg N N 0.4245455 0.2696652 33 S 0.6965000 0.3838760 20 The test statistic we observe in our sample is \\(0.2696-0.3839 = -0.1142\\) ppm. We need to determine whether a difference this large could have plausibly occurred in our sample, just by chance, if there is really no difference in standard deviation among all lakes in Northern Florida, compared to Southern Florida. Null Hypothesis: There is no difference in standard deviation of mercury levels between all lakes in Northern Florida and all lakes in Southern Florida. Alternative Hypothesis: There is a difference in standard deviation of mercury levels between all lakes in Northern Florida and all lakes in Southern Florida. We’ll apply the general hypothesis testing procedure, using standard deviation as our test statistic. Procedure: Randomly shuffle the locations of the lakes, so that any relationship between the location and mercury level occurs just by chance. Calculate the difference in standard deviation between lakes in the two samples of the shuffled data. Repeat steps 1 and 2 many (say 10,000) times, recording the difference in standard deviations each time. Analyze the distribution of difference in standard deviations, simulated under the assumption that there is no difference in standard deviations between North and South. Look whether the value of the test statistic we observed in the sample is consistent with values simulated under the assumption that there is no difference in standard deviations. R Code for Permutation Test We’ll write a for loop to perform 10,000 permutations and record the value of \\(b_1\\) (the difference in sample means) for each simulation. SDTab &lt;- FloridaLakes %&gt;% group_by(Location) %&gt;% summarize(SD=sd(Mercury)) DiffSD &lt;- SDTab$SD[2] - SDTab$SD[1] ## perform simulation DiffSim &lt;- rep(NA, 10000) ## vector to hold results ShuffledLakes &lt;- FloridaLakes ## create copy of dataset for (i in 1:10000){ #randomly shuffle locations ShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] SDTabSim &lt;- ShuffledLakes %&gt;% group_by(Location) %&gt;% summarize(SD=sd(Mercury)) DiffSim[i] &lt;- SDTabSim$SD[2] - SDTabSim$SD[1] #record difference in SD for simulated data } NSLakes_SDSimResults &lt;- data.frame(DiffSim) #save results in dataframe The distribution of the simulated differences in standard deviation is shown below. Recall that these were simulated assuming that the null hypothesis, that there is no difference in standard deviation of mercury levels among all lakes in Northern Florida, compared to Southern Florida is true. The red lines represent differences as extreme as -0.1142 that we saw in our sample. NSLakes_SDSimResultsPlot &lt;- ggplot(data=NSLakes_SDSimResults, aes(x=DiffSim)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + geom_vline(xintercept=c(DiffSD, -1*DiffSD), color=&quot;red&quot;) + xlab(&quot;Simulated Difference in SD&#39;s&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Distribution of Difference in SD under assumption of no relationship&quot;) NSLakes_SDSimResultsPlot We calculate the number of simulations (out of 10,000) resulting in standard deviations greater the 0.1142. sum(abs(DiffSim) &gt; abs(DiffSD)) ## [1] 612 p-value: Proportion of simulations (out of 10,000) resulting in difference in standard deviations greater the 0.1142. mean(abs(DiffSim) &gt; abs(DiffSD)) ## [1] 0.0612 This p-value represents the probability of observing a difference in sample standard deviations as extreme as 0.1142 in a samples of size 33 and 20 by chance, if in fact, the standard deviation in mercury concentration levels is the same for lakes in Northern Florida as in Southern Florida. Since the p-value is small, it is unlikely that we would observe a difference in standard deviations as extreme as 0.1142 by chance. There is evidence that lakes in Southern Florida exhibit more variability in mercury levels than lakes in Northern Florida (though the evidence is not as strong as it was when we were testing for a difference in means). Note that we have avoided the fallacy of using 0.05 as a strict cutoff for rejecting the null hypothesis. Although the difference in standard deviations is statistically discernible, it is hard to say whether it is practically meaningful. Without knowing a lot about mercury levels, and their impact on the ecosystem, it’s harder to tell whether an estimated difference in standard deviations of 0.11 ppm is meaningful or not. It would be good to consult a biologist before making any decisions based on these results. 3.2.4 Permutation Test for Slope In addition to the mercury levels of the Florida lakes, we have data on the pH level of each lake. pH level measures the acidity of a lake, ranging from 0 to 14, with 7 being neutral, and lower levels indicating more acidity. We plot the pH level against the mercury level in our sample of 53 lakes. ggplot(data=FloridaLakes, aes(y=Mercury, x=pH)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) + xlim(c(3, 10)) + ylim(c(0,1.5)) The regression equation is \\[ \\widehat{\\text{Mercury}} = b_0 + b_1\\times\\text{pH} \\] Regression estimates \\(b_0\\) and \\(b_1\\) are shown below. Lakes_M_pH &lt;- lm(data=FloridaLakes, Mercury~pH) Lakes_M_pH ## ## Call: ## lm(formula = Mercury ~ pH, data = FloridaLakes) ## ## Coefficients: ## (Intercept) pH ## 1.5309 -0.1523 We can use the slope of the regression line \\(b_1\\) to measure the strength relationship between Mercury and pH. Based on our sample, each one-unit increase in pH, mercury level is expected to decrease by 0.15 ppm. If there was really no relationship, then the slope among all lakes would be 0. But, of course, we would not expect the slope in our sample to exactly match the slope for all lakes. Our question of interest is whether it is plausible that we could have randomly selected a sample resulting in a slope as extreme as 0.15 by chance, when there is actually no relationship between mercury and pH levels, among all lakes. In other words, could we plausible have drawn the sample of 53 lakes shown in blue from a population like the one in red, shown below? Key Question: How likely is it that we would have observed a slope (i.e. a value of \\(b_1\\)) as extreme as 0.15 by chance, if there is really no relationship between mercury level and pH? Null Hypothesis: Among all Florida lakes, there is no relationship between mercury level and pH. Alternative Hypothesis: Among all Florida lakes, there is a relationship between mercury level and pH. Procedure: Randomly shuffle the pH values, so that any relationship between acceleration mercury and pH is due only to chance. Fit a regression line to the shuffled data and record the slope of the regression line. Repeat steps 1 and 2 many (say 10,000) times, recording the slope (i.e. value of \\(b_1\\)) each time. Analyze the distribution of slopes, simulated under the assumption that there is no relationship between mercury and pH. Look whether the actual slope we observed is consistent with the simulation results. We’ll illustrate the first three permutations. First Permutation ShuffledLakes &lt;- FloridaLakes ## create copy of dataset ShuffledLakes$pH &lt;- ShuffledLakes$pH[sample(1:nrow(ShuffledLakes))] Shuffle1df &lt;- data.frame(ShuffledLakes$Lake, FloridaLakes$Mercury, FloridaLakes$pH, ShuffledLakes$pH) names(Shuffle1df) &lt;- c(&quot;Lake&quot;, &quot;Mercury&quot;, &quot;pH&quot;, &quot;Shuffled_pH&quot;) kable(head(Shuffle1df)) Lake Mercury pH Shuffled_pH Alligator 1.23 6.1 8.3 Annie 1.33 5.1 7.8 Apopka 0.04 9.1 7.2 Blue Cypress 0.44 6.9 6.7 Brick 1.20 4.6 7.6 Bryant 0.27 7.3 3.6 The red line indicates the slope of the regression line fit to the shuffled data. The blue line indicates the regression line for the actual lakes in the sampe, which has a slope of -0.15. ggplot(data=Shuffle1df, aes(x=Shuffled_pH, y=Mercury)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE, color=&quot;red&quot;) + xlim(c(3, 10)) + ylim(c(0,1.5)) + geom_abline(slope=-0.1523, intercept=1.5309, color=&quot;blue&quot;) Slope of regression line from permuted data: M_Lakes_Shuffle &lt;- lm(data=Shuffle1df, Mercury~Shuffled_pH) summary(M_Lakes_Shuffle)$coef[2] ## [1] 0.03177036 Second Permutation ShuffledLakes &lt;- FloridaLakes ## create copy of dataset ShuffledLakes$pH &lt;- ShuffledLakes$pH[sample(1:nrow(ShuffledLakes))] Shuffle2df &lt;- data.frame(ShuffledLakes$Lake, FloridaLakes$Mercury, FloridaLakes$pH, ShuffledLakes$pH) names(Shuffle2df) &lt;- c(&quot;Lake&quot;, &quot;Mercury&quot;, &quot;pH&quot;, &quot;Shuffled_pH&quot;) kable(head(Shuffle2df)) Lake Mercury pH Shuffled_pH Alligator 1.23 6.1 7.9 Annie 1.33 5.1 8.3 Apopka 0.04 9.1 5.2 Blue Cypress 0.44 6.9 4.6 Brick 1.20 4.6 6.1 Bryant 0.27 7.3 9.1 ggplot(data=Shuffle2df, aes(x=Shuffled_pH, y=Mercury)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE, color=&quot;red&quot;) + xlim(c(3, 10)) + ylim(c(0,1.5)) + geom_abline(slope=-0.1523, intercept=1.5309, color=&quot;blue&quot;) Slope of regression line from permuted data: M_Lakes_Shuffle &lt;- lm(data=Shuffle2df, Mercury~Shuffled_pH) summary(M_Lakes_Shuffle)$coef[2] ## [1] 0.04657482 ShuffledLakes &lt;- FloridaLakes ## create copy of dataset ShuffledLakes$pH &lt;- ShuffledLakes$pH[sample(1:nrow(ShuffledLakes))] Shuffle3df &lt;- data.frame(ShuffledLakes$Lake, FloridaLakes$Mercury, FloridaLakes$pH, ShuffledLakes$pH) names(Shuffle3df) &lt;- c(&quot;Lake&quot;, &quot;Mercury&quot;, &quot;pH&quot;, &quot;Shuffled_pH&quot;) kable(head(Shuffle3df)) Lake Mercury pH Shuffled_pH Alligator 1.23 6.1 6.9 Annie 1.33 5.1 7.1 Apopka 0.04 9.1 7.2 Blue Cypress 0.44 6.9 7.9 Brick 1.20 4.6 7.5 Bryant 0.27 7.3 6.8 ggplot(data=Shuffle3df, aes(x=Shuffled_pH, y=Mercury)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE, color=&quot;red&quot;) + xlim(c(3, 10)) + ylim(c(0,1.5)) + geom_abline(slope=-0.1523, intercept=1.5309, color=&quot;blue&quot;) Slope of regression line from permuted data: M_Lakes_Shuffle &lt;- lm(data=Shuffle3df, Mercury~Shuffled_pH) summary(M_Lakes_Shuffle)$coef[2] ## [1] 0.05767238 None of our three simulations resulted in a slope near as extreme as the -0.15 that we saw in the actual data. This seems to suggest that it is unlikely that we would have observed a slope as extreme as -0.15 if there is actually no relationship between mercury and pH among all lakes. That said, we should repeat the simulation many more times to see whether getting a slope as extreme as -0.15 is plausible. b1 &lt;- Lakes_M_pH$coef[2] ## record value of b1 from actual data ## perform simulation b1Sim &lt;- rep(NA, 10000) ## vector to hold results ShuffledLakes &lt;- FloridaLakes ## create copy of dataset for (i in 1:10000){ #randomly shuffle acceleration times ShuffledLakes$pH &lt;- ShuffledLakes$pH[sample(1:nrow(ShuffledLakes))] ShuffledLakes_M&lt;- lm(data=ShuffledLakes, Mercury ~ pH) #fit model to shuffled data b1Sim[i] &lt;- ShuffledLakes_M$coef[2] ## record b1 from shuffled model } Lakes_pHSimulationResults &lt;- data.frame(b1Sim) #save results in dataframe b1 &lt;- Lakes_M_pH$coef[2] ## record value of b1 from actual data Lakes_pHSimulationResultsPlot &lt;- ggplot(data=Lakes_pHSimulationResults, aes(x=b1Sim)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + geom_vline(xintercept=c(b1, -1*b1), color=&quot;red&quot;) + xlab(&quot;Simulated Value of b1&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Distribution of b1 under assumption of no relationship&quot;) Lakes_pHSimulationResultsPlot p-value: Proportion of simulations resulting in value of \\(b_1\\) more extreme than -0.15 mean(abs(b1Sim) &gt; abs(b1)) ## [1] 0 The p-value represents the probability of observing a slope as extreme or more extreme than -0.15 by chance when there is actually no relationship between mercury level and pH. It is extremely unlikely that we would observe a value of \\(b_1\\) as extreme as -0.15 by chance, if there is really no relationship between mercury level and pH. In fact, this never happened in any of our 10,000 simulations! There is very strong evidence of a relationship mercury level and pH. A low p-value tells us only that there is evidence of a relationship, not that it is practically meaningful. We have seen that for each one-unit increase in pH, mercury level is expected to decrease by 0.15 ppm on average, which seems like a pretty meaningful decrease, especially considering that mercury levels typically stay between 0 and 1. We used the slope as our test statistic to measure the evidence of the relationship between the explanatory and response variables. In fact, we could have also used the correlation coefficient \\(r\\) as our test statistic, and we would have gotten the same p-value. Either slope or correlation may be used for a hypothesis test involving two quantitative variables, but we will use slope in this class. 3.2.5 F-Statistic Recall when we examined the prices of houses in King County, WA, whose conditions were rated as either very good, good, or average or below. Suppose we want to test the hypotheses: Null Hypothesis: There is no difference in average prices between houses of the three different conditions, among all houses in King County, WA. Alternative Hypothesis: There is a difference in average prices between houses of the three different conditions, among all houses in King County, WA. Comparative boxplots are shown below. ggplot(data=Houses, aes(x=condition, y=price, fill=condition)) + geom_boxplot(outlier.shape = NA) + geom_jitter(alpha=0.5) + coord_flip() + ggtitle(&quot;Houses&quot;) Cond_Tab &lt;- Houses %&gt;% group_by(condition) %&gt;% summarize(Mean_Price = mean(price), SD_Price= sd (price), N= n()) kable(Cond_Tab) condition Mean_Price SD_Price N average or below 700.6349 768.1179 61 good 861.0000 1048.9521 30 very_good 551.8361 332.8597 9 We notice differences in price. Surprisingly, houses in good condition cost more than 300 thousand dollars more than those in very good condition, on average. If we were only comparing two groups, we could use the difference in average price between them as a test statistic. But since we’re comparing three, we need a statistic that can measure the size of differences between all three groups. An F-statistic can do this, so we’ll use the F-statistic as our test statistic here. We calculated the F-statistic in the previous chapter. anova(M_House_Cond, M0_House)$F[2] ## [1] 0.6046888 Our question of interest is “How likely is it to observe an F-statistic as extreme or more extreme than 0.605 if there is actually no difference in average price between houses of the three conditions?” We’ll use a permutation-based hypothesis test to investigate this question. Procedure: Randomly shuffle the conditions of the houses, so that any relationship between condition and price is due only to chance. Using the shuffled data, calculate an F-statistic for a predicting price, comparing a full model that uses condition as an explanatory variable, to a reduced model with no explanatory variables. Repeat steps 1 and 2 many (say 10,000) times, recording the F-statistic each time. Analyze the distribution of F-statistics, simulated under the assumption that there is no relationship between condition and price. Look whether the actual F-statistic we observed is consistent with the simulation results. We’ll illustrate the first three permutations. First Permutation ShuffledHouses &lt;- Houses ## create copy of dataset ShuffledHouses$condition &lt;- ShuffledHouses$condition[sample(1:nrow(ShuffledHouses))] Shuffle1df &lt;- data.frame(Houses$Id, Houses$price, Houses$condition, ShuffledHouses$condition) names(Shuffle1df) &lt;- c(&quot;Id&quot;, &quot;price&quot;, &quot;condition&quot;, &quot;Shuffled_Condition&quot;) kable(head(Shuffle1df)) Id price condition Shuffled_Condition 1 1225.0 average or below average or below 2 885.0 average or below average or below 3 385.0 good average or below 4 252.7 average or below very_good 5 468.0 good average or below 6 310.0 good good ggplot(data=ShuffledHouses, aes(x=condition, y=price, fill=condition)) + geom_boxplot(outlier.shape = NA) + geom_jitter(alpha=0.5) + coord_flip() + ggtitle(&quot;Shuffled Houses&quot;) We fit a model to predict price using condition, and compare it to one that predicts price without using condition, and calculate the F-statistic. M1_Shuffled_Houses &lt;- lm(data=ShuffledHouses, price~condition) M0_Shuffled_Houses &lt;- lm(data=ShuffledHouses, price~1) anova(M1_Shuffled_Houses, M0_Shuffled_Houses)$F[2] ## [1] 0.7510288 Second Permutation ShuffledHouses &lt;- Houses ## create copy of dataset ShuffledHouses$condition &lt;- ShuffledHouses$condition[sample(1:nrow(ShuffledHouses))] Shuffle2df &lt;- data.frame(Houses$Id, Houses$price, Houses$condition, ShuffledHouses$condition) names(Shuffle2df) &lt;- c(&quot;Id&quot;, &quot;price&quot;, &quot;condition&quot;, &quot;Shuffled_Condition&quot;) kable(head(Shuffle2df)) Id price condition Shuffled_Condition 1 1225.0 average or below average or below 2 885.0 average or below good 3 385.0 good average or below 4 252.7 average or below average or below 5 468.0 good good 6 310.0 good good ggplot(data=ShuffledHouses, aes(x=condition, y=price, fill=condition)) + geom_boxplot(outlier.shape = NA) + geom_jitter(alpha=0.5) + coord_flip() + ggtitle(&quot;Shuffled Houses&quot;) We fit a model to predict price using condition, and compare it to one that predicts price without using condition, and calculate the F-statistic. M1_Shuffled_Houses &lt;- lm(data=ShuffledHouses, price~condition) M0_Shuffled_Houses &lt;- lm(data=ShuffledHouses, price~1) anova(M1_Shuffled_Houses, M0_Shuffled_Houses)$F[2] ## [1] 0.204532 Third Permutation ShuffledHouses &lt;- Houses ## create copy of dataset ShuffledHouses$condition &lt;- ShuffledHouses$condition[sample(1:nrow(ShuffledHouses))] Shuffle3df &lt;- data.frame(Houses$Id, Houses$price, Houses$condition, ShuffledHouses$condition) names(Shuffle3df) &lt;- c(&quot;Id&quot;, &quot;price&quot;, &quot;condition&quot;, &quot;Shuffled_Condition&quot;) kable(head(Shuffle1df)) Id price condition Shuffled_Condition 1 1225.0 average or below average or below 2 885.0 average or below average or below 3 385.0 good average or below 4 252.7 average or below very_good 5 468.0 good average or below 6 310.0 good good ggplot(data=ShuffledHouses, aes(x=condition, y=price, fill=condition)) + geom_boxplot(outlier.shape = NA) + geom_jitter(alpha=0.5) + coord_flip() + ggtitle(&quot;Shuffled Houses&quot;) We fit a model to predict price using condition, and compare it to one that predicts price without using condition, and calculate the F-statistic. M1_Shuffled_Houses &lt;- lm(data=ShuffledHouses, price~condition) M0_Shuffled_Houses &lt;- lm(data=ShuffledHouses, price~1) anova(M1_Shuffled_Houses, M0_Shuffled_Houses)$F[2] ## [1] 0.5485496 We’ll simulate 10,000 permutations and record the F-statistic for each set of permuted data. Fstat &lt;- anova(M_House_Cond, M0_House)$F[2] ## record value of F-statistic from actual data ## perform simulation FSim &lt;- rep(NA, 10000) ## vector to hold results ShuffledHouses &lt;- Houses ## create copy of dataset for (i in 1:10000){ #randomly shuffle acceleration times ShuffledHouses$condition &lt;- ShuffledHouses$condition[sample(1:nrow(ShuffledHouses))] ShuffledHouses_M1&lt;- lm(data=ShuffledHouses, price ~ condition) #fit full model to shuffled data ShuffledHouses_M0&lt;- lm(data=ShuffledHouses, price ~ 1) #fit reduced model to shuffled data FSim[i] &lt;- anova(ShuffledHouses_M1, ShuffledHouses_M0)$F[2] ## record F from shuffled model } House_Cond_SimulationResults &lt;- data.frame(FSim) #save results in dataframe The distribution of the F-statistics is shown below. Recall that these are simulated under the assumption that there is no difference in average price between houses of the three different conditions, i.e. no relationship between price and condition. The red line shows the location of the F-statistic we saw in our data (0.60). Since F-statistics cannot be negative, we don’t need to worry about finding an F-statistic as extreme in the opposite direction. House_Cond_SimulationResults_Plot &lt;- ggplot(data=House_Cond_SimulationResults, aes(x=FSim)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + geom_vline(xintercept=c(Fstat), color=&quot;red&quot;) + xlab(&quot;Simulated Value of F&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Distribution of F under assumption of no relationship&quot;) House_Cond_SimulationResults_Plot The F-statistic in our actual we observed does not appear to be very extreme. p-value: Proportion of simulations resulting in value of F more extreme than 0.60. mean(FSim &gt; Fstat) ## [1] 0.5568 The p-value represents the probability of observing an F-statistic as extreme as 0.60 by chance, in samples of size 61, 30, and 9, if in fact there is no relationship between price and size of car. More than half of our simulations resulted in an F-statistic as extreme or more extreme than the one we saw in our actual data, even though the simulation was performed in a situation where there was no relationship between price and condition. Thus, it is very plausible that we would observe an F-statistic as extreme or more extreme than we saw in our data, even if there is no relationship between price and condition (or no difference in average price between the conditions), among all houses. Since the p-value is large, we cannot reject the null hypothesis. We do not have evidence to say that average price differs between houses of the different condition types. It is important to note that we are not saying that we believe the average price is the same for each condition. Recall that the average prices among the conditions in our sample differed by more than 300 thousand dollars! It’s just that given the size of our samples, and the amount of variability in our data, we cannot rule out the possibility that this difference occurred purely by chance. 3.3 Responsible Hypothesis Testing While hypothesis tests are a powerful tool in statistics, they are also one that has been widely misused, to the detriment of scientific research. The hard caused by these misuses caused the American Statistical Association to release a 2016 statement, intended to provide guidance and clarification to scientists who use hypothesis testing and p-values in their research. The statement provides the following six principles for responsible use of hypothesis tests and p-values. P-values can indicate how incompatible the data are with a specified statistical model. P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone. Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold. Proper inference requires full reporting and transparency. A p-value, or statistical significance, does not measure the size of an effect or the importance of a result. By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis. The statement provides important guidance for us to consider as we work with hypothesis testing in this class, as well as in future classes and potentially in our own research. A hypothesis test can only tell us the strength of evidence against the null hypothesis. The absence of evidence against the null hypothesis should not be interpreted as evidence for the null hypothesis. We should never say that the data support/prove/confirm the null hypothesis. We can only say that the data do not provide evidence against the null hypothesis. What to conclude from p-values and what not to: A low p-value provides evidence against the null hypothesis. It suggests the test statistic we observed is inconsistent with the null hypothesis. A low p-value does not tell us that the difference or relationship we observed is meaningful in a practical sense. Researchers should look at the size of the difference or strength of the relationship in the sample before deciding whether it merits being acted upon. A high p-value means that the data could have plausibly been obtained when the null hypothesis is true. The test statistic we observed is consistent with what we would have expected to see when the null hypothesis is true, and thus we cannot rule out the null hypothesis. A high p-value does not mean that the null hypothesis is true or probably true. A p-value can only tell us the strength of evidence against the null hypothesis, and should never be interpreted as support for the null hypothesis. Just because our result is consistent with the null hypothesis does not mean that we should believe that null hypothesis is true. Lack of evidence against a claim does not necessarily mean that the claim is true. In this scenario, we got a small p-value, but we should also be aware of what we should conclude if the p-value is large. Remember that the p-value only measures the strength of evidence against the null hypothesis. A large p-value means we lack evidence against the null hypothesis. This does not mean, however, that we have evidence supporting null hypothesis. A hypothesis test can be thought of as being analogous to a courtroom trial, where the null hypothesis is that the defendant did not commit the crime. Suppose that after each side presents evidence, the jury remains unsure whether the defendant committed the crime. Since the jury does not have enough evidence to be sure, they must, under the law of the United States find the defendant “not guilty.” This does not mean that the jury thinks the defendant is innocent, only that they do not have enough evidence to be sure they are guilty. Similarly in a hypothesis test, a large p-value indicates a lack of evidence against the null hypothesis, rather than evidence supporting it. As such, we should avoid statements suggesting we “support”, “accept”, or “believe” the null hypothesis, and simply state that we lack evidence against it. Things to say when the p-value is large: The data are consistent with the null hypothesis. We do not have enough evidence against the null hypothesis. We cannot reject the null hypothesis. The null hypothesis is plausible. Things NOT to say when the p-value is large: The data support the null hypothesis. The data provide evidence for the null hypothesis. We accept the null hypothesis. We conclude that the null hypothesis is true. Thus, if we had obtained a large p-value in the comparison in mercury levels between Northern and Southern lakes, the appropriate conclusion would be “We do not have evidence that the average mercury level differs between lakes in Northern Florida, compared to Southern Florida.” Even if we got a large p-value it would be incorrect to say “There is no difference in average mercury levels between lakes in Northern Florida and Southern Florida.” We would just be saying that given the size of our sample and the amount of variability on the date, we cannot rule out the possibility of observing a difference like we did by chance, when there really is no difference. "],["bootstrap-interval-estimation.html", "Chapter 4 Bootstrap Interval Estimation 4.1 Sampling Distributions 4.2 Bootstrapping 4.3 Bootstrap Confidence Interval Example 4.4 Estimating Standard Error", " Chapter 4 Bootstrap Interval Estimation Learning Outcomes: Explain the impact of sampling variability on interval estimation. Define sampling distribution and standard error. Interpret confidence intervals, or explain why it is inappropriate to do so. Explain the purpose of bootstrapping. Explain how to obtain a bootstrap distribution for a statistic (such as a sample mean, median, standard deviation, proportion, difference in means or proportions, or regression coefficient). Determine when it is appropriate to use standard error and percentile confidence intervals. Explain whether or not the results of a confidence interval are consistent with the conclusion of a hypothesis test. Interpret confidence intervals, or explain why it is inappropriate to do so. Define standard error of a statistic and interpret it in context. Explain how sample size and level of confidence impact the width of a confidence interval. Explain how sample size impacts variability in individual observations, and the sampling distribution for a test statistic. Define standard error of a statistic and interpret it in context. Explain when it is appropriate to use “theory-based” standard error formulas. 4.1 Sampling Distributions 4.1.1 Sampling From a Population In statistics, we often do not have the time, money, or means to collect data on all individuals or units on which we want to draw conclusions. Instead, we might collect data on only a subset of the individuals, and then make inferences about all individuals we are interested in, using the information we collected. Vocabulary: A population is the entire set of individuals that we want to draw conclusions about. A sample is a subset of a population A parameter is a numerical quantity pertaining to an entire population A statistic is a numerical quantity calculated from a sample We’ll work with a dataset containing information on all 20,591 flights from New York to Chicago in 2013. Our population of interest is all 20,591 flights. In this situation, we have information on the entire population, but suppose temporarily that we didn’t. Instead, suppose we had only information on a random sample of 75 flights. The parameter of interest is the proportion of on-time arrivals out of all flights in the population of 20,591. When the parameter is proportion, we’ll denote it with the letter \\(p\\). We take a sample of 75 flights. The first 6 flights in the sample are shown below. The ontime variable tells whether or not the flight arrived on time. set.seed(08082023) S1 &lt;- sample_n(Flights_NY_CHI, 75) head(S1) ## # A tibble: 6 × 9 ## year month day carrier origin dest sched_dep_time arr_delay ontime ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2013 3 8 AA LGA ORD 1720 106 N ## 2 2013 12 15 AA JFK ORD 1715 -24 Y ## 3 2013 10 22 UA EWR ORD 1300 -12 Y ## 4 2013 8 26 UA EWR ORD 2110 15 N ## 5 2013 7 23 AA LGA ORD 1359 66 N ## 6 2013 5 13 UA EWR ORD 900 -21 Y We’ll calculate the number, and proportion of flights that arrived on time. num_ontime &lt;- sum(S1$ontime == &quot;Y&quot;) # count number of on-time arrivals Number of on-time arrivals in the sample. num_ontime ## [1] 39 Proportion of on-time arrivals in the sample. p_hat &lt;- num_ontime/75 p_hat ## [1] 0.52 In out sample 52 percent of flights arrived on-time. We’ll denote this quantity \\(\\hat{p}\\), keeping with our convention of using the \\(\\hat{}\\) symbol to represent a quantity that is calculated from data (like the predictions and estimates we saw in the previous chapter). The sample statistic \\(\\hat{p}\\) is an estimate of the population proportion \\(p\\). Of course, this was just one sample of 75 flights. We should not expect the proportion of on-time flights in our sample (\\(\\hat{p}\\)) to exactly match the proportion of on-time flights in the entire population (\\(p\\)). Nevertheless we can use the sample to estimate the proportion of all flights in the population that arrive on time. Perhaps, we could say that we would expect between 42 and 62 percent of all 2013 flights from New York to arrive on time. Or, perhaps we could be more precise and estimate that the percentage of on-time flights to be between 47 and 57. We’ll need to figure out how precise we can make our range, while still being confident that it does, in fact, contain the true population parameter. To summarize: The population is all 20,591 flights from New York to Chicago in 2013. The sample is the flights that we randomly selected. The parameter \\(p\\) is the proportion of on-time arrivals among all 2013 flights from New York to Chicago, which we do not know (though in this particular example, we could find it, since we have data on all flights in the population.) The sample statistic is the proportion of flights in our sample that arrived on time, which we know to be \\(\\hat{p}\\) = 0.52. Now, let’s take a different sample of 75 flights and see how the proportion of on-time arrivals compares. S2 &lt;- sample_n(Flights_NY_CHI, 75) num_ontime2 &lt;- sum(S2$ontime == &quot;Y&quot;) # count number of on-time arrivals p_hat2 &lt;- num_ontime2/75 p_hat2 ## [1] 0.5066667 By studying the behavior of the proportion of on-time arrivals in different samples we can gauge how close the proportion in a given sample is likely be to the unknown population parameter. If all of our samples produce very similar estimates, then it is likely that the population parameter is close to these estimates. If the sample proportion varies considerably from sample to sample, then it is possible that the proportion in any given sample might be very different than the population parameter. Let’s take 10,000 more random samples of 75 flights and record the proportion of on-time arrivals in each sample. nreps &lt;- 10000 # number of repetitions p_hat_val &lt;- rep(NA, nreps) # create vector to hold proportion of on-time arrivals Sample &lt;- 1:nreps for(i in 1:nreps){ S &lt;- sample_n(Flights_NY_CHI, 75) # take sample of 75 N_ontime &lt;- sum(S$ontime == &quot;Y&quot;) # count number of on-time arrivals p_hat_val[i] &lt;- N_ontime/75 # record proportion on-time } Samples_df &lt;- data.frame(Sample, p_hat_val) # store results in a data frame The table shows the proportion of on-time arrivals in the first 20 samples of 75 flights. kable(head(Samples_df, 20)) Sample p_hat_val 1 0.6533333 2 0.5333333 3 0.5866667 4 0.6933333 5 0.5866667 6 0.6666667 7 0.6133333 8 0.5866667 9 0.6533333 10 0.6533333 11 0.6133333 12 0.5866667 13 0.6133333 14 0.4800000 15 0.6000000 16 0.4400000 17 0.4933333 18 0.5733333 19 0.6400000 20 0.5600000 The histogram below shows the distribution of the proportion of on-time arrivals in the 10,000 different samples. Prop_Samp_Dist&lt;- ggplot(data=Samples_df, aes(x=p_hat_val)) + geom_histogram(color=&quot;white&quot;, fill=&quot;blue&quot;) + ggtitle(&quot;Sampling Distribution for Proportion On Time&quot;) + xlab(&quot;Prop. on time in sample&quot;) Prop_Samp_Dist We notice that most of our 10,000 samples yielded proportions of on-time arrivals between 0.5 and 0.7, The distribution of proportion of on-time arrivals is roughly symmetric and bell-shaped. The distribution shown in this histogram is called the sampling distribution for \\(\\hat{p}\\). We can gauge how much the proportion of on-time arrivals varies between samples by calculating the standard deviation of this sampling distribution. The standard deviation of a sampling distribution for a statistic is also called the standard error of the statistic. In this case it represents the standard error \\(\\hat{p}\\) (the proportion of on-time arrivals), and is denoted \\(\\text{SE}(\\hat{p})\\). This standard error is shown below. SE_p_hat &lt;- sd(Samples_df$p_hat_val) SE_p_hat ## [1] 0.05659102 Vocabulary: The sampling distribution of a statistic is the distribution of values the statistic takes on across many different samples of a given size. The standard error of a statistic is the standard deviation of that statistic’s sampling distribution. It measures how much the statistic varies between different samples of a given size. In this rare situation, we actually have data on all 20,591 flights from New York to Chicago in 2013 (our entire population), Let’s calculate the true value of the population parameter \\(p\\), the proportion of flights that arrived on-time in our actual population. p &lt;- sum(Flights_NY_CHI$ontime == &quot;Y&quot;)/20591 p ## [1] 0.6079841 In fact, just over 60% of all flights in the population arrived on time. The sampling distribution for the proportion of on-time flights is shown again below. The true proportion of on-time flights is marked by the green dotted line. The gold bar at the bottom of the histogram represents the range of sample proportions that lie within \\(\\pm 2\\) standard errors of the true population proportion of flights that arrived on time. 0.6079841 - 2(0.056591) to 0.6079841 + 2(0.056591) Prop_Samp_Dist+ geom_vline(xintercept=p, color=&quot;green&quot;, linetype=&quot;dotted&quot;, linewidth=2) + geom_segment(aes(x=p - 2*SE_p_hat,xend=p + 2*SE_p_hat, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We calculate the proportion of samples whose proportion of on-time arrivals lies within \\(\\pm 2\\) standard errors of the true proportion. Lower &lt;- p - 2*SE_p_hat Upper &lt;- p + 2*SE_p_hat sum((Samples_df$p_hat_val &gt;=Lower) &amp; (Samples_df$p_hat_val &lt;= Upper)) ## [1] 9539 Approximately 95% 10,000 samples produced proportions within \\(\\pm 2\\) standard errors of the true population proportion of on-time flights. 4.1.2 Confidence Intervals In a real situation, we won’t have access to the entire population of flights, only the flights in a single sample. For example, recall our original sample of 75 flights, in which we observed a proportion of on-time arrivals of \\(\\hat{p}=\\) 0.52. Since we now know that 95% of all samples produce proportions that lie within two standard errors of the population proportion, we can obtain an estimate of the population proportion \\(p\\) by adding and subtracting \\(2\\times \\text{SE}(\\hat{p})\\) from our observed sample proportion \\(\\hat{p}\\). Such an interval is called an approximate 95% confidence interval for the true population proportion \\(p\\). Approximate 95% Confidence Interval for \\(\\hat{p}\\) \\[ \\hat{p} \\pm 2\\times \\text{SE}(\\hat{p}) \\] The confidence interval, based on our original sample, is calculated below. c(p_hat - 2*SE_p_hat, p_hat + 2*SE_p_hat) ## [1] 0.406818 0.633182 Based on our sample of 75 flights, we can be 95% confident that the true proportion of on-time arrivals among all 2013 flights from New York to Chicago is between 0.406818 and p_hat + 2*SE_p_hat. In fact, knowing what we do about the true value of the population parameter \\(p\\), we can see that our confidence interval does indeed contain this value. Of course, in a real situation, we won’t know the true value of the population parameter, so we won’t know for sure whether or not our confidence interval contains this true parameter value. A pertinent question at this stage would be ``What does 95% confidence mean?\". To answer that, let’s explore what happens when we calculate confidence intervals based on estimates many different samples. For each of our 10,000 different samples taken from our population, we’ll add and subtract two standard errors from the sample proportion \\(\\hat{p}\\) corresponding to that sample. The table below displays the value of \\(\\hat{p}\\), for the first 20 samples we took, along with the lower and upper bounds of the confidence interval, and whether or not the confidence interval contains the true parameter value \\(p\\) (either TRUE or FALSE). Samples_df &lt;- Samples_df %&gt;% mutate( Lower = p_hat_val - 2*SE_p_hat, Upper = p_hat_val + 2*SE_p_hat, Contains = p &gt;= Lower &amp; p &lt;= Upper) kable(head(Samples_df, 20)) Sample p_hat_val Lower Upper Contains 1 0.6533333 0.5401513 0.7665154 TRUE 2 0.5333333 0.4201513 0.6465154 TRUE 3 0.5866667 0.4734846 0.6998487 TRUE 4 0.6933333 0.5801513 0.8065154 TRUE 5 0.5866667 0.4734846 0.6998487 TRUE 6 0.6666667 0.5534846 0.7798487 TRUE 7 0.6133333 0.5001513 0.7265154 TRUE 8 0.5866667 0.4734846 0.6998487 TRUE 9 0.6533333 0.5401513 0.7665154 TRUE 10 0.6533333 0.5401513 0.7665154 TRUE 11 0.6133333 0.5001513 0.7265154 TRUE 12 0.5866667 0.4734846 0.6998487 TRUE 13 0.6133333 0.5001513 0.7265154 TRUE 14 0.4800000 0.3668180 0.5931820 FALSE 15 0.6000000 0.4868180 0.7131820 TRUE 16 0.4400000 0.3268180 0.5531820 FALSE 17 0.4933333 0.3801513 0.6065154 FALSE 18 0.5733333 0.4601513 0.6865154 TRUE 19 0.6400000 0.5268180 0.7531820 TRUE 20 0.5600000 0.4468180 0.6731820 TRUE The graphic below visualizes the confidence intervals produced using the estimates from the first 100 samples. The green dotted line indicates the true value of \\(p\\). The black dots indicate the value of \\(\\hat{p}\\) for each sample. Intervals that do in fact contain the true value of \\(p\\) are shown in blue, and intervals that do not contain the true value of \\(p\\) are shown in green. ggplot(data=Samples_df[1:100,], aes(y=Sample, x=p_hat_val)) + geom_point() + geom_errorbar(aes(xmin = Lower, xmax = Upper, color=Contains)) + xlab(&quot;Confidence Interval&quot;) + ylab(&quot;Sample&quot;) + geom_vline(xintercept = p, color=&quot;green&quot;, linetype=&quot;dotted&quot;, size=2) + theme_bw() Out of these 100 samples, 93 contain the true value of the population parameter \\(p\\). This is close to the 95% confidence level. The picture shows confidence intervals produced by the first 100 samples, but we actually took 10,000 different samples of 75 flights. Let’s calculate how many of these samples produced confidence intervals that contain the true value of \\(p\\). sum(Samples_df$Contains == TRUE) ## [1] 9539 Again, notice that close to 95% of the samples produced confidence intervals contain the true population parameter \\(p\\). Note that for the red intervals that do not contain \\(p\\) nothing was done incorrectly. The sample was taken at random, and the confidence interval was calculated using the correct formula. It just happened that by chance, we obtained a sample proportion \\(\\hat{p}\\) that was unusually high or low, leading to an interval that did not capture the true population parameter. This, of course, happens rarely, and approximately 95% of the samples do, in fact, result in intervals that contain the true value of \\(p\\). This brings us back to the question “what does 95% confidence mean?”. An approximate 95% confidence interval means that if we take a large number of samples and calculate confidence intervals from each of them, then approximately 95% of the samples will produce intervals containing the true population parameter. In reality, we’ll only have on sample, and won’t know whether or not our interval contains the true parameter value. Assuming we have taken the sample and calculated the interval correctly, we can rest assured in the knowledge that that 95% of all intervals taken would contain the true parameter value, and hope that ours is among that 95%. We calculated the confidence interval by taking our sample statistic \\(\\hat{p}\\) plus/minus two standard errors. Confidence intervals that are calculated by adding and subtracting a certain number of standard errors from the sample statistic are called standard error confidence intervals. This approach will work as long as the sampling distribution is symmetric and bell-shaped. Probability theory tells us that in a symmetric and bell-shaped distribution, approximately 95% of the area lies within two standard errors of the center of the distribution, given by the true parameter value. We will, however, see that this approach will not work in all cases. Not all statistics produce sampling distributions that are symmetric and bell-shaped, and we will need an alternative way to calculate confidence intervals in these situations. knitr::include_graphics(&quot;Emp_Rule.png&quot;) Figure 4.1: Image from https://openintro-ims.netlify.app/foundations-mathematical If we want to use a level of confidence that is different than 95%, we can adjust the value we multiply the standard error by. In general, a standard error confidence interval has the form \\[ \\text{Statistic } \\pm m\\times \\text{Standard Error}, \\] where the value of \\(m\\) depends on the desired level of confidence. Of course, you might ask why we needed to calculate a confidence interval for the proportion of on-time flights in the first place, since we actually have data on all 20,591 flights in the population and already know the true proportion of on-time arrivals to be 0.608. The answer is that we don’t. But, in most real situations, we will only have data from a single sample, not the entire population, and we won’t know the true population parameter. We’ll be able to build on the ideas of sampling distributions and standard error that we learned about in this section to calculate confidence intervals in those scenarios. 4.2 Bootstrapping 4.2.1 Mercury Levels in Florida Lakes A 2004 study by Lange, T., Royals, H. and Connor, L. examined Mercury accumulation in large-mouth bass, taken from a sample of 53 Florida Lakes. If Mercury accumulation exceeds 0.5 ppm, then there are environmental concerns. In fact, the legal safety limit in Canada is 0.5 ppm, although it is 1 ppm in the United States. Recall that our data come from a sample of 53 lakes, out of more then 30,000 in the entire state of Florida. The mercury levels of the first 10 lakes in the sample are shown in the table below. data(&quot;FloridaLakes&quot;) FloridaLakes &lt;- FloridaLakes %&gt;% rename(Mercury = AvgMercury) kable(head(FloridaLakes %&gt;% select(ID, Lake, Mercury), 10)) ID Lake Mercury 1 Alligator 1.23 2 Annie 1.33 3 Apopka 0.04 4 Blue Cypress 0.44 5 Brick 1.20 6 Bryant 0.27 7 Cherry 0.48 8 Crescent 0.19 9 Deer Point 0.83 10 Dias 0.81 The histogram shows the distribution of mercury levels in the 53 lakes in the sample. Lakes exceeding the US standard of 1 ppm are shown in red. Lakes_Hist &lt;- ggplot(data=FloridaLakes, aes(x=Mercury)) + geom_histogram(aes(fill=Mercury&lt;=1), color=&quot;white&quot;, binwidth = 0.1) + ggtitle(&quot;Mercury Levels in Sample of 53 Florida Lakes&quot;) + xlab(&quot;Mercury Level&quot;) + ylab(&quot;Frequency&quot;) + theme_bw() Lakes_Hist The proportion of lakes with mercury levels exceeding 1 ppm is calculated below. p_hat &lt;- sum(FloridaLakes$Mercury &gt; 1)/53 p_hat ## [1] 0.1132075 We see that in our sample of 53 lakes, approximately 11% have mercury levels exceeding the US standard of 1 ppm. Suppose we want to estimate the proportion of all Florida Lakes whose mercury level exceeds this standard. As we saw in the previous section, we would not expect the population proportion to exactly match the sample, due to random variability between samples. We can use the sample proportion as an estimate (\\(\\hat{p} = 0.1132\\)), and construct a confidence interval for the unknown population proportion \\(p\\). In order to construct the confidence interval, we need to know how much the sample proportion of lakes exceeding 1 ppm \\(\\hat{p}\\) could vary between different samples of size 53. That is, we need to know the standard error of \\(\\hat{p}\\). In the previous section, we calculated the standard error by taking 10,000 different samples of the same size as ours from the population, calculating the proportion for each sample, and then calculating the standard deviation of the proportions obtained from these 10,000 different samples. This procedure will not work here, however, because unlike the previous example where we really did have data on the entire population of all flights from New York to Chicago, we do not have data on all 30,000+ lakes in Florida. We cannot take a lot of different samples of size 53 from the population of all lakes, and thus, cannot obtain the sampling distribution for the the proportion of lakes exceeding 1 ppm, or estimate the standard error of \\(\\hat{p}\\). 4.2.2 Bootstrap Sampling All we have is a single sample of 53 lakes. We need to figure out how much the proportion of lakes with mercury levels exceeding 1 ppm would vary between different samples of size 53, using only the information contained in our one sample. To do this, we’ll implement a popular simulation-based strategy, known as bootstrapping. Let’s assume our sample is representative of all Florida lakes. Then, we’ll duplicate the sample many times to create a large set that will look like the population of all Florida Lakes. We can then draw samples of 53 from that large population, and record the mean mercury level for each sample of 53. An illustration of the bootstrapping procedure is shown below, using a sample of 12 colored dots, instead of the 53 lakes. In fact, duplicating the sample many times and selecting new samples of size \\(n\\) has the same effect as drawing samples of size \\(n\\) from the original sample, by putting the item drawn back in each time, a procedure called sampling with replacement. Thus, we can skip the step of copying/pasting the sample many times, and instead draw our samples with replacement. This means that in each new sample, some lakes will be drawn multiple times and others not at all. It also ensures that each sample is different, allowing us to estimate variability in the sample mean between the different samples of size 53. An illustration of the concept of bootstrapping, using sampling with replacement is shown below. The variability in sample means in our newly drawn samples is used to approximate the variability in proportion \\(\\hat{p}\\) that would occur between different samples of 53 lakes, drawn from the population of all Florida Lakes. The point of bootstrapping is to observe how much a statistic (in this case the proportion of lakes with Mercury levels exceeding 1 ppm) varies between bootstrap samples. This can act as an estimate of how much that statistic would vary between different samples of size \\(n\\), drawn from the population. The steps of bootstrap sampling can be summarized in the following algorithm. Bootstrap Algorithm For an original sample of size \\(n\\): Take a sample size \\(n\\) by randomly sampling from the original, with replacement. Thus, some observations will show up multiple times, and others not at all. This sample is called a bootstrap sample. Calculate the statistic of interest in the bootstrap sample (in this case \\(\\hat{p}\\), the proportion of lakes whose mercury levels exceed 1 ppm). Repeat steps 1 and 2 many (say 10,000) times, keeping track of the statistic of interest that is calculated in each bootstrap sample. Look at the distribution of the statistic across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the statistic of interest. 4.2.3 Bootstrap Samples of Lakes The sample_n() function samples the specified number rows from a data frame, with or without replacement. The lakes in the first sample are shown below. Notice that some lakes occur multiple times, and others not at all. Bootstrap Sample 1 BootstrapSample1 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) BootstrapSample1 %&gt;% select(ID, Lake, Mercury) ## # A tibble: 53 × 3 ## ID Lake Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Alligator 1.23 ## 2 2 Annie 1.33 ## 3 3 Apopka 0.04 ## 4 5 Brick 1.2 ## 5 5 Brick 1.2 ## 6 6 Bryant 0.27 ## 7 7 Cherry 0.48 ## 8 9 Deer Point 0.83 ## 9 9 Deer Point 0.83 ## 10 12 Down 0.5 ## # ℹ 43 more rows We calculate the proportion of lakes with mercury levels exceeding 1 ppm in this bootstrap sample. Note that if a lake shows up more than once in the bootstrap sample, then it is counted however many times it shows up. sum(BootstrapSample1$Mercury &gt; 1) / 53 ## [1] 0.0754717 Bootstrap Sample #2 We take a second bootstrap sample. Notice that the lakes chosen and omitted differ from the first sample. BootstrapSample2 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) BootstrapSample2 %&gt;% select(ID, Lake, Mercury) ## # A tibble: 53 × 3 ## ID Lake Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Alligator 1.23 ## 2 2 Annie 1.33 ## 3 2 Annie 1.33 ## 4 2 Annie 1.33 ## 5 2 Annie 1.33 ## 6 3 Apopka 0.04 ## 7 6 Bryant 0.27 ## 8 11 Dorr 0.71 ## 9 11 Dorr 0.71 ## 10 11 Dorr 0.71 ## # ℹ 43 more rows Proportion exceeding 1 ppm: sum(BootstrapSample2$Mercury &gt; 1) / 53 ## [1] 0.1698113 Bootstrap Sample #3 We’ll take one more bootstrap sample and calculate the proportion of lakes with mercury levels exceeding 1 ppm. BootstrapSample3 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) BootstrapSample3 %&gt;% select(ID, Lake, Mercury) ## # A tibble: 53 × 3 ## ID Lake Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Alligator 1.23 ## 2 1 Alligator 1.23 ## 3 2 Annie 1.33 ## 4 3 Apopka 0.04 ## 5 3 Apopka 0.04 ## 6 5 Brick 1.2 ## 7 6 Bryant 0.27 ## 8 7 Cherry 0.48 ## 9 9 Deer Point 0.83 ## 10 9 Deer Point 0.83 ## # ℹ 43 more rows Proportion exceeding 1 ppm: sum(BootstrapSample3$Mercury &gt; 1) / 53 ## [1] 0.09433962 4.2.4 Bootstrap Distribution Now that we have seen how bootstrap sampling works, we’ll take a large number (10,000) different bootstrap samples and examine how the proportion of lakes with mercury levels exceeding 1 ppm varies between samples. We’ll use a for-loop to take many different bootstrap samples and record the observed proportion in a vector called p_hat_b p_hat &lt;- sum(FloridaLakes$Mercury &gt; 1)/53 #calculate sample statistic Bootstrap_prop &lt;- rep(NA, 10000) #setup vector to hold bootstrap statistics for (i in 1:10000){ BootstrapSample &lt;- sample_n(FloridaLakes, 53, replace=TRUE) #take bootstrap sample Bootstrap_prop[i] &lt;- sum(BootstrapSample$Mercury &gt; 1)/53 # calc. prop exceeding 1 } Lakes_Bootstrap_Prop &lt;- data.frame(Bootstrap_prop) #store values in a dataframe The distribution of proportions observed in the 10,000 different bootstrap samples is shown below. This distribution is called the bootstrap distribution. Lakes_Bootstrap_Prop_plot &lt;- ggplot(data=Lakes_Bootstrap_Prop, aes(x=Bootstrap_prop)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Prop &gt; 1 in Bootstrap Sample &quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Bootstrap Distribution for Prop. of Lakes Exeeding 1 ppm Hg&quot;) + theme(legend.position = &quot;none&quot;) Lakes_Bootstrap_Prop_plot 4.2.5 Bootstrap SE Confidence Interval We calculate the standard deviation of this bootstrap distribution, which is an estimate of the standard error of \\(\\hat{p}\\). SE_p_hat &lt;- sd(Lakes_Bootstrap_Prop$Bootstrap_prop) Since the bootstrap distribution is roughly symmetric and bell-shaped, we can calculate a 95% confidence interval for the proportion of all Florida lakes with mercury levels exceeding 1 ppm, using bootstrap standard error confidence interval method \\[ \\hat{p} \\pm 2\\times\\text{SE}(\\hat{p}) \\] c(p_hat - 2*SE_p_hat, p_hat + 2*SE_p_hat) ## [1] 0.02684504 0.19957005 We are 95% confident that the proportion of all Florida lakes with mercury levels exceeding 1 ppm is between 0.026845 and 0.1995701. The gold bar at the bottom of the bootstrap distribution represents this 95% confidence interval. Lakes_Bootstrap_Prop_plot + geom_segment(aes(x=p_hat - 2*SE_p_hat,xend=p_hat + 2*SE_p_hat, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) 4.2.6 Bootstrap Distribution vs Sampling Distribution We stated that the standard error of the bootstrap distribution is meant to give us and estimate of the standard error of the statistic of interest (\\(\\hat{p}\\) in this case). We do note, however, that the bootstrap distribution is not the same as the sampling distribution for a statistic illustrated in the previous section. The sampling distribution shows the distribution of values the statistic would take on accross many different samples drawn from the population. As such, it will be centered at the true population parameter (\\(p\\) in this case). The bootstrap distribution, on the other hand, shows us the distribution of values the statistic takes on across different bootstrap samples drawn from the original sample, using replacement. Since it comes entirely from the original sample, rather than the population, it will be centered at the sample statistic ( in this case \\(\\hat{p}\\)), rather than the unknown \\(p\\). Nevertheless, the amount of variability in a statistic, calculated between different bootstrap samples, is often a reasonable approximation of the amount of variability we would observe in that statistic if we could take many samples of the same size from the entire population. 4.3 Bootstrap Confidence Interval Example 4.3.1 Bootstrapping Other Statistics We’ve seen how to use bootstrapping to calculate confidence intervals for an unknown population parameter \\(p\\), using an estimate \\(\\hat{p}\\), calculated from a sample of size \\(n\\). This procedure can be applied to calculate confidence intervals for a wide range of population parameters, using statistics calculated from a sample. For example, we could calculate confidence intervals any of the following parameters, using the corresponding sample statistic. Context Parameter Statistic Proportion \\(p\\) \\(\\hat{p}\\) Mean \\(\\mu\\) \\(\\bar{x}\\) Standard Deviation \\(\\sigma\\) \\(s\\) Median no comon abbreviations Difference in Means \\(\\mu_2-\\mu_1\\) \\(\\bar{x}_2 - \\bar{x}_1\\) Regression Coefficient \\(\\beta_j\\) \\(b_j\\) Estimated Regression Response \\(\\beta_0 + \\beta_1x_{i1} + \\ldots + \\beta_px_{ip}\\) \\(b_0 + b_1x_{i1} + \\ldots + b_px_{ip}\\) We follow the same algorithm as we did when working with a proportion, and simply calculate whatever statistic we are interested in step 2, in place of \\(\\hat{p}\\), as we did previously. The bootstrap algorithm is given again, below. Bootstrap Algorithm For an original sample of size \\(n\\): Take a sample of size \\(n\\) by randomly sampling from the original sample with replacement. (Thus some observations will show up multiple times and others not at all.) Calculate the statistic of interest in the bootstrap sample. Repeat steps 1 and 2 many (say 10,000) times, keeping track of the statistic of interest that is calculated in each bootstrap sample. Look at the distribution of the statistic across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the statistic of interest. We’ll now go through examples, calculating bootstrap confidence intervals for each of the parameters listed above. 4.3.2 CI for Mean The histogram shows the distribution of mercury levels of the 53 lakes in our sample. The mean and standard deviation in mercury levels for these 53 lakes is shown. Lakes_Hist &lt;- ggplot(data=FloridaLakes, aes(x=Mercury)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;, binwidth = 0.2) + ggtitle(&quot;Mercury Levels in Sample of Florida Lakes&quot;) + xlab(&quot;Mercury Level&quot;) + ylab(&quot;Frequency&quot;) Lakes_Hist We’ll calculate the mean and median mercury level for the 53 lakes in the sample. Lakes_Stats &lt;- FloridaLakes %&gt;% summarize(MeanHg = mean(Mercury), StDevHG = sd(Mercury), N=n()) kable(Lakes_Stats) MeanHg StDevHG N 0.5271698 0.3410356 53 We want to calculate a 95% confidence interval for the mean mercury level among all Florida lakes. We’ll use bootstrapping again, this time using the sample mean, rather than the proportion exceeding 1 ppm, as our statistic of interest. Bootstrap Steps Take a sample of 53 lakes by randomly sampling from the original sample of 53 lakes, with replacement. Calculate the mean mercury level in the bootstrap sample. Repeat steps 1 and 2 many (say 10,000) times, keeping track of the mean mercury level in each bootstrap sample. Look at the distribution of the mean across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the mean mercury level. We’ll illustrate the procedure on 3 bootstrap samples. Bootstrap Sample 1 BootstrapSample1 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) BootstrapSample1 %&gt;% select(ID, Lake, Mercury) ## # A tibble: 53 × 3 ## ID Lake Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Alligator 1.23 ## 2 1 Alligator 1.23 ## 3 3 Apopka 0.04 ## 4 4 Blue Cypress 0.44 ## 5 4 Blue Cypress 0.44 ## 6 6 Bryant 0.27 ## 7 6 Bryant 0.27 ## 8 6 Bryant 0.27 ## 9 7 Cherry 0.48 ## 10 8 Crescent 0.19 ## # ℹ 43 more rows We calculate the mean mercury level among the lakes in the bootstrap sample. mean(BootstrapSample1$Mercury) ## [1] 0.4986792 Bootstrap Sample #2 BootstrapSample2 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) BootstrapSample2 %&gt;% select(ID, Lake, Mercury) ## # A tibble: 53 × 3 ## ID Lake Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Alligator 1.23 ## 2 3 Apopka 0.04 ## 3 4 Blue Cypress 0.44 ## 4 6 Bryant 0.27 ## 5 6 Bryant 0.27 ## 6 7 Cherry 0.48 ## 7 7 Cherry 0.48 ## 8 8 Crescent 0.19 ## 9 8 Crescent 0.19 ## 10 8 Crescent 0.19 ## # ℹ 43 more rows Mean Mercury Level: mean(BootstrapSample2$Mercury) ## [1] 0.5192453 Bootstrap Sample #3 BootstrapSample3 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) BootstrapSample3 %&gt;% select(ID, Lake, Mercury) ## # A tibble: 53 × 3 ## ID Lake Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Alligator 1.23 ## 2 2 Annie 1.33 ## 3 4 Blue Cypress 0.44 ## 4 6 Bryant 0.27 ## 5 6 Bryant 0.27 ## 6 8 Crescent 0.19 ## 7 12 Down 0.5 ## 8 12 Down 0.5 ## 9 12 Down 0.5 ## 10 13 Eaton 0.49 ## # ℹ 43 more rows Mean Mercury Level: mean(BootstrapSample3$Mercury) ## [1] 0.445283 Now, we’ll take 10,000 bootstrap samples, and record the mean mercury concentration in each sample. mean &lt;- mean(FloridaLakes$Mercury) #calculate sample statistic Bootstrap_Mean &lt;- rep(NA, 10000) # setup vector to hold bootstrap statistics for (i in 1:10000){ BootstrapSample &lt;- sample_n(FloridaLakes, 53, replace=TRUE) # take bootstrap sample Bootstrap_Mean[i] &lt;- mean(BootstrapSample$Mercury) # calculate mean in bootstrap sample } Lakes_Bootstrap_Results_Mean &lt;- data.frame(Bootstrap_Mean) #store results in data frame The bootstrap distribution for the mean mercury level is shown below, along with its standard error. Lakes_Bootstrap_Mean_Plot &lt;- ggplot(data=Lakes_Bootstrap_Results_Mean, aes(x=Bootstrap_Mean)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Mean Mercury in Bootstrap Sample &quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Bootstrap Distribution for Sample Mean in Florida Lakes&quot;) + theme(legend.position = &quot;none&quot;) Lakes_Bootstrap_Mean_Plot We’ll calculate the standard error of the mean. This is a measure of how much the mean varies between samples. SE_mean &lt;- sd(Lakes_Bootstrap_Results_Mean$Bootstrap_Mean) SE_mean ## [1] 0.04655744 Notice that the standard error of the mean is much less than the sample standard deviation of 0.341. Interpretations of sample standard deviation and standard error of the mean The sample standard deviation measures the amount of variability in mercury levels between the 53 individual lakes in our sample. The standard error of the mean measures the amount of variability in sample mean mercury levels between different samples of size 53. There is more variability between mercury levels in individual lakes than there is between average mercury levels in different samples of size 53. Since the bootstrap distribution is roughly symmetric and bell-shaped, we can use the bootstrap standard error method to calculate an approximate 95% confidence interval for the mean mercury level among all Florida lakes. \\[ \\text{Statistic} \\pm 2\\times\\text{Standard Error} \\] In this case, the statistic of interest is the sample mean \\(\\bar{x}=0.527\\). The confidence interval is \\[ \\begin{aligned} &amp; \\bar{x} \\pm 2\\times\\text{SE}(\\bar{x}) \\\\ &amp; = 0.527 \\pm 2\\times\\text{0.0465574} \\end{aligned} \\] 95% Confidence Interval: c(mean - 2*SE_mean, mean + 2*SE_mean) ## [1] 0.4340549 0.6202847 The 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below. Lakes_Bootstrap_Mean_Plot + geom_segment(aes(x=mean - 2*SE_mean,xend=mean + 2*SE_mean, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that the average mercury level among all Florida lakes is between 0.4340549 and 0.6202847 parts per million. It is important to note that we are not saying that we are 95% confident that an individual lake lie in this range, or that 95% of all individual lakes lie in this range. We are only saying that we are confident that the average mercury level among all lakes lies in this range. A confidence interval is a statement about a population parameter (in this case the average mercury level), rather than about individual lakes in the population. Since there is more variability about individual lakes than overall averages, we’ll need to make a wider interval when talking about the mercury level for an individual lake. 4.3.3 CI for Standard Deviation Now, we’ll calculate a confidence interval for the standard deviation in mercury levels among all Florida lakes. Recall that the sample standard deviation (\\(s\\)) was: Sample_SD &lt;- sd(FloridaLakes$Mercury) Sample_SD ## [1] 0.3410356 We’ll use this estimate to calculate a confidence interval for the population standard deviation \\(\\sigma\\). This time, our statistic of interest is the sample standard deviation \\(s\\). Bootstrap Steps Take a sample of 53 lakes by randomly sampling from the original sample of 53 lakes, with replacement. Calculate the standard deviation in mercury level in the bootstrap sample. Repeat steps 1 and 2 many (say 10,000) times, keeping track of the standard deviation mercury level in each bootstrap sample. Look at the distribution of the standard deviations across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the standard deviation in mercury level. We’ll illustrate the procedure on 3 bootstrap samples. Bootstrap Sample 1 BootstrapSample1 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) BootstrapSample1 %&gt;% select(ID, Lake, Mercury) ## # A tibble: 53 × 3 ## ID Lake Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 Annie 1.33 ## 2 3 Apopka 0.04 ## 3 3 Apopka 0.04 ## 4 5 Brick 1.2 ## 5 6 Bryant 0.27 ## 6 7 Cherry 0.48 ## 7 7 Cherry 0.48 ## 8 8 Crescent 0.19 ## 9 8 Crescent 0.19 ## 10 9 Deer Point 0.83 ## # ℹ 43 more rows We calculate the standard deviation in mercury levels among the lakes in the bootstrap sample. sd(BootstrapSample1$Mercury) ## [1] 0.3169752 Bootstrap Sample #2 BootstrapSample2 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) BootstrapSample2 %&gt;% select(ID, Lake, Mercury) ## # A tibble: 53 × 3 ## ID Lake Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 Annie 1.33 ## 2 2 Annie 1.33 ## 3 3 Apopka 0.04 ## 4 5 Brick 1.2 ## 5 5 Brick 1.2 ## 6 8 Crescent 0.19 ## 7 8 Crescent 0.19 ## 8 9 Deer Point 0.83 ## 9 9 Deer Point 0.83 ## 10 12 Down 0.5 ## # ℹ 43 more rows Standard Deviation in Mercury Level: sd(BootstrapSample2$Mercury) ## [1] 0.3927499 Bootstrap Sample #3 BootstrapSample3 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) BootstrapSample3 %&gt;% select(ID, Lake, Mercury) ## # A tibble: 53 × 3 ## ID Lake Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 Annie 1.33 ## 2 2 Annie 1.33 ## 3 5 Brick 1.2 ## 4 5 Brick 1.2 ## 5 6 Bryant 0.27 ## 6 6 Bryant 0.27 ## 7 9 Deer Point 0.83 ## 8 9 Deer Point 0.83 ## 9 9 Deer Point 0.83 ## 10 9 Deer Point 0.83 ## # ℹ 43 more rows Standard Deviation Mercury Level: sd(BootstrapSample3$Mercury) ## [1] 0.3589536 Now, we’ll take 10,000 bootstrap samples, and record the standard deviation in mercury concentration in each sample. Sample_SD &lt;- sd(FloridaLakes$Mercury) #calculate sample statistic Bootstrap_SD &lt;- rep(NA, 10000) # setup vector to hold bootstrap statistics for (i in 1:10000){ BootstrapSample &lt;- sample_n(FloridaLakes, 53, replace=TRUE) # take bootstrap sample Bootstrap_SD[i] &lt;- sd(BootstrapSample$Mercury) # calculate standard deviation in bootstrap sample } Lakes_Bootstrap_Results_SD &lt;- data.frame(Bootstrap_SD) #store results in data frame The bootstrap distribution for the mean mercury level is shown below, along with its standard error. Lakes_Bootstrap_SD_Plot &lt;- ggplot(data=Lakes_Bootstrap_Results_SD, aes(x=Bootstrap_SD)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;SD in Mercury in Bootstrap Sample &quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Bootstrap Distribution for Sample SD in Florida Lakes&quot;) + theme(legend.position = &quot;none&quot;) Lakes_Bootstrap_SD_Plot We’ll calculate the standard error of the standard deviation. This is a measure of how much the standard deviation varies between samples. SE_SD &lt;- sd(Lakes_Bootstrap_Results_SD$Bootstrap_SD) SE_SD ## [1] 0.02869572 Since the bootstrap distribution is roughly symmetric and bell-shaped, we can use the bootstrap standard error method to calculate an approximate 95% confidence interval for the standard deviation in mercury levels among all Florida lakes. \\[ \\text{Statistic} \\pm 2\\times\\text{Standard Error} \\] In this case, the statistic of interest is the sample standard deviation \\(s=0.341\\). The confidence interval is \\[ \\begin{aligned} &amp; s \\pm 2\\times\\text{SE}(s) \\\\ &amp; = 0.341 \\pm 2\\times{0.029} \\end{aligned} \\] 95% Confidence Interval: c(Sample_SD - 2*SE_SD, Sample_SD + 2*SE_SD ) ## [1] 0.2836442 0.3984271 The 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below. Lakes_Bootstrap_SD_Plot + geom_segment(aes(x=Sample_SD - 2*SE_SD,xend=Sample_SD + 2*SE_SD, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that the standard deviation in mercury levels among all Florida lakes is between 0.2836442 and 0.3984271 parts per million. 4.3.4 CI for Median We already calculated a confidence interval for the mean mercury level among all Florida lakes. We could calculate a bootstrap confidence interval for the median mercury level as well, but since the distribution of mercury levels in the lakes is roughly symmetric, the mean is a reasonable measure of center, and there is not a clear reason for using the median instead. When a distribution is skewed or contains large outliers, however, the median is a more robust measure of center than the mean. Recall the distribution of 100 Seattle house prices seen in Chapters 1 and 2. ggplot(data=Houses, aes(x=price)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + ggtitle(&quot;Distribution of House Prices&quot;) + xlab(&quot;Price&quot;) + ylab(&quot;Frequency&quot;) These 100 houses are a sample of all houses sold in Seattle in 2014 and 2015, so we can use statistics from our sample to draw conclusions about all houses sold in Seattle in this time period. In this subsection, we’ll use bootstrapping to calculate a 95% confidence interval for the median price among all houses sold in Seattle in this time period. We calculate the sample median price. Sample_Median &lt;- median(Houses$price) Sample_Median ## [1] 507.5 Bootstrap Steps Take a sample of 100 houses by randomly sampling from the original sample of 100 houses, with replacement. Calculate the median price in the bootstrap sample. Repeat steps 1 and 2 many (say 10,000) times, keeping track of the median price in each bootstrap sample. Look at the distribution of the median price across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the median price. We’ll illustrate the procedure on 3 bootstrap samples. Bootstrap Sample 1 BootstrapSample1 &lt;- sample_n(Houses, 100, replace=TRUE) %&gt;% arrange(Id) BootstrapSample1 %&gt;% select(Id, price) ## # A tibble: 100 × 2 ## Id price ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1225 ## 2 1 1225 ## 3 1 1225 ## 4 4 253. ## 5 4 253. ## 6 6 310. ## 7 7 550. ## 8 7 550. ## 9 9 315. ## 10 11 410 ## # ℹ 90 more rows We calculate the median price among the houses in the bootstrap sample. median(BootstrapSample1$price) ## [1] 520 Bootstrap Sample #2 BootstrapSample2 &lt;- sample_n(Houses, 100, replace=TRUE) %&gt;% arrange(Id) BootstrapSample2 %&gt;% select(Id, price) ## # A tibble: 100 × 2 ## Id price ## &lt;int&gt; &lt;dbl&gt; ## 1 2 885. ## 2 2 885. ## 3 4 253. ## 4 4 253. ## 5 5 468. ## 6 6 310. ## 7 7 550. ## 8 7 550. ## 9 7 550. ## 10 8 485. ## # ℹ 90 more rows Median Price: median(BootstrapSample2$price) ## [1] 510 Bootstrap Sample #3 BootstrapSample3 &lt;- sample_n(Houses, 100, replace=TRUE) %&gt;% arrange(Id) BootstrapSample3 %&gt;% select(Id, price) ## # A tibble: 100 × 2 ## Id price ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1225 ## 2 1 1225 ## 3 1 1225 ## 4 2 885. ## 5 4 253. ## 6 5 468. ## 7 6 310. ## 8 8 485. ## 9 8 485. ## 10 8 485. ## # ℹ 90 more rows Median Price: median(BootstrapSample3$price) ## [1] 538 Now, we’ll take 10,000 bootstrap samples, and record the median price in each sample. Sample_Med &lt;- median(Houses$price) #calculate sample median Bootstrap_Med &lt;- rep(NA, 10000) # setup vector to hold bootstrap statistics for (i in 1:10000){ BootstrapSample &lt;- sample_n(Houses, 100, replace=TRUE) # take bootstrap sample Bootstrap_Med[i] &lt;- median(BootstrapSample$price) # calculate standard deviation in bootstrap sample } Houses_Bootstrap_Results_Med &lt;- data.frame(Bootstrap_Med) #store results in data frame The bootstrap distribution for the median price is shown below, along with its standard error. Houses_Bootstrap_Med_Plot &lt;- ggplot(data=Houses_Bootstrap_Results_Med, aes(x=Bootstrap_Med)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Median Price in Bootstrap Sample &quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Bootstrap Distribution for Median Price in Seattle Houses&quot;) + theme(legend.position = &quot;none&quot;) Houses_Bootstrap_Med_Plot We’ll calculate the standard error of the median. This is a measure of how much the median varies between samples. SE_Med &lt;- sd(Houses_Bootstrap_Results_Med$Bootstrap_Med) SE_Med ## [1] 47.88898 The standard error measures the amount of variability in median house price between different samples of size 100. Note that this is different than the sample standard deviation, which represents the standard deviation in prices between the 100 different houses in the sample. Notice that the bootstrap distribution for the median is not symmetric and bell-shaped. Thus, we cannot be assured that 95% of samples will produce a statistic within two standard errors of the mean, so the standard error confidence interval method is not appropriate here. Instead, we’ll calculate a confidence interval by taking the middle 95% of the values in the bootstrap distribution. A cionfidence interval calculated this way is called a percentile bootstrap interval. We’ll calculate the 0.025 quantile and the 0.975 quantile of this bootstrap distribution. These are the points below which lie 2.5% and 97.5% of the medians in the bootstrap distribution. Thus, the middle 95% of the medians lie between these values. q.025 &lt;- quantile(Houses_Bootstrap_Results_Med$Bootstrap_Med, 0.025) q.025 ## 2.5% ## 410 q.975 &lt;- quantile(Houses_Bootstrap_Results_Med$Bootstrap_Med, 0.975) q.975 ## 97.5% ## 600 The 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below. Houses_Bootstrap_Med_Plot + geom_segment(aes(x=q.025,xend=q.975, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that the median price among all houses that sold in Seattle between 2014 and 2015 is between 410 and 600 thousand dollars. 4.3.5 CI for Difference in Means We previously calculated a confidence interval for the average mercury level among all lakes in Florida. Now, we’ll calculate an interval for the difference in average mercury levels between lakes in Northern Florida, compared to Southern Florida. The boxplot shows and table below describe the distribution of mercury levels for lakes in Northern Florida, compared to Southern Florida. LakesBP &lt;- ggplot(data=FloridaLakes, aes(x=Location, y=Mercury, fill=Location)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + coord_flip() LakesBP LakesTable &lt;- FloridaLakes %&gt;% group_by(Location) %&gt;% summarize(MeanHg=mean(Mercury), StDevHg=sd(Mercury), N=n()) LakesTable ## # A tibble: 2 × 4 ## Location MeanHg StDevHg N ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 N 0.425 0.270 33 ## 2 S 0.696 0.384 20 In our sample of 33 Northern Lakes and 20 Southern Lakes, we saw a difference of 0.27 ppm. We’ll calculate a confidence interval to estimate how big or small this difference could be among all Florida lakes. We’ll use a statistical model to calculate the average mercury levels in Northern and Southern Florida. \\(\\widehat{\\text{Mercury}} = b_0 +b_1\\times{\\text{South}}\\) \\(b_0\\) represents the mean mercury level for lakes in North Florida, and \\(b_1\\) represents the mean difference in mercury level for lakes in South Florida, compared to North Florida The estimates for corresponding to the original sample are shown below. M &lt;- lm(data=FloridaLakes, Mercury~Location) M ## ## Call: ## lm(formula = Mercury ~ Location, data = FloridaLakes) ## ## Coefficients: ## (Intercept) LocationS ## 0.4245 0.2720 Thus, we can obtain a confidence interval for the difference in average mercury levels by fitting a regression model to each of our bootstrap samples and recording the value of the sample statistic \\(b_1\\), which represents this difference. Alternatively, we could calculate the mean from each group separately and subtract. When comparing groups, we make one modification in Step #1 of the bootstrap process. Rather than drawing a sample of size \\(n\\) at random, with replacement, we’ll draw the same number of observations from each group as were in the original sample. In this case, we had 33 northern lakes, and 20 southern lakes. Bootstrap Steps Take a sample of 33 northern lakes and 20 southern lakes by randomly sampling from the original sample, with replacement. Fit a regression model with location as the explanatory variable and record the value of \\(b_1\\), representing the difference between the means for each group (South-North). Repeat steps 1 and 2 many (say 10,000) times, keeping track of the difference in means in each bootstrap sample. Look at the distribution of the differences in means across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the difference in means between mercury levels in Northern and Southern Florida. We’ll illustrate the procedure on 3 bootstrap samples. Bootstrap Sample 1 NLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==&quot;N&quot;), 33, replace=TRUE) ## sample 33 northern lakes SLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==&quot;S&quot;), 20, replace=TRUE) ## sample 20 southern lakes BootstrapSample1 &lt;- rbind(NLakes, SLakes) %&gt;% arrange(ID) %&gt;% select(ID, Lake, Location, Mercury) ## combine Northern and Southern Lakes BootstrapSample1 ## # A tibble: 53 × 4 ## ID Lake Location Mercury ## &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 Alligator S 1.23 ## 2 2 Annie S 1.33 ## 3 3 Apopka N 0.04 ## 4 6 Bryant N 0.27 ## 5 7 Cherry N 0.48 ## 6 8 Crescent N 0.19 ## 7 8 Crescent N 0.19 ## 8 11 Dorr N 0.71 ## 9 12 Down S 0.5 ## 10 12 Down S 0.5 ## # ℹ 43 more rows We fit a regression model to the bootstrap sample and calculate the regression coefficients. We’re interested in the second coefficient, \\(b_1\\), which represents the mean difference between lakes in Southern and Northern Florida Mb1 &lt;- lm(data=BootstrapSample1, Mercury ~ Location) ## fit linear model Mb1 ## ## Call: ## lm(formula = Mercury ~ Location, data = BootstrapSample1) ## ## Coefficients: ## (Intercept) LocationS ## 0.3864 0.2391 NLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==&quot;N&quot;), 33, replace=TRUE) ## sample 33 northern lakes SLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==&quot;S&quot;), 20, replace=TRUE) ## sample 20 southern lakes BootstrapSample2 &lt;- rbind(NLakes, SLakes) %&gt;% arrange(ID) %&gt;% select(ID, Lake, Location, Mercury) ## combine Northern and Southern Lakes BootstrapSample2 ## # A tibble: 53 × 4 ## ID Lake Location Mercury ## &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 Alligator S 1.23 ## 2 1 Alligator S 1.23 ## 3 2 Annie S 1.33 ## 4 3 Apopka N 0.04 ## 5 3 Apopka N 0.04 ## 6 4 Blue Cypress S 0.44 ## 7 8 Crescent N 0.19 ## 8 9 Deer Point N 0.83 ## 9 10 Dias N 0.81 ## 10 10 Dias N 0.81 ## # ℹ 43 more rows Bootstrap Sample 2 Mb2 &lt;- lm(data=BootstrapSample2, Mercury ~ Location) ## fit linear model Mb2 ## ## Call: ## lm(formula = Mercury ~ Location, data = BootstrapSample2) ## ## Coefficients: ## (Intercept) LocationS ## 0.4876 0.3934 Bootstrap Sample 3 NLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==&quot;N&quot;), 33, replace=TRUE) ## sample 33 northern lakes SLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==&quot;S&quot;), 20, replace=TRUE) ## sample 20 southern lakes BootstrapSample3 &lt;- rbind(NLakes, SLakes) %&gt;% arrange(ID) %&gt;% select(ID, Lake, Location, Mercury) ## combine Northern and Southern Lakes BootstrapSample3 ## # A tibble: 53 × 4 ## ID Lake Location Mercury ## &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 2 Annie S 1.33 ## 2 2 Annie S 1.33 ## 3 2 Annie S 1.33 ## 4 3 Apopka N 0.04 ## 5 5 Brick S 1.2 ## 6 6 Bryant N 0.27 ## 7 7 Cherry N 0.48 ## 8 9 Deer Point N 0.83 ## 9 10 Dias N 0.81 ## 10 12 Down S 0.5 ## # ℹ 43 more rows Mb3 &lt;- lm(data=BootstrapSample3, Mercury ~ Location) ## fit linear model Mb3 ## ## Call: ## lm(formula = Mercury ~ Location, data = BootstrapSample3) ## ## Coefficients: ## (Intercept) LocationS ## 0.4436 0.2259 We’ll now take 10,000 different bootstrap samples and look at the bootstrap distribution for \\(b_1\\), the difference in mean mercury levels between lakes in Southern and Northern Florida M &lt;- lm(data=FloridaLakes, Mercury~Location) #fit model to original sample Sample_b1 &lt;- M$coefficients[2] # record b1 value (second coefficient) Bootstrap_b1 &lt;- rep(NA, 10000) #vector to store b1 values for (i in 1:10000){ NLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==&quot;N&quot;), 33, replace=TRUE) ## sample 33 northern lakes SLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==&quot;S&quot;), 20, replace=TRUE) ## sample 20 southern lakes BootstrapSample &lt;- rbind(NLakes, SLakes) ## combine Northern and Southern Lakes M &lt;- lm(data=BootstrapSample, Mercury ~ Location) ## fit linear model Bootstrap_b1[i] &lt;- M$coefficients[2] ## record b1 } NS_Lakes_Bootstrap_Results &lt;- data.frame(Bootstrap_b1) #save results as dataframe The bootstrap distribution for the difference in means, \\(b_1\\), is shown below, along with the standard error for the difference. NS_Lakes_Bootstrap_Plot_b1 &lt;- ggplot(data=NS_Lakes_Bootstrap_Results, aes(x=Bootstrap_b1)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Mean Difference (b1) in Bootstrap Sample&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Northern vs Southern Lakes: Bootstrap Distribution for b1&quot;) NS_Lakes_Bootstrap_Plot_b1 Standard Error of the Difference in Means \\(b_1\\) SE_b1 &lt;- sd(NS_Lakes_Bootstrap_Results$Bootstrap_b1) SE_b1 ## [1] 0.09620419 The bootstrap distribution is symmetric and bell-shaped, so we can use the standard error method to calculate a 95% confidence interval. \\[ \\begin{aligned} &amp; b_1 \\pm 2\\times\\text{SE}(b_1) \\\\ &amp; = 0.271 \\pm 2\\times{0.095} \\end{aligned} \\] 95% Confidence Interval: c(Sample_b1 - 2*SE_b1, Sample_b1 + 2*SE_b1) ## LocationS LocationS ## 0.07954616 0.46436293 The 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below. NS_Lakes_Bootstrap_Plot_b1 + geom_segment(aes(x=Sample_b1 - 2*SE_b1,xend=Sample_b1 + 2*SE_b1, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that the mean mercury level among all lakes in Southern Florida is between 0.0795462 and 0.4643629 higher than the mean mercury level among all lakes in Northern Florida. Question: We previously performed a hypothesis test and concluded that there was evidence that mean mercury level was higher for lakes in South Florida than Northern Florida. Is this confidence interval consistent with the result of the hypothesis test? Why or why not? 4.3.6 CI for Regression Slope Recall that we examined the relationship between mercury levels and pH in Florida lakes. ggplot(data=FloridaLakes, aes(y=Mercury, x=pH)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) The regression equation is \\[ \\widehat{\\text{Mercury}} = b_0 + b_1\\times\\text{pH} \\] Regression estimates \\(b_0\\) and \\(b_1\\) are shown below. M &lt;- lm(data=FloridaLakes, Mercury~pH) M ## ## Call: ## lm(formula = Mercury ~ pH, data = FloridaLakes) ## ## Coefficients: ## (Intercept) pH ## 1.5309 -0.1523 On average, lakes with pH level 0 are expected to have a mercury level of 1.53 ppm. For each one-unit increase in pH, mercury level is expected to decrease by 0.15 ppm. These estimates are sample statistics, calculated from our sample of 53 lakes. We can think of our regression equation estimates \\(b_0\\) and \\(b_1\\) as estimates of parameters \\(\\beta_0\\) and \\(\\beta_1\\), which pertain to the slope and intercept of the regression line pertaining to the entire population of all lakes in Florida. We’ll use \\(b_0\\) and \\(b_1\\) to estimate \\(\\beta_0\\) and \\(\\beta_1\\) in the same way that we used sample proportion \\(\\hat{p}\\) to estimate population proportion \\(p\\) and sample mean \\(\\bar{x}\\) to estimate population mean \\(\\mu\\). The intercept, \\(\\beta_0\\) has little meaning here, but the slope \\(\\beta_1\\) represents the average change in mercury level for each one-unit increase in pH, among all Florida lakes. We’ll use bootstrapping to find a confidence interval for this quantity. Bootstrap Steps Take a sample of 53 lakes by randomly sampling from the original sample, with replacement. Fit a regression model with pH as the explanatory variable and record the value of slope \\(b_1\\). Repeat steps 1 and 2 many (say 10,000) times, keeping track of slope of the regression line for each bootstrap sample. Look at the distribution of the slopes across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the slope relating mercury and pH levels. We’ll illustrate the procedure on 3 bootstrap samples. Bootstrap Sample 1 BootstrapSample1 &lt;- sample_n(FloridaLakes , 53, replace=TRUE) %&gt;% arrange(ID) %&gt;% select(ID, Lake, pH, Mercury) # take bootstrap sample BootstrapSample1 ## # A tibble: 53 × 4 ## ID Lake pH Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Alligator 6.1 1.23 ## 2 1 Alligator 6.1 1.23 ## 3 1 Alligator 6.1 1.23 ## 4 2 Annie 5.1 1.33 ## 5 2 Annie 5.1 1.33 ## 6 2 Annie 5.1 1.33 ## 7 3 Apopka 9.1 0.04 ## 8 3 Apopka 9.1 0.04 ## 9 3 Apopka 9.1 0.04 ## 10 4 Blue Cypress 6.9 0.44 ## # ℹ 43 more rows We fit a regression model to the bootstrap sample and calculate the regression coefficients. We’re again interested in the second coefficient, \\(b_1\\), which now represents the slope of the regression line. Mb1 &lt;- lm(data=BootstrapSample1, Mercury ~ pH) # fit linear model Mb1 ## ## Call: ## lm(formula = Mercury ~ pH, data = BootstrapSample1) ## ## Coefficients: ## (Intercept) pH ## 1.7903 -0.1918 Bootstrap Sample 2 BootstrapSample2 &lt;- sample_n(FloridaLakes , 53, replace=TRUE) %&gt;% arrange(ID) %&gt;% select(ID, Lake, pH, Mercury) BootstrapSample2 ## # A tibble: 53 × 4 ## ID Lake pH Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Alligator 6.1 1.23 ## 2 4 Blue Cypress 6.9 0.44 ## 3 4 Blue Cypress 6.9 0.44 ## 4 4 Blue Cypress 6.9 0.44 ## 5 5 Brick 4.6 1.2 ## 6 5 Brick 4.6 1.2 ## 7 6 Bryant 7.3 0.27 ## 8 7 Cherry 5.4 0.48 ## 9 8 Crescent 8.1 0.19 ## 10 8 Crescent 8.1 0.19 ## # ℹ 43 more rows Mb2 &lt;- lm(data=BootstrapSample2, Mercury ~ pH) # fit linear model Mb2 ## ## Call: ## lm(formula = Mercury ~ pH, data = BootstrapSample2) ## ## Coefficients: ## (Intercept) pH ## 1.5724 -0.1661 Bootstrap Sample 3 BootstrapSample3 &lt;- sample_n(FloridaLakes , 53, replace=TRUE) %&gt;% arrange(ID) %&gt;% select(ID, Lake, pH, Mercury) BootstrapSample3 ## # A tibble: 53 × 4 ## ID Lake pH Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5 Brick 4.6 1.2 ## 2 6 Bryant 7.3 0.27 ## 3 6 Bryant 7.3 0.27 ## 4 7 Cherry 5.4 0.48 ## 5 8 Crescent 8.1 0.19 ## 6 9 Deer Point 5.8 0.83 ## 7 9 Deer Point 5.8 0.83 ## 8 10 Dias 6.4 0.81 ## 9 12 Down 7.2 0.5 ## 10 12 Down 7.2 0.5 ## # ℹ 43 more rows Mb3 &lt;- lm(data=BootstrapSample3, Mercury ~ pH) # fit linear model Mb3 ## ## Call: ## lm(formula = Mercury ~ pH, data = BootstrapSample3) ## ## Coefficients: ## (Intercept) pH ## 1.6215 -0.1612 We’ll now take 10,000 different bootstrap samples and look at the bootstrap distribution for \\(b_1\\), the slope of the regression line relating mercury level and pH. M &lt;- lm(data=FloridaLakes, Mercury~pH) #fit model to original sample Sample_b1 &lt;- M$coefficients[2] # record b1 value (second coefficient) Bootstrap_b1 &lt;- rep(NA, 10000) #vector to store b1 values for (i in 1:10000){ BootstrapSample &lt;- sample_n(FloridaLakes , 53, replace=TRUE) #take bootstrap sample M &lt;- lm(data=BootstrapSample, Mercury ~ pH) # fit linear model Bootstrap_b1[i] &lt;- M$coefficients[2] # record b1 } Lakes_Bootstrap_Slope_Results &lt;- data.frame(Bootstrap_b1) #save results as dataframe The bootstrap distribution for the slopes, \\(b_1\\), is shown below, along with the standard error for the difference. Lakes_Bootstrap_Plot_Slope &lt;- ggplot(data=Lakes_Bootstrap_Slope_Results, aes(x=Bootstrap_b1)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Slope in Bootstrap Sample&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Bootstrap Distribution for Slope&quot;) Lakes_Bootstrap_Plot_Slope Standard Error of the slope \\(b_1\\) SE_b1 &lt;- sd(Lakes_Bootstrap_Slope_Results$Bootstrap_b1) SE_b1 ## [1] 0.02676588 The bootstrap distribution is symmetric and bell-shaped, so we can use the standard error method to calculate a 95% confidence interval. \\[ \\begin{aligned} &amp; b_1 \\pm 2\\times\\text{SE}(b_1) \\\\ &amp; = -0.1523 \\pm 2\\times{0.027} \\end{aligned} \\] 95% Confidence Interval: c(Sample_b1 - 2*SE_b1, Sample_b1 + 2*SE_b1) ## pH pH ## -0.2058326 -0.0987691 The 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below. Lakes_Bootstrap_Plot_Slope + geom_segment(aes(x=Sample_b1 - 2*SE_b1,xend=Sample_b1 + 2*SE_b1, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that among all Florida lakes, for each 1 unit increase in pH, mercury level decreases between 0.2058326 and 0.0987691, on average. 4.3.7 CI for Regression Response In addition to calculating a confidence interval for the slope of the regression line relating mercury and pH levels in a lake, we can also calculate a confidence interval for the average mercury level among all lakes with a given pH. We’ll calculate a confidence interval for the average mercury level among all lakes with a neutral pH level of 7. The regression equation is \\[ \\begin{aligned} \\widehat{\\text{Mercury}} &amp; = b_0 + b_1\\times\\text{pH} \\\\ &amp; = 1.5309 - 0.1523\\times\\text{pH} \\end{aligned} \\] so the expected mercury level among all lakes with \\(\\text{pH} = 7\\) is \\(b_0+7b_1 = 1.5309-0.1523(7)=0.4648\\) ppm. This quantity is a statistic calculated from a sample of 53 lakes, so we would not expect the average mercury level among all lakes in the population to be exactly equal to 0.4648. Again, we’ll use this sample statistic as an estimate of the population parameter, and use bootstrapping to estimate the variability associated with this statistic, in order to make a confidence interval. Bootstrap Steps Take a sample of 53 lakes by randomly sampling from the original sample, with replacement. Fit a regression model with location as the explanatory variable and record the values of \\(b_0\\) and \\(b_1\\). Use these to calculate \\(b_0+7b_1\\). Repeat steps 1 and 2 many (say 10,000) times, keeping track of \\(b_0\\) and \\(b_1\\), and calculating \\(b_0+7b_1\\) in each bootstrap sample. Look at the distribution of the expected response, \\(b_0 + 7b_1\\), across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the expected mercury level among all lakes with pH level of 7. We’ll illustrate the procedure on 3 bootstrap samples. Bootstrap Sample 1 BootstrapSample1 &lt;- sample_n(FloridaLakes , 53, replace=TRUE) %&gt;% arrange(ID) %&gt;% select(ID, Lake, pH, Mercury) # take bootstrap sample BootstrapSample1 ## # A tibble: 53 × 4 ## ID Lake pH Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 Annie 5.1 1.33 ## 2 3 Apopka 9.1 0.04 ## 3 5 Brick 4.6 1.2 ## 4 9 Deer Point 5.8 0.83 ## 5 10 Dias 6.4 0.81 ## 6 10 Dias 6.4 0.81 ## 7 11 Dorr 5.4 0.71 ## 8 11 Dorr 5.4 0.71 ## 9 12 Down 7.2 0.5 ## 10 13 Eaton 7.2 0.49 ## # ℹ 43 more rows We fit a regression model to the bootstrap sample and calculate the regression coefficients. We’re interested in the second coefficient, \\(b_1\\), which represents the mean difference between lakes in Southern and Northern Florida Mb1 &lt;- lm(data=BootstrapSample1, Mercury ~ pH) ## fit linear model b0 &lt;- Mb1$coefficients[1] # record value of b0 (first coefficient) b1 &lt;- Mb1$coefficients[2] # record value of b1 (second coefficient) b0+7*b1 #calculate b0+7*b1 ## (Intercept) ## 0.4726683 Bootstrap Sample 2 BootstrapSample2 &lt;- sample_n(FloridaLakes , 53, replace=TRUE) %&gt;% arrange(ID) %&gt;% select(ID, Lake, pH, Mercury) BootstrapSample2 ## # A tibble: 53 × 4 ## ID Lake pH Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 Blue Cypress 6.9 0.44 ## 2 6 Bryant 7.3 0.27 ## 3 6 Bryant 7.3 0.27 ## 4 6 Bryant 7.3 0.27 ## 5 7 Cherry 5.4 0.48 ## 6 8 Crescent 8.1 0.19 ## 7 8 Crescent 8.1 0.19 ## 8 10 Dias 6.4 0.81 ## 9 10 Dias 6.4 0.81 ## 10 11 Dorr 5.4 0.71 ## # ℹ 43 more rows Mb2 &lt;- lm(data=BootstrapSample2, Mercury ~ pH) # fit linear model b0 &lt;- Mb2$coefficients[1] # record value of b0 (first coefficient) b1 &lt;- Mb2$coefficients[2] # record value of b1 (second coefficient) b0+7*b1 #calculate b0+7*b1 ## (Intercept) ## 0.4574841 Bootstrap Sample 3 BootstrapSample3 &lt;- sample_n(FloridaLakes , 53, replace=TRUE) %&gt;% arrange(ID) %&gt;% select(ID, Lake, pH, Mercury) BootstrapSample3 ## # A tibble: 53 × 4 ## ID Lake pH Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Alligator 6.1 1.23 ## 2 2 Annie 5.1 1.33 ## 3 3 Apopka 9.1 0.04 ## 4 3 Apopka 9.1 0.04 ## 5 4 Blue Cypress 6.9 0.44 ## 6 4 Blue Cypress 6.9 0.44 ## 7 4 Blue Cypress 6.9 0.44 ## 8 5 Brick 4.6 1.2 ## 9 6 Bryant 7.3 0.27 ## 10 6 Bryant 7.3 0.27 ## # ℹ 43 more rows Mb3 &lt;- lm(data=BootstrapSample3, Mercury ~ pH) # fit linear model b0 &lt;- Mb3$coefficients[1] # record value of b0 (first coefficient) b1 &lt;- Mb3$coefficients[2] # record value of b1 (second coefficient) b0+7*b1 #calculate b0+7*b1 ## (Intercept) ## 0.4077507 We’ll now take 10,000 different bootstrap samples and record the values of \\(b_0\\), \\(b_1\\), which we’ll then use to calculate \\(b_0+7b_1\\). M &lt;- lm(data=FloridaLakes, Mercury~pH) #fit model to original sample Sample_b0 &lt;- M$coefficients[1] # record b0 value (second coefficient) Sample_b1 &lt;- M$coefficients[2] # record b1 value (second coefficient) Sample_Exp7 &lt;- Sample_b0 + 7*Sample_b1 # calculate sample expected mercury when pH=7 Bootstrap_b0 &lt;- rep(NA, 10000) #vector to store b1 values Bootstrap_b1 &lt;- rep(NA, 10000) #vector to store b1 values for (i in 1:10000){ BootstrapSample &lt;- sample_n(FloridaLakes , 53, replace=TRUE) #take bootstrap sample M &lt;- lm(data=BootstrapSample, Mercury ~ pH) # fit linear model Bootstrap_b0[i] &lt;- M$coefficients[1] # record b0 Bootstrap_b1[i] &lt;- M$coefficients[2] # record b1 } Bootstrap_Exp7 &lt;- Bootstrap_b0 + 7*Bootstrap_b1 # calcualte expected response for each bootstrap sample Lakes_Bootstrap_Exp7_Results &lt;- data.frame(Bootstrap_b0, Bootstrap_b1, Bootstrap_Exp7) #save results as dataframe The bootstrap distribution for the expected mercury level among all lakes with pH level 7, \\(b_0+7b_1\\), is shown below, along with the standard error for this quantity. Lakes_Bootstrap_Plot_Exp7 &lt;- ggplot(data=Lakes_Bootstrap_Exp7_Results, aes(x=Bootstrap_Exp7)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Expected Mercury Level in Bootstrap Sample&quot;) + ylab(&quot;Frequency&quot;) + ggtitle( &quot;Bootstrap Distribution for Exp. Mercury when pH=7&quot;) Lakes_Bootstrap_Plot_Exp7 Standard Error of the expected response \\(b_0 + 7b_1\\) SE_Exp7 &lt;- sd(Lakes_Bootstrap_Exp7_Results$Bootstrap_Exp7) SE_Exp7 ## [1] 0.03676639 Again, the bootstrap distribution is symmetric and bell-shaped, so we can use the standard error method to calculate a 95% confidence interval. \\[ \\begin{aligned} &amp; b_1 \\pm 2\\times\\text{SE}(b_1) \\\\ &amp; = 0.4648 \\pm 2\\times{0.037} \\end{aligned} \\] 95% Confidence Interval: c(Sample_Exp7 - 2*SE_Exp7, Sample_Exp7 + 2*SE_Exp7) ## (Intercept) (Intercept) ## 0.3912799 0.5383454 The 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below. Lakes_Bootstrap_Plot_Exp7 + geom_segment(aes(x=Sample_Exp7 - 2*SE_Exp7,xend=Sample_Exp7 + 2*SE_Exp7, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that average mercury level among all Florida lakes with pH level 7 is between 0.3912799 and 0.5383454 ppm. Again, we are not saying that we think an individual like with a pH level of 7 will lie in this range, only that the average mercury level among all such lakes lies in this range. 4.3.8 More CI’s in Regression We saw in the previous two examples how to calculate a confidence interval for the slope of a regression line, and for an expected response in regression. In fact, we can calculate confidence intervals for any function involving regression coefficients \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\), in a similar manner. For example, let’s consider the model for Seattle house prices that involved square feet, whether or not the house was on the waterfront, and an interaction term between these variables. The model is \\[ \\widehat{Price} = b_0 + b_1\\times\\text{Sq. Ft.} + b_2\\times\\text{waterfront} + b_3\\times\\text{Sq.Ft}\\times\\text{Waterfront} \\] We fit the model and obtain the parameter estimates shown below. M &lt;- lm(data=Houses, price~sqft_living + waterfront + sqft_living:waterfront) #fit model to original sample Sample_b0 &lt;- M$coefficients[1] # record b0 value (second coefficient) Sample_b1 &lt;- M$coefficients[2] # record b1 value (second coefficient) Sample_b2 &lt;- M$coefficients[3] # record b1 value (second coefficient) Sample_b3 &lt;- M$coefficients[4] # record b1 value (second coefficient) M ## ## Call: ## lm(formula = price ~ sqft_living + waterfront + sqft_living:waterfront, ## data = Houses) ## ## Coefficients: ## (Intercept) sqft_living ## 67.3959 0.2184 ## waterfrontYes sqft_living:waterfrontYes ## -364.5950 0.4327 Consider the following quantities that we might be interested in estimating: The expected price of a 2,000 square foot waterfront house. The expected price of a 1,500 square foot non-waterfront house. The difference between the expected price of a house 1,800 square foot house on the waterfront, compared to a house the same size that is not on the waterfront. The difference in the rate of change in house prices for each additional 100 square feet for houses on the waterfront, compared to houses not on the waterfront. Each of these quantities can be expressed as a linear function of our regression coefficients \\(b_0, b_1, b_2, b_3\\). We just need to find the appropriate function of the \\(b_j\\)’s, and then calculate a bootstrap confidence interval for that quantity, using the same steps we’ve seen in the previous examples. Substituting into the regression equation, we see that: The expected price of a 2,000 square foot waterfront house is given by \\[b_0 + 2000b_1 + b_2 + 2000b_3\\] We calculate this estimate from the model, based on our sample of 100 houses: 2000*Sample_b1 +Sample_b2+2000*Sample_b3 # calculate b0+2000b1+b2+2000b3 ## sqft_living ## 937.4777 We estimate that the average price of all 2,000 square foot waterfront houses in Seattle is 937 thousand dollars. The expected price of a 1,500 square foot non-waterfront house is given by \\[b_0 + 1500b_1\\] Sample_b0 + 1500*Sample_b1 # calculate b0+1500b1+ ## (Intercept) ## 394.9499 We estimate that the average price of all 1,500 square foot non-waterfront houses in Seattle is 395 thousand dollars. The difference between the expected price of a house 1,800 square foot house on the waterfront, compared to a house the same size that is not on the waterfront is given by: \\[ \\begin{aligned} &amp; (b_0 + 1800b_1 + b_2 + 1800b_3) - (b_0 + 1800b_1) \\\\ &amp; = b_2 +1800b_3 \\end{aligned} \\] Sample_b2+1800*Sample_b3 # calculate b2+1800b3 ## waterfrontYes ## 414.2058 We estimate that on average a 1,800 square foot house on the waterfront will cost 414 thousand dollars more than a 1,800 square foot house not on the waterfront. The difference in the rate of change in house prices for each additional 100 square feet for houses on the waterfront, compared to houses not on the waterfront. This question is asking about the difference in slopes of the regression lines relating price and square feet for houses on the waterfront, compared to those not on the waterfront. For houses on the waterfront, the regression equation is \\[ \\widehat{Price} = (b_0 + b_2) + (b_1 +b_3)\\times\\text{Sq. Ft.}, \\] so the slope is \\(b_1 + b_3\\). For houses not on the waterfront, the regression equation is \\[ \\widehat{Price} = b_0 + b_1 \\times\\text{Sq. Ft.}, \\] so the slope is \\(b_1\\). These slope pertain to the expected change in price for each additional 1 square foot. So, for a 100-square foot increase, the price of a waterfront house is expected to increase by \\(100(b_1+b_3)\\), compared to an increase of \\(100b_1\\) for a non-waterfront house. Thus, the difference in the rates of change is \\(100b_3\\). 100*Sample_b3 # calculate 100b3 ## sqft_living:waterfrontYes ## 43.26671 We estimate that the price of waterfront houses increases by 43 thousand dollars more for each additional 100 square feet than the price of non-waterfront houses. These estimates calculated from the sample are statistics, which, like all the other statistics we’ve seen are likely to vary from the true values of the corresponding population parameters, due to variability between samples. We can use bootstrapping to calculate confidence intervals for the relevant population parameters, using these sample statistics (the functions of \\(b_j\\)’s), just as we’ve done for the other statistics we’ve seen. Bootstrap Steps Take a sample of 100 houses by randomly sampling from the original sample, with replacement. Fit a regression model with location as the explanatory variable and record the values of regression coefficients \\(b_0, b_1, b_2, b_3\\). Use these to calculate each of the four desired quantities (i.e. \\(b_0 + 2000b_1 + b_2 +2000b_3\\)) Repeat steps 1 and 2 many (say 10,000) times, keeping track of the regression coefficients and calculating the desired quantities in each bootstrap sample. Look at the distribution of the quantities of interest, across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for each of these quantities. We’ll illustrate the procedure on 3 bootstrap samples. Bootstrap Sample 1 We take the first bootstrap sample and fit a model with interaction. For brevity, we won’t list out the houses in each of the bootstrap samples, as the idea should be clear by now. Model coefficients are shown below. BootstrapSample1 &lt;- sample_n(Houses , 100, replace=TRUE) %&gt;% arrange(Id) %&gt;% select(Id, price, sqft_living, waterfront) Mb1 &lt;- lm(data=BootstrapSample1, price ~ sqft_living + waterfront + sqft_living:waterfront) # fit linear model with interaction b0 &lt;- Mb1$coefficients[1] # record value of b0 (first coefficient) b1 &lt;- Mb1$coefficients[2] # record value of b1 (second coefficient) b2 &lt;- Mb1$coefficients[3] # record value of b2 (third coefficient) b3 &lt;- Mb1$coefficients[4] # record value of b3 (fourth coefficient) Mb1 ## ## Call: ## lm(formula = price ~ sqft_living + waterfront + sqft_living:waterfront, ## data = BootstrapSample1) ## ## Coefficients: ## (Intercept) sqft_living ## 23.1250 0.2416 ## waterfrontYes sqft_living:waterfrontYes ## -552.9883 0.5005 We calculate each of the four desired quantities. b0+2000*b1 + b2 + 2000*b3 ## (Intercept) ## 954.3864 b0+1500*b1 ## (Intercept) ## 385.5317 b2+1800*b3 ## waterfrontYes ## 347.9484 100*b3 ## sqft_living:waterfrontYes ## 50.05204 Bootstrap Sample 2 BootstrapSample2 &lt;- sample_n(Houses , 100, replace=TRUE) %&gt;% arrange(Id) %&gt;% select(Id, price, sqft_living, waterfront) Mb2 &lt;- lm(data=BootstrapSample2, price ~ sqft_living + waterfront + sqft_living:waterfront) # fit linear model with interaction b0 &lt;- Mb2$coefficients[1] # record value of b0 (first coefficient) b1 &lt;- Mb2$coefficients[2] # record value of b1 (second coefficient) b2 &lt;- Mb2$coefficients[3] # record value of b2 (third coefficient) b3 &lt;- Mb2$coefficients[4] # record value of b3 (fourth coefficient) Mb2 ## ## Call: ## lm(formula = price ~ sqft_living + waterfront + sqft_living:waterfront, ## data = BootstrapSample2) ## ## Coefficients: ## (Intercept) sqft_living ## 97.8551 0.2133 ## waterfrontYes sqft_living:waterfrontYes ## -622.7018 0.5163 We calculate each of the four desired quantities. b0+2000*b1 + b2 + 2000*b3 ## (Intercept) ## 934.5378 b0+1500*b1 ## (Intercept) ## 417.871 b2+1800*b3 ## waterfrontYes ## 306.7251 100*b3 ## sqft_living:waterfrontYes ## 51.63483 Bootstrap Sample 3 BootstrapSample3 &lt;- sample_n(Houses , 100, replace=TRUE) %&gt;% arrange(Id) %&gt;% select(Id, price, sqft_living, waterfront) Mb3 &lt;- lm(data=BootstrapSample3, price ~ sqft_living + waterfront + sqft_living:waterfront) # fit linear model with interaction b0 &lt;- Mb3$coefficients[1] # record value of b0 (first coefficient) b1 &lt;- Mb3$coefficients[2] # record value of b1 (second coefficient) b2 &lt;- Mb3$coefficients[3] # record value of b2 (third coefficient) b3 &lt;- Mb3$coefficients[4] # record value of b3 (fourth coefficient) Mb3 ## ## Call: ## lm(formula = price ~ sqft_living + waterfront + sqft_living:waterfront, ## data = BootstrapSample3) ## ## Coefficients: ## (Intercept) sqft_living ## 15.6004 0.2456 ## waterfrontYes sqft_living:waterfrontYes ## -85.2989 0.3362 We calculate each of the four desired quantities. b0+2000*b1 + b2 + 2000*b3 ## (Intercept) ## 1093.984 b0+1500*b1 ## (Intercept) ## 384.0153 b2+1800*b3 ## waterfrontYes ## 519.9176 100*b3 ## sqft_living:waterfrontYes ## 33.62314 We’ll now take 10,000 different bootstrap samples and record the values of \\(b_0\\), \\(b_1\\), \\(b_3\\), and \\(b_4\\), which we’ll then use to calculate each of our four desired quantities. M &lt;- lm(data=Houses, price~sqft_living + waterfront + sqft_living:waterfront) #fit model to original sample Sample_b0 &lt;- M$coefficients[1] # record b0 value (second coefficient) Sample_b1 &lt;- M$coefficients[2] # record b1 value (second coefficient) Sample_b2 &lt;- M$coefficients[3] # record b1 value (second coefficient) Sample_b3 &lt;- M$coefficients[4] # record b1 value (second coefficient) Sample_Q1 &lt;- Sample_b0 + 2000*Sample_b1 +Sample_b2+2000*Sample_b3 # calculate b0+2000b1+b2+2000b3 Sample_Q2 &lt;- Sample_b0 + 1500*Sample_b1 # calculate b0+1500b1+ Sample_Q3 &lt;- Sample_b2+1800*Sample_b3 # calculate b2+1800b3 Sample_Q4 &lt;- 100*Sample_b3 # calculate 100b3 Bootstrap_b0 &lt;- rep(NA, 10000) #vector to store b0 values Bootstrap_b1 &lt;- rep(NA, 10000) #vector to store b1 values Bootstrap_b2 &lt;- rep(NA, 10000) #vector to store b2 values Bootstrap_b3 &lt;- rep(NA, 10000) #vector to store b3 values for (i in 1:10000){ BootstrapSample &lt;- sample_n(Houses, 1000, replace=TRUE) #take bootstrap sample Mb &lt;- lm(data=BootstrapSample, price ~ sqft_living + waterfront + sqft_living:waterfront) # fit linear model with interaction Bootstrap_b0[i] &lt;- Mb$coefficients[1] # record value of b0 (first coefficient) Bootstrap_b1[i] &lt;- Mb$coefficients[2] # record value of b1 (second coefficient) Bootstrap_b2[i] &lt;- Mb$coefficients[3] # record value of b2 (third coefficient) Bootstrap_b3[i] &lt;- Mb$coefficients[4] # record value of b3 (fourth coefficient) } Bootstrap_Q1 &lt;- Bootstrap_b0 + 2000*Bootstrap_b1 + Bootstrap_b2 + 2000*Bootstrap_b3 Bootstrap_Q2 &lt;- Bootstrap_b0 + 1500*Bootstrap_b1 Bootstrap_Q3 &lt;- Bootstrap_b2 + 1800*Bootstrap_b3 Bootstrap_Q4 &lt;- 100*Bootstrap_b3 Houses_Bootstrap_Results &lt;- data.frame(Bootstrap_b0, Bootstrap_b1, Bootstrap_b2, Bootstrap_b3, Bootstrap_Q1, Bootstrap_Q2 , Bootstrap_Q3 , Bootstrap_Q4) #save results as dataframe Bootstrap Distribution for b_0 + 2000b_1 + b_2 + 2000b_3 Houses_Bootstrap_Plot_Q1 &lt;- ggplot(data=Houses_Bootstrap_Results, aes(x=Bootstrap_Q1)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Expected Price of 2000 Sq. Ft. Waterfront House&quot;) + ylab(&quot;Frequency&quot;) + ggtitle( &quot;Bootstrap Distribution b0+2000b1+b2+2000b3&quot;) Houses_Bootstrap_Plot_Q1 Standard Error: SE_Q1 &lt;- sd(Houses_Bootstrap_Results$Bootstrap_Q1) SE_Q1 ## [1] 40.29367 The bootstrap distribution is symmetric and bell-shaped, so we can use the standard error method to calculate a 95% confidence interval. 95% Confidence Interval: c(Sample_Q1 - 2*SE_Q1, Sample_Q1 + 2*SE_Q1) ## (Intercept) (Intercept) ## 924.2863 1085.4610 The 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below. Houses_Bootstrap_Plot_Q1 + geom_segment(aes(x=Sample_Q1 - 2*SE_Q1,xend=Sample_Q1 + 2*SE_Q1, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that average price among all 2,000 square foot Seattle waterfront houses is between 924.2863407 and 1085.4610253 thousand dollars. Bootstrap Distribution for b_0 + 1500b_1 Houses_Bootstrap_Plot_Q2 &lt;- ggplot(data=Houses_Bootstrap_Results, aes(x=Bootstrap_Q2)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Expected Price of 1500 Sq. Ft. Non-Waterfront House&quot;) + ylab(&quot;Frequency&quot;) + ggtitle( &quot;Bootstrap Distribution b0+1500b1&quot;) Houses_Bootstrap_Plot_Q2 Standard Error: SE_Q2 &lt;- sd(Houses_Bootstrap_Results$Bootstrap_Q2) SE_Q2 ## [1] 5.859015 The bootstrap distribution is symmetric and bell-shaped, so we can use the standard error method to calculate a 95% confidence interval. 95% Confidence Interval: c(Sample_Q2 - 2*SE_Q2, Sample_Q2 + 2*SE_Q2) ## (Intercept) (Intercept) ## 383.2318 406.6679 The 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below. Houses_Bootstrap_Plot_Q2 + geom_segment(aes(x=Sample_Q2 - 2*SE_Q2,xend=Sample_Q2 + 2*SE_Q2, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that average price among all 1,500 square foot Seattle non-waterfront houses is between 383.2318261 and 406.6678858 thousand dollars. Bootstrap Distribution for b_2 + 1800b_3 Houses_Bootstrap_Plot_Q3 &lt;- ggplot(data=Houses_Bootstrap_Results, aes(x=Bootstrap_Q3)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Expected Price Difference WF vs NWF for 1800 sq. Ft. House&quot;) + ylab(&quot;Frequency&quot;) + ggtitle( &quot;Bootstrap Distribution b2+1800b3&quot;) Houses_Bootstrap_Plot_Q3 Standard Error: SE_Q3 &lt;- sd(Houses_Bootstrap_Results$Bootstrap_Q3) SE_Q3 ## [1] 40.66194 The bootstrap distribution is symmetric and bell-shaped, so we can use the standard error method to calculate a 95% confidence interval. 95% Confidence Interval: c(Sample_Q3 - 2*SE_Q3, Sample_Q3 + 2*SE_Q3) ## waterfrontYes waterfrontYes ## 332.8819 495.5297 The 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below. Houses_Bootstrap_Plot_Q3 + geom_segment(aes(x=Sample_Q3 - 2*SE_Q3,xend=Sample_Q3 + 2*SE_Q3, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that the average price among all 1800 square feet waterfront houses in Seattle is between 332.8818906 and 495.5296513 thousand dollars more than the average price among all non-waterfront houses of the same size. Bootstrap Distribution for 100b_3 Houses_Bootstrap_Plot_Q4 &lt;- ggplot(data=Houses_Bootstrap_Results, aes(x=Bootstrap_Q4)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Expected Difference per 100 square feet&quot;) + ylab(&quot;Frequency&quot;) + ggtitle( &quot;Bootstrap Distribution 100b3&quot;) Houses_Bootstrap_Plot_Q4 Standard Error: SE_Q4 &lt;- sd(Houses_Bootstrap_Results$Bootstrap_Q4) SE_Q4 ## [1] 2.224772 The bootstrap distribution is symmetric and bell-shaped, so we can use the standard error method to calculate a 95% confidence interval. 95% Confidence Interval: c(Sample_Q4 - 2*SE_Q4, Sample_Q4 + 2*SE_Q4) ## sqft_living:waterfrontYes sqft_living:waterfrontYes ## 38.81716 47.71625 The 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below. Houses_Bootstrap_Plot_Q4 + geom_segment(aes(x=Sample_Q4 - 2*SE_Q4,xend=Sample_Q4 + 2*SE_Q4, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that for each 100 square foot increase, the average price among all waterfront houses increases by between 38.8171639 and 47.7162531 thousand dollars more than the increase in average price among all non-waterfront. 4.3.9 Bootstrapping Cautions While bootstrapping is a popular and robust procedure for calculating confidence intervals, it does have cautions and limitations. We should be sure to use the bootstrap procedure appropriate for our context. A standard-error bootstrap interval is appropriate when the sampling distribution for our statistic is roughly symmetric and bell-shaped. When this is not true, a percentile bootstrap interval can be used as long as there are no gaps or breaks in the bootstrap distribution. In situations where there are gaps and breaks in the bootstrap distribution, then the bootstrap distribution may not be a reasonable approximation of the sampling distribution we are interested in. 4.4 Estimating Standard Error 4.4.1 Standard Error vs Standard Deviation Recall that standard error is the standard deviation of the distribution of a statistic (sample mean, proportion, regression coefficient, etc.). It describes the amount of variability in this statistic between samples of a given size. This is different than the sample standard deviation, which pertains to the amount of variability between individuals in the sample. For example, the histogram displays the distribution of mercury levels in our sample of 53 lakes in Florida. Lakes_Hist Standard Deviation of Mercury Levels Between Lakes: sd(FloridaLakes$Mercury) ## [1] 0.3410356 The standard deviation in mercury levels between individual lakes is 0.341 ppm. This describes the amount of variability in mercury levels between individual lakes. Bootstrap Distribution for Mean Mercury Level (\\(n=53\\)) Lakes_Bootstrap_Mean_Plot + xlim(c(0,1.5)) Standard Error for Mean: SE &lt;- sd(Lakes_Bootstrap_Results_Mean$Bootstrap_Mean); SE ## [1] 0.04655744 The standard deviation in the distribution for mean mercury levels between different samples of 53 lakes is approximately 0.0465574 ppm. This describes the amount of variability in mean mercury levels between different samples of 53 lakes. There is less variability in average among samples of size 53, than there is between mercury levels of individual lakes. Although mercury levels might vary quite a bit between individual lakes, in a sample of size 53, the higher and lower levels tend to average out to something in the middle, resulting in less variability associated with the average than with individual lakes. 4.4.2 Sample Size and Standard Error Question: Suppose the sample consisted of 10 lakes, or 30 lakes, or 100 lakes, instead of 53, and that the distribution of the lakes in the sample was otherwise similar to that of the original 53. Would you expect the mercury level of individual lakes to increase, decrease, or stay about the same? What about the standard error of the mean mercury level? The table shows the standard deviation in each of the samples. Sample_Size SD 10 0.3831797 30 0.3198103 53 0.3410356 100 0.3352758 Sample size does not impact the amount of variability between individual lakes. Standard deviation in mercury levels between individual lakes does not systematically increase or decrease based on sample size (of course it varies a little based on the lakes randomly chosen in the sample). Now, we’ll examine what happens to the standard error of the mean as the sample size changes. Distributions of Mean Between Different Samples The table shows the standard error of the mean for samples of different size: Sample_Size SE 10 0.1166601 30 0.0579587 53 0.0465574 100 0.0331806 As sample size increases, variability between means of different samples decreases. Standard error of the mean decreases. This is also true of standard errors for other statistics (i.e. difference in means, regression slopes, etc.) 4.4.3 Standard Error Formulas So far, we’ve used simulation (permutation tests and bootstrap intervals) to determine the amount of variability associated with a test statistic or estimate, in order to perform hypotheses tests and create confidence intervals. In special situations, there are mathematical formulas, based on probability theory, that can be used to approximate the amount of variability without having to perform the simulations. These approximations only exist for certain kinds of statistics, and they are only valid when the sampling distribution for the statistic is symmetric and bell-shaped. Thus, they cannot be used in all situations, and we should be careful to check whether they are appropriate before applying them. Nevertheless, when appropriate, they can allow us to bypass the computation required in a simulation. Theory-Based Standard Error Formulas Scenario Standard Error Single Mean \\(SE(b_0)=\\frac{s}{\\sqrt{n}}\\) Difference in Means \\(SE(b_j)=s\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}\\) Single Proportion \\(SE(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\) Difference in Proportions \\(SE(\\hat{p}) = \\sqrt{\\left(\\frac{\\hat{p_1}(1-\\hat{p}_1)}{n_1}+\\frac{\\hat{p_2}(1-\\hat{p_2})}{n_2}\\right)}\\) Intercept in Simple Linear Regression \\(SE(b_0)=s\\sqrt{\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum(x_i-\\bar{x})^2}}\\) Slope in Simple Linear Regression \\(SE(b_1)=\\sqrt{\\frac{s^2}{\\sum(x_i-\\bar{x})^2}}=\\sqrt{\\frac{1}{n-2}\\frac{{\\sum(\\hat{y}_i-y_i)^2}}{\\sum(x_i-\\bar{x})^2}}\\) \\(s=\\sqrt{\\frac{\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{(n-(p+1))}}\\), (p is number of regression coefficients not including \\(b_0\\)) is sample standard deviation. Note that in the one-sample case, this simplifies to the standard deviation formula we’ve seen previously. In the 2nd formula, the standard error estimate \\(s\\sqrt{\\frac{1}{n_1+n_2}}\\) is called a “pooled” estimate since it combines information from all groups. When there is reason to believe standard deviation differs between groups, we often use an “unpooled” standard error estimate of \\(\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}\\), where \\(s_1, s_2\\) represent the standard deviation for groups 1 and 2. There is no theory-based formula for standard error associated with the median or standard deviation. For these, and many other statistics, we rely on simulation to estimate variability between samples. There are formulas for standard errors associated with coefficients in multiple regression, but these require mathematics beyond what is assumed in this class. They involve linear algebra and matrix inversion, which you can read about here if you are interested. 4.4.4 One-Sample Mean Example We use the first formula to calculate the standard error of the mean mercury concentration associated with samples of 53 lakes. \\(SE(\\bar{x})=\\frac{s}{\\sqrt{n}} = \\frac{0.341}{\\sqrt{53}}=0.04684\\) The calculation in R is shown below. sd(FloridaLakes$Mercury)/sqrt(53) ## [1] 0.04684485 The standard error column in the R summary output is calculated using the theory-based formulas. M &lt;- lm(data=FloridaLakes, Mercury~1) summary(M) ## ## Call: ## lm(formula = Mercury ~ 1, data = FloridaLakes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.48717 -0.25717 -0.04717 0.24283 0.80283 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.52717 0.04684 11.25 0.00000000000000151 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.341 on 52 degrees of freedom Let’s compare this to the standard error we obtained using 10,000 bootstrap simulations. Lakes_Bootstrap_Mean_Plot Bootstrap standard error: SE &lt;- sd(Lakes_Bootstrap_Results_Mean$Bootstrap_Mean); SE ## [1] 0.04655744 The theory-based formula gives a standard error estimate very close to the one we obtained via bootstrap simulation. 4.4.5 Difference in Means Example We use the probability-based formula to calculate the standard error for difference in means between 33 lakes in North Florida, and 20 lakes in South Florida \\[ SE(\\bar{x}_1-\\bar{x}_2)=s\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}, \\] s &lt;- sqrt(sum(Lakes_M$residuals^2)/(53-2)) SE &lt;- s*sqrt(1/20+1/33); SE ## [1] 0.08984774 R model summary output: Lakes_M &lt;- lm(data=FloridaLakes, Mercury~Location) summary(Lakes_M) ## ## Call: ## lm(formula = Mercury ~ Location, data = FloridaLakes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.65650 -0.23455 -0.08455 0.24350 0.67545 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.42455 0.05519 7.692 0.000000000441 *** ## LocationS 0.27195 0.08985 3.027 0.00387 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3171 on 51 degrees of freedom ## Multiple R-squared: 0.1523, Adjusted R-squared: 0.1357 ## F-statistic: 9.162 on 1 and 51 DF, p-value: 0.003868 Comparison to Bootstrap standard error: NS_Lakes_Bootstrap_Plot_b1 &lt;- ggplot(data=NS_Lakes_Bootstrap_Results, aes(x=Bootstrap_b1)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;b1 in Bootstrap Sample&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Northern vs Southern Lakes: Bootstrap Distribution for b1&quot;) NS_Lakes_Bootstrap_Plot_b1 sd(NS_Lakes_Bootstrap_Results$Bootstrap_b1) ## [1] 0.09620419 The mathematical standard error approximation is close to the one observed in our simulation, though not exact. 4.4.6 Regression Example We use the theory-based approach to calculate standard error associated with the slope of the regression line relating mercury level and pH in Florida lakes. \\(SE(b_1)=\\sqrt{\\frac{s^2}{\\sum(x_i-\\bar{x})^2}}=\\sqrt{\\frac{1}{n-2}\\frac{{\\sum(\\hat{y}_i-y_i)^2}}{\\sum(x_i-\\bar{x})^2}}\\) M &lt;- lm(data=FloridaLakes, Mercury~pH) # fit model s2 &lt;- sum(M$residuals^2)/(53-2) #calculate s^2 x &lt;- FloridaLakes$pH SSx &lt;- sum((x-mean(x))^2) #calculate denominator SE &lt;- sqrt(s2/SSx) # calculate SE SE ## [1] 0.03031326 R model summary output: Lakes_M_pH &lt;- lm(data=FloridaLakes, Mercury~pH) summary(Lakes_M_pH) ## ## Call: ## lm(formula = Mercury ~ pH, data = FloridaLakes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.48895 -0.19188 -0.05774 0.09456 0.71134 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.53092 0.20349 7.523 0.000000000814 *** ## pH -0.15230 0.03031 -5.024 0.000006572811 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2816 on 51 degrees of freedom ## Multiple R-squared: 0.3311, Adjusted R-squared: 0.318 ## F-statistic: 25.24 on 1 and 51 DF, p-value: 0.000006573 Comparison to bootstrap standard error: Lakes_Bootstrap_Plot_Slope SE_b1 &lt;- sd(Lakes_Bootstrap_Slope_Results$Bootstrap_b1) SE_b1 ## [1] 0.02676588 The mathematical standard error approximation is again close to the one observed in our simulation. 4.4.7 Theory-Based Confidence Intervals If the sampling distribution for a statistic is symmetric and bell-shaped, we can obtain an approximate 95% confidence interval using the formula: \\[ \\text{Statistic} \\pm 2\\times{\\text{Standard Error}}, \\] where the standard error is calculated by formula, rather than via bootstrap simulations. The confint command in R provides confidence intervals associate with each of the model parameters, which are calculated using the theory-based standard errors Confidence Intervals for N vs S Florida Lakes confint(Lakes_M) ## 2.5 % 97.5 % ## (Intercept) 0.31374083 0.5353501 ## LocationS 0.09157768 0.4523314 Intercept Interval: \\(\\approx 0.42455 \\pm 2\\times0.05519\\) We are 95% confident that the average mercury level among all lakes in North Florida is between 0.313 and 0.535 ppm. LocationS Interpretation \\(\\approx -0.27195 \\pm 2\\times 0.08985\\) We are 95% confident that the average mercury level among all lakes in South Florida is between 0.09 and 0.45 ppm higher than the average mercury concentration among all lakes in North Florida. Note that the use of 2 in the confidence interval formula is an approximation, though one that is reasonable and widely used. Rather than using 2, R uses a value very close to 2, which varies depending on the sample size. Thus R’s calculations will not exactly match the formulas shown, but are similar enough for practical purposes. Remember that in statistics, everything is an approximation. Confidence Intervals for pH and Mercury in Florida Lakes confint(Lakes_M_pH) ## 2.5 % 97.5 % ## (Intercept) 1.1223897 1.93944769 ## pH -0.2131573 -0.09144445 Intercept Interval: \\(\\approx 1.53092 \\pm 2\\times 0.20349\\) We are 95% confident that the average mercury concentration among all lakes with pH level 0 is between 1.12 and 1.94 ppm. This is not a meaningful interpretation, since none of the lakes in our sample have a pH level anywhere close to 0. pH Interval \\(\\approx -0.15230 \\pm 2\\times 0.03031\\) We are 95% confident that for each one-unit increase in pH, mercury level is expected to decrease between 0.09 and 0.21 ppm, on average. 4.4.8 CI Method Comparison We’ve now seen 3 different ways to obtain confidence intervals based on statistics calculated from data. The table below tells us what must be true of the sampling distribution for a statistic in order to use each technique. Technique No Gaps Bell-Shaped Known Formula for SE Bootstrap Percentile x Bootstrap Standard Error x x Theory-Based x x x "],["normal-error-regression-model.html", "Chapter 5 Normal Error Regression Model 5.1 The Normal Error Regression Model 5.2 Inference in Normal Error Regression Model 5.3 F-Distributions 5.4 Regression Model Assumptions 5.5 Intervals for Expected Response 5.6 Transformations 5.7 Case Studies 5.8 Impact of Model Assumption Violations", " Chapter 5 Normal Error Regression Model Learning Outcomes: Explain when it is appropriate to use “theory-based” standard error formulas. Interpret estimates, standard errors, test statistics, and p-values resulting from linear model output in R. List the assumptions made in the normal error regression model. Calculate p-values corresponding to t-statistics and F-statistics in R. Interpret confidence intervals for an expected response, and prediction intervals, and distinguish between these two types of intervals. Assess the whether linear model assumptions are reasonably satisfied, using residual plots, histograms, and normal QQ plots. Explain when we should or should not expect p-values and confidence intervals obtained via “theory-based” approaches to agree with those obtained via simulation. Identify situations where a log transformation of the response variable is appropriate. Calculate predicted values for models involving a log transformation of the response variable. Interpret regression coefficients in models involving a log transformation of the response variable. Explain the regression effect. 5.1 The Normal Error Regression Model You’ve probably noticed that many (though not all) of the distributions of statistics associated with permutation-based hypothesis tests in Chapter 3 and bootstrap confidence intervals in Chapter 4 were symmetric and bell-shaped in nature. We also saw in Section 4.4 that certain statistics, such as differences in means, and regression coefficients have known standard error formulas, allowing us to approximate their standard errors without performing simulation. When working with statistics that have symmetric and bell-shaped distributions and know standard error formulas, it is possible to use well-known probability facts to obtain confidence intervals and perform hypothesis tests without actually performing the simulation seen in Chapters 3 and 4. In order to be able to use these facts, however, we must know that the sampling distribution of our statistic is in fact symmetric and bell-shaped. One way to know that would be to actually perform the simulations and check the shape of the distribution. This, of course, would defeat the purpose of bypassing the simulations, however. In this chapter, we’ll examine ways to check whether a statistic such as a mean, regression slope, or expected response will follow a symmetric and bell-shaped sampling distribution, without actually having to perform a simulation. For situations where the statistic does follow such a distribution, we’ll examine methods for obtaining confidence intervals and p-values, based on probability theory, rather than simulation. 5.1.1 Example: Ice Cream dispenser Suppose an ice cream machine is manufactured to dispense 2 oz. of ice cream per second, on average. If 15 people used the ice cream machine, holding the dispenser for different amounts of time, and each person got exactly 2 oz. per second, the relationship between time holding the dispenser and amount dispensed would look like this: In reality, however, the actual amount dispensed each time it is used will vary due to unknown factors like: force applied to dispenser temperature build-up of ice cream other unknown factors Thus, if 15 real people held the dispenser and recorded the amount of ice cream they got, the scatter plot we would see would look something like this: 5.1.2 Signal and Noise We can think of the amount of ice cream a person receives as being a result of two separate components, often referred to as signal and noise. Signal represents the average amount of ice cream a person is expected to receive based on the amount of time holding the dispenser. In this case, signal is given by the function \\(\\text{Expected Amount} = 2\\times\\text{Time}\\). Everyone who holds the dispenser for \\(t\\) seconds is expected to receive \\(2t\\) ounces of ice cream. Noise represents how much each person’s actual amount of ice cream deviates from their expected amount. For example, a person who holds the dispenser for 1.5 seconds and receives 3.58 oz. of ice cream will have received 0.58 ounces more than expected due to noise (i.e. factors beyond time holding the dispenser). In a statistical model, we assume that the response value of a response variable we observe is the sum of the signal, or expected response, which is a function of the explanatory variables in the model, and noise, which results from deviations due to factos beyond those accounted for in the model. 5.1.3 Normal Distribution It is common to model noise using a symmetric, bell-shaped distribution, known as a normal distribution. We can think of the error term as a random draw from somewhere in the area below the bell-curve. For example, in the above illustration, most of the area under the curve lies between \\(-1\\leq x\\leq 1\\). If this curve represented the noise term in the ice cream example, it would mean that most people’s actual amount of ice cream dispensed would be within \\(\\pm 1\\) ounce of their expected amount (or signal). Notice that the normal distribution is centered at 0, indicating that on average, a person would be expected to get an amount exactly equal to their signal, but that they might deviate above or below this amount by unexplained factors, which can be modeled by random chance. A normal distribution is defined by two parameters: - \\(\\mu\\) representing the center of the distribution - \\(\\sigma\\) representing the standard deviation This distribution is denoted \\(\\mathcal{N}(\\mu, \\sigma)\\). When the standard deviation is small, such as for the blue curve, noise tends to be close to 0, meaning the observed values will be close to their expectation. On the other hand, the green curve, which has higher standard deviation, would often produce noise values as extreme as \\(\\pm 2\\) or more. Note that the square of the standard deviation \\(\\sigma^2\\) is called the variance. Some books denote the normal distribution as \\(\\mathcal{N}(\\mu, \\sigma^2)\\), instead of \\(\\mathcal{N}(\\mu,\\sigma)\\). We will use the \\(\\mathcal{N}(\\mu,\\sigma)\\) here, which is consistent with R. 5.1.4 Signal and Noise in Icecream Example In this example, we’ll simulate the amount of ice cream dispensed for each person by adding a random number from a normal distribution with mean 0 and standard deviation 0.5 to the expected amount dispensed, which is given by \\(2x\\), where \\(x\\) represents time pressing the dispenser. We’ll let \\(\\epsilon_i\\) represent the random noise term for the \\(i\\) person. Thus, amount dispensed (\\(Y_i\\)) for person \\(i\\) is given by \\[Y_i = 2x_i+\\epsilon_i, \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, 0.5) \\] We simulate the amount dispensed for a sample of 15 people below. set.seed(10082020) # set times time &lt;- c(1, 1.2, 1.5, 1.8, 2.1, 2.1, 2.3, 2.5, 2.6, 2.8, 2.9, 2.9, 3.1, 3.2, 3.6) expected &lt;- 2*time # expected amount noise &lt;-rnorm(15, 0, 0.5) %&gt;% round(2) #generate noise from normal distribution amount &lt;- 2*time + noise # calculate observed amounts Icecream &lt;- data.frame(time, signal, noise, amount) # set up data table kable((Icecream)) #display table time signal noise amount 1.0 2.0 0.23 2.23 1.2 2.4 -0.49 1.91 1.5 3.0 0.58 3.58 1.8 3.6 -0.03 3.57 2.1 4.2 0.17 4.37 2.1 4.2 -0.93 3.27 2.3 4.6 0.05 4.65 2.5 5.0 -0.37 4.63 2.6 5.2 -0.46 4.74 2.8 5.6 0.17 5.77 2.9 5.8 -0.59 5.21 2.9 5.8 0.12 5.92 3.1 6.2 0.00 6.20 3.2 6.4 0.67 7.07 3.6 7.2 0.05 7.25 The scatterplot displays the amount dispensed, compared to the time pressing the dispenser. The red line indicates the line \\(y=2x\\). If there was no random noise, then each person’s amount dispensed would lie exactly on this line. ggplot(data=Icecream1, aes(x=time, y=amount)) + geom_point() + ggtitle(&quot;Icecream Dispensed&quot;) + xlab(&quot;Time Pressing dispenser&quot;) + ylab(&quot;Amount Dispensed&quot;) + geom_abline(slope=2, intercept=0, color=&quot;red&quot;) + annotate(&quot;text&quot;, label=&quot;y=2x&quot;, x= 3.5, y=6.5, size=10, color=&quot;red&quot;) In a real situation, we would not see the signal and noise columns in the table or the red line on the graph. We would only see the time and amount, and points on the scatter plot. From these, we would need to estimate the location of the red line by fitting a least squares regression line to the data, as we’ve done before. The blue line represents the location of the least squares regression line fit to the time and amounts observed. ggplot(data=Icecream1, aes(x=time, y=amount)) + geom_point() + ggtitle(&quot;Icecream Dispensed&quot;) + xlab(&quot;Time Pressing dispenser&quot;) + ylab(&quot;Amount Dispensed&quot;) + stat_smooth(method=&quot;lm&quot;, se=FALSE) + geom_abline(slope=2, intercept=0, color=&quot;red&quot;) + annotate(&quot;text&quot;, label=&quot;y=2x&quot;, x= 3.5, y=6.5, size=10, color=&quot;red&quot;) The blue line is close, but not identical to the red line, representing the true (usually unknown) signal. The slope and intercept of the blue line are given by: IC_Model &lt;- lm(data=Icecream1, lm(amount~time)) IC_Model ## ## Call: ## lm(formula = lm(amount ~ time), data = Icecream1) ## ## Coefficients: ## (Intercept) time ## -0.1299 2.0312 Notice that these estimates are close, but not identical to the intercept and slope of the red line, which are 0 and 2, respectively. The equation of the red line is given by: \\(Y_i = \\beta_0 + \\beta_1X_{i} + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\), where \\(Y_i\\) represents amount dispensed, and \\(X_i\\) represents time. \\(\\beta_0, \\beta_1,\\), and \\(\\sigma\\) are the unknown model parameters associated with the ice cream machine’s process. Using the values of \\(b_0\\) and \\(b_1\\) obtained by fitting a model to our observed data as estimates of \\(\\beta_0\\) and \\(\\beta_1\\), our estimated regression equation is \\[Y_i = b_0 + b_1X_i + \\epsilon_i = -0.1299087 + 2.0312489X_i + \\epsilon_i \\] where \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\). An estimate for \\(\\sigma\\) is given by \\(s =\\sqrt{\\frac{\\text{SSR}}{n-(p+1)}} = \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{(n-(p+1))}}\\). We calculate this estimate of \\(\\hat{\\sigma}\\), using R. s &lt;- sqrt(sum(IC_Model$residuals^2)/(15-2)) s ## [1] 0.4527185 The estimates of \\(b_0 = -0.1299087\\), \\(b_1=2.0312489\\), and \\(s = 0.4527185\\) are resonably close estimates to the values \\(\\beta_0=0, \\beta_1=2\\), and \\(\\sigma = 0.5\\), that we used to generate the data. In a real situation, we’ll have only statistics \\(b_0\\), \\(b_1\\), and \\(s\\), and we’ll need to use them to draw conclusions about parameters \\(\\beta_0=0, \\beta_1=2\\), and \\(\\sigma = 0.5\\). 5.1.5 Normal Error Regression Model In the ice cream example, the relationship between expected amount and time holding the dispenser was given by a linear equation involving a single numeric explanatory variable. We can generalize this to situations with multiple explanatory variables, which might be numeric or categorical. Individual observations are then assumed to vary from their expectation in accordance with a normal distribution, representing random noise (or error). The mathematical form of a normal error linear regression model is \\(Y_i = \\beta_0 + \\beta_1X_{i1}+ \\ldots + \\beta_pX_{ip} + \\epsilon_i\\), with \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\). Note that in place of \\(X_{ip}\\), we could have indicators for categories, or functions of \\(X_{ip}\\), such as \\(X_{ip}^2\\), \\(\\text{log}(X_{ip})\\), or \\(\\text{sin}(X_{ip})\\). The quantities \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) are parameters, pertaining to the true but unknown data generating mechanism. The estimates \\(b_0, b_1, \\ldots, b_p\\), are statistics, calculated from our observed data. We use confidence intervals and hypothesis tests to make statements about parameters, based on information provided by statistics. 5.1.6 Examples of Normal Error Regression Model We can formulate all of the examples we’ve worked with so far in terms of the normal error regression model. In the house price example, consider the following models: Model 1: \\(\\text{Price}_i = \\beta_0 + \\beta_1\\text{Sq.Ft.}_{i} + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\). If we use this model, we’re saying that we believe the expected price of a house is a linear function of its size, and that for any given size, the distribution of actual prices are normally distributed around their expected value of \\(\\beta_0 + \\beta_1\\text{Sq.Ft.}_{i}\\). Model 2: \\(\\text{Price}_i = \\beta_0 + \\beta_2\\text{Waterfront}_{i}+ \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\). If we use this model, we’re saying that we believe the expected price of a house depends only on whether or not it is on the waterfront, and that prices of both waterfront and non-waterfront houses follow normal distributions, though these distributions may have different means (\\(\\beta_0\\) for non-waterfront houses, and \\(\\beta_1\\) for waterfront houses). Model 3: \\(\\text{Price}_i = \\beta_0 + \\beta_1\\text{Sq.Ft.}_{i}+ \\beta_2\\text{Waterfront}_{i}+ \\beta_3\\times\\text{Sq.Ft.}_i\\times\\text{Waterfront}_{i} + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\). and Model 4: \\(\\text{Price}_i = \\beta_0 + \\beta_1\\text{Sq.Ft.}_{i}+ \\beta_2\\text{Waterfront}_{i}+ \\beta_3\\times\\text{Sq.Ft.}_i\\times\\text{Waterfront}_{i} + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\). Both models assume that actual prices of houses with the same size and waterfront status are normally distributed, and that the mean of the normal distribution is a linear function of its size. Model 3 allows for the intercept of the lines to differ between waterfront and non-waterfront houses, while Model 4 allows both the intercept and slope to differ. 5.1.7 Implications of Normal Error Regression Model If we really believe that data come about as the normal error regression model describes, then probability theory tells us that regression coefficients \\(b_j\\)’s, representing differences between categories for categorical variables and rates of change for quantitative variables, follow symmetric and bell-shaped distributions. We can use this fact, along with the standard error formulas in Section 4.4 to create confidence intervals and perform hypothesis tests, without needing to perform simulation. This is, in fact what R does in it’s model summary output. These methods are only valid, however, if data can reasonably be thought of as having come the normal error regression model process. Thus, if we don’t believe that our observed data can be reasonably thought of terms representing underlying signal as a linear function of explanatory variables, and a normally distributed random error (or noise) term, then the confidence intervals and p-values produced by R, and other places that rely on probability-based methods will not be reliable. 5.1.8 Philosophical Question We close the section with a philosophical question: Do data really come about from processes like the normal error regression model? That is, do you think it is reasonable to believe that data we see in the real world (perhaps the amount of ice cream dispensed by an ice cream machine) is a combination of some true, but unknown equation involving the explanatory and response variables, and some unexplained noise, coming from a normal distribution? We won’t attempt to answer that question here, but it is worth thinking about. After all, it is an assumption on which many frequently employed methods of statistical inference depends. 5.2 Inference in Normal Error Regression Model When data can reasonably be assumed to have come from a process consistent with the normal error regression model, we can perform hypothesis tests and make confidence intervals for regression coefficients \\(\\beta_j\\)’s, (which represent slopes or differences in means), using probability-based methods rather than simulation. This is done in the R output for the lm summary command. 5.2.1 lm summary Output The summary command for a linear model in R displays a table including 4 columns. Linear Model summary() Output in R Estimate gives the least-squares estimates \\(b_0, b_1, \\ldots, b_p\\) Standard Error gives estimates of the standard deviation in the sampling distribution for estimate. (i.e. how much uncertainty is there about the estimate?) These are computed using the formulas in Section 4.4. t value is the estimate divided by its standard error. Pr(&gt;|t|) is a p-value for the hypothesis test associated with the null hypothesis \\(\\beta_j = 0\\), where \\(\\beta_j\\) is the regression coefficient pertaining to the given line. Note that \\(\\beta_j\\) is the unknown population parameter estimated by \\(b_j\\). The Residual Standard Error is \\(s =\\sqrt{\\frac{\\text{SSR}}{n-(p+1)}} = \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{(n-(p+1))}}\\). This is an estimate of \\(\\sigma\\), which represents the standard deviation in the distribution of the response variable for given value(s) or category(ies) of explanatory variable(s). The degrees of freedom are \\(n-(p+1)\\). The Multiple R-Squared value is the \\(R^2\\) value seen in Chapter 2. \\(R^2 = \\frac{\\text{SST} -\\text{SSR}}{\\text{SST}} = \\frac{\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{\\displaystyle\\sum_{i=1}^n(y_i-\\bar{y}_i)^2}\\) We know that \\(R^2\\) can never decrease when additional variables are added to a model. The Adjusted-R^2 value is an alternate version of \\(R^2\\) that is designed to penalize adding variables that do little to explain variation in the response. Adjusted \\(R^2\\) can decrease as additional variables are added to a model. We’ll learn more about adjusted \\(R^2\\) in Chapter 7. The F-statistic on the bottom line of the R-output corresponds to an F-test of the given model against a reduced model that include no explanatory variables. The p-value on this line is associated with the test of the null hypothesis that there is no relationship between the response variable and any of the explanatory variables. Since SSR for this reduced model is equal to SST, the F-statistic calculation simplifies to: \\[ F=\\frac{\\frac{SSR - SST}{p}}{\\frac{SSR}{n-(n+1)}} \\] The degrees of freedom associated with the F-statistic are given by \\(p\\) and \\((n-(p+1))\\). Example: Northern vs Southern Florida Lakes Recall our linear model for mercury levels of lakes in Northern Florida, compared to Southern Florida. The equation of the model is: \\[ \\text{Mercury} = \\beta_0+\\beta_1\\times\\text{South} + \\epsilon_i, \\text{where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma) \\] We fit the model in R and display its summary output below. Lakes_M &lt;- lm(data=FloridaLakes, Mercury~Location) summary(Lakes_M) ## ## Call: ## lm(formula = Mercury ~ Location, data = FloridaLakes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.65650 -0.23455 -0.08455 0.24350 0.67545 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.42455 0.05519 7.692 0.000000000441 *** ## LocationS 0.27195 0.08985 3.027 0.00387 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3171 on 51 degrees of freedom ## Multiple R-squared: 0.1523, Adjusted R-squared: 0.1357 ## F-statistic: 9.162 on 1 and 51 DF, p-value: 0.003868 The estimated regression equation is \\[ \\text{Mercury} = 0.42455+0.27195\\times\\text{South}, \\text{where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma) \\] SSR is: sum(Lakes_M$residuals^2) ## [1] 5.126873 SST is: sum((FloridaLakes$Mercury - mean(FloridaLakes$Mercury))^2) ## [1] 6.047875 The residual standard error \\(s\\) is our estimate of \\(\\sigma\\), the standard deviation among lakes in the same location (either Northern or Southern Florida). \\[ s =\\sqrt{\\frac{\\text{SSR}}{n-(p+1)}} = \\sqrt{\\frac{\\text{SSR}}{n-(p+1)}} = \\sqrt{\\frac{5.126873}{53-(1+1)}}=0.3171 \\] The degrees of freedom associated with this estimate is \\(53-(1+1) = 51\\). The Multiple R-Squared is: \\[ R^2 = \\frac{6.047875 - 5.126873}{6.047875} = 0.1523 \\] The F-statistic is \\[ F=\\frac{\\frac{SST - SSR}{p}}{\\frac{SSR}{n-(n+1)}} = \\frac{\\frac{6.047875 - 5.126873}{1}}{\\frac{5.126873}{53-(1+1)}} = 9.162 \\] This F-statistic is associated with 1 and 51 degrees of freedom. Using formulas in Section 4.4.3, we obtain the standard error estimates for \\(b_0\\) and \\(b_1\\), given in the second column of the table. \\[ SE(b_0) = SE(\\bar{x}_N)=s\\frac{1}{\\sqrt{n_{\\text{North}}}} = \\frac{0.3171}{\\sqrt{33}} =0.0552 \\] \\(SE(b_0)\\) represents the variability in average mercury levels between different samples of 33 Northern Florida lakes. \\[ SE(b_1) = SE(\\bar{x}_{South}-\\bar{x}_{North})=s\\sqrt{\\frac{1}{n_{North}}+\\frac{1}{n_{South}}} = 0.3171\\sqrt{\\frac{1}{20} + \\frac{1}{33}} =0.0898 \\] \\(SE(b_1)\\) represents the variability in average difference in mercury levels between northern and southern lakes between different samples of 33 Northern Florida lakes and 20 Southern Florida lakes. The last column, labeled “Pr(&gt;|t|)” is, in fact a p-value associated with associated with the null hypothesis that the regression parameter on that line is zero. (i.e. \\(\\beta_j=0\\)). Hypothesis Test for line (intercept) Null Hypothesis: The average mercury level among all lakes in North Florida is 0 (\\(\\beta_0=0\\)). Alternative Hypothesis: The average mercury level among all lakes in Northern Florida is not 0 (\\(\\beta_0\\neq 0\\)). We already know the average mercury level among all lakes in North Florida is not 0, so this is a silly test. Hypothesis Test for line LocationS Null Hypothesis: There is no difference in average mercury levels between Northern and Southern Florida (\\(\\beta_1=0\\)). Alternative Hypothesis: There is a difference in average mercury levels in Northern and Southern Florida (\\(\\beta_1\\neq 0\\)). This test is relevant to us. R does not obtain these p-values through simulation, but rather by using the symmetric and bell-shaped t-distribution to approximate the distribution of these statistics. This is appropriate when the sampling distribution for our test statistic is reasonably symmetric and bell-shaped. You’ve probably noticed that the sampling distributions in our permutation-based hypothesis tests, and our bootstrap distributions for regression coefficients have often been roughly symmetric and bell-shaped. When this happens, we can use a symmetric and bell-shaped distribution to model the distribution of a test statistic when the null hypothesis is true, bypassing the need to use simulation. There is statistical theory which shows that if data really do come from the normal error regression model process, like the ice cream dispenser in the previous section, then the ratio of regression coefficients (means, differences in means, slopes) divided by their standard error, will follow a symmetric bell-shaped distribution called a t-distribution. 5.2.2 t-distribution A t-distribution is a symmetric, bell-shaped curve, with thicker tails (hence more variability), than a \\(\\mathcal{N}(0,1)\\) distribution. The t-distribution depends on a parameter called degrees of freedom, which determines the thickness of the distribution’s tails. For data that come from a normal error regression model, we can use a t-distribution to approximate the sampling distribution used in our hypothesis tests, when the null hypothesis is assumed to be true. Important Fact: If \\(Y_i = \\beta_0 + \\beta_1X_{i1}+ \\ldots + \\beta_pX_{ip} + \\epsilon_i\\), with \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\), then \\[ t= \\frac{{b_j}}{\\text{SE}(b_j)} \\] follows a t-distribution. The \\(t=\\frac{{b_j}}{\\text{SE}(b_j)}\\) is called a t-statistic. We’ll use this t-statistic as the test statistic in our hypothesis test. The degrees of freedom are given by \\(n-(p+1)\\), where \\(p\\) represents the number of terms in the model, not including the intercept. 5.2.3 Difference in Means Example Recall the hypothesis test we performed to investigate whether there is a difference in average mercury level between lakes in Northern Florida and Southern Florida. Null Hypothesis: There is no difference in average mercury levels between Northern and Southern Florida (\\(\\beta_1=0\\)). Alternative Hypothesis: There is a difference in average mercury levels in Northern and Southern Florida (\\(\\beta_1\\neq 0\\)). Test Statistic: \\(t=\\frac{{b_j}}{\\text{SE}(b_j)} = \\frac{0.27195}{0.08985} = 3.027\\) Key Question: What is the probability of getting a t-statistic as extreme as 3.027 if \\(\\beta_1=0\\) (i.e. there is no difference in mercury levels between northern and southern lakes). We plot the t-statistic of 3.027 that we observed in our data and observe where it lies on a t-distribution. ts=3.027 gf_dist(&quot;t&quot;, df=51, geom = &quot;area&quot;, fill = ~ (abs(x)&lt; abs(ts)), show.legend=FALSE) + geom_vline(xintercept=c(ts, -ts), color=&quot;red&quot;) + xlab(&quot;t&quot;) 2*pt(-abs(ts), df=51) ## [1] 0.003866374 The low p-value gives us strong evidence of a difference in average mercury levels between lakes in Northern and Southern Florida. This is the p-value reported in R’s lm summary() output. A t-statistic more extreme than \\(\\pm 2\\) will roughly correspond to a p-value less than 0.05. *Comparison to Simulation Let’s compare these results to those given by the permutation test and bootstrap confidence interval. Permutation Test NSLakes_SimulationResultsPlot p-value: b1 &lt;- Lakes_M$coef[2] ## record value of b1 from actual data mean(abs(NSLakes_SimulationResults$b1Sim) &gt; abs(b1)) ## [1] 0.0039 5.2.4 Simple Linear Regression Example We examine the model summary output for the model predicting a lake’s mercury level, using pH as the explanatory variable. M_pH &lt;- lm(data=FloridaLakes, Mercury~pH) summary(M_pH) ## ## Call: ## lm(formula = Mercury ~ pH, data = FloridaLakes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.48895 -0.19188 -0.05774 0.09456 0.71134 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.53092 0.20349 7.523 0.000000000814 *** ## pH -0.15230 0.03031 -5.024 0.000006572811 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2816 on 51 degrees of freedom ## Multiple R-squared: 0.3311, Adjusted R-squared: 0.318 ## F-statistic: 25.24 on 1 and 51 DF, p-value: 0.000006573 The estimated regression equation is \\[ \\text{Mercury} = 1.53 - 0.15 \\times\\text{pH}, \\text{where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma) \\] SSR is: sum(M_pH$residuals^2) ## [1] 4.045513 SST is: sum((FloridaLakes$Mercury - mean(FloridaLakes$Mercury))^2) ## [1] 6.047875 The residual standard error \\(s\\) is our estimate of \\(\\sigma\\), the standard deviation among lakes with the same pH. \\[ s =\\sqrt{\\frac{\\text{SSR}}{n-(p+1)}} = \\sqrt{\\frac{\\text{SSR}}{n-(p+1)}} = \\sqrt{\\frac{4.045513}{53-(1+1)}}=0.2816 \\] The degrees of freedom associated with this estimate is \\(53-(1+1) = 51\\). The Multiple R-Squared is: \\[ R^2 = \\frac{6.047875 - 4.045513}{6.047875} = 0.3311 \\] The F-statistic is \\[ F=\\frac{\\frac{SST - SSR}{p}}{\\frac{SSR}{n-(n+1)}} = \\frac{\\frac{6.047875 - 4.045513}{1}}{\\frac{4.045513}{53-(1+1)}} = 25.24 \\] This F-statistic is associated with 1 and 51 degrees of freedom. Using formulas in Section 4.4.3, we obtain the standard error estimates for \\(b_0\\) and \\(b_1\\), given in the second column of the table. To do this, we need to calculate \\(\\bar{x}\\) and \\(\\sum(x_i-\\bar{x})^2\\), where \\(x\\) represents the explanatory variable, \\(pH\\). mean(FloridaLakes$pH) ## [1] 6.590566 sum((FloridaLakes$pH-mean(FloridaLakes$pH))^2) ## [1] 86.32528 \\[ SE(b_0)=s\\sqrt{\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum(x_i-\\bar{x})^2}} = 0.2816\\sqrt{\\frac{1}{53} + \\frac{6.59^2}{86.32528} } = 0.2034 \\] \\(SE(b_0)\\) represents the variability in mercury levels among lakes with pH of 0 between different samples of size 53. Since we don’t have any lakes with pH of 0, this is not a meaningful calculation. \\[ SE(b_1)=\\sqrt{\\frac{s^2}{\\sum(x_i-\\bar{x})^2}}=\\sqrt{\\frac{0.2816^2}{86.32528}} = 0.0303 \\] \\(SE(b_1)\\) represents the variability in rate of change in mercury level for each additional one unit increase in pH, between different samples of size 53. Hypothesis Test for Intercept Line Null Hypothesis: The average mercury level among all Florida lakes with pH = 0 is 0. (\\(\\beta_0=0\\)). Alternative Hypothesis: The average mercury level among all Florida lakes with pH = 0 not 0. (\\(\\beta_0 \\neq 0\\)). Since there are no lakes with pH level 0, this is not a meaningful test. Hypothesis Test for pH Line Null Hypothesis: There is no relationship between mercury and pH level among all Florida lakes. (\\(\\beta_1=0\\)). Alternative Hypothesis: There is a relationship between mercury and pH level among all Florida lakes. (\\(\\beta_1 \\neq 0\\)). Test Statistic: \\(t=\\frac{{b_j}}{\\text{SE}(b_j)} = \\frac{-0.15230}{0.03031} = -5.024\\) ts=5.024 gf_dist(&quot;t&quot;, df=51, geom = &quot;area&quot;, fill = ~ (abs(x)&lt; abs(ts)), show.legend=FALSE) + geom_vline(xintercept=c(ts, -ts), color=&quot;red&quot;) + xlab(&quot;t&quot;) 2*pt(-abs(ts), df=51) ## [1] 0.000006578117 The p-value is extremely small, just as the simulation-based p-value we saw in Chapter 3. 5.2.5 Multiple Regression Example We perform hypothesis tests on a model predicting house price using square feet and waterfront status as explanatory variables. M_wf_sqft &lt;- lm(data=Houses, price~sqft_living+waterfront) summary(M_wf_sqft) ## ## Call: ## lm(formula = price ~ sqft_living + waterfront, data = Houses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1363.79 -251.55 59.28 177.58 1599.72 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -407.6549 86.2868 -4.724 0.00000779668 *** ## sqft_living 0.4457 0.0353 12.626 &lt; 0.0000000000000002 *** ## waterfrontYes 814.3613 124.8546 6.522 0.00000000313 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 412.7 on 97 degrees of freedom ## Multiple R-squared: 0.7607, Adjusted R-squared: 0.7558 ## F-statistic: 154.2 on 2 and 97 DF, p-value: &lt; 0.00000000000000022 We won’t go through the standard error calculations here, though the details are given in the link provided in Section 4.4. Intercept Line: Null Hypothesis The average price among all non-waterfront houses with 0 square feet is 0 dollars. (\\(\\beta_0=0\\)) This is not a sensible hypothesis to test. sqft_living Line: Null Hypothesis There is no relationship between price and square feet in a house, after accounting for waterfront status. (\\(\\beta_1=0\\)) The large t-statistic (12.626) and small p-value provide strong evidence against this null hypothesis. We know that a small p-value alone does not provide evidence of a relationship that is practically meaningful, but since our model estimates an expected 45 thousand dollar increase for each additional 100 square feet, this seems like a meaningful relationship. waterfrontYes Line: Null Hypothesis On average, there is no difference between average price of waterfront and non-waterfront houses, assuming they are the same size. (\\(\\beta_2=0\\)) The large t-statistic (6.522) and small p-value provide strong evidence against this null hypothesis. Waterfront houses are estimated to cost 814 thousand dollars more, on average, than non-waterfront houses of the same size. 5.2.6 MR with Interaction Example M_House_Int &lt;- lm(data=Houses, price ~ sqft_living * waterfront) summary(M_House_Int) ## ## Call: ## lm(formula = price ~ sqft_living * waterfront, data = Houses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1559.34 -114.93 -30.24 131.09 1266.58 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 67.39594 91.39267 0.737 0.4627 ## sqft_living 0.21837 0.04035 5.412 0.00000045752269 *** ## waterfrontYes -364.59498 180.75875 -2.017 0.0465 * ## sqft_living:waterfrontYes 0.43267 0.05566 7.773 0.00000000000857 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 325 on 96 degrees of freedom ## Multiple R-squared: 0.8531, Adjusted R-squared: 0.8486 ## F-statistic: 185.9 on 3 and 96 DF, p-value: &lt; 0.00000000000000022 Intercept Line: Null Hypothesis The average price among all non-waterfront houses with 0 square feet is 0 dollars. (\\(\\beta_0=0\\)) This is not a sensible hypothesis to test. sqft_living Line: Null Hypothesis There is no relationship between price and square feet among non-waterfront houses. (\\(\\beta_1=0\\)) The large t-statistic (5.412) and small p-value provide strong evidence against this null hypothesis. waterfrontYes Line: Null Hypothesis On average, there is no difference between average price of waterfront and non-waterfront houses with 0 square feet. (\\(\\beta_2=0\\)) This is not a sensible hypothesis to test. sqft_living:waterfrontYes Null Hypothesis: There is no interaction between square feet and waterfront. (\\(\\beta_3=0\\)) (That is, the effect of size on price is the same for waterfront and non-waterfront houses). The large t-statistic (7.773) and small p-value provide strong evidence against this null hypothesis. It appears there really is evidence of an interaction between price and waterfront status, as we previously suspected, based on graphical representation and background knowledge. Note that if the interaction term had yielded a large p-value, indicating a lack of evidence of an interaction, we might have wanted to drop the interaction term from the model, in order to make the interpretations of the other estimates and hypothesis tests simpler. 5.2.7 Limitations We’ve seen that in situations where the sampling distribution for a regression coefficient \\(b_j\\) is symmetric and bell-shaped, we can create confidence intervals and perform hypothesis tests using the t-distribution without performing permutation for hypothesis tests, or bootstrapping for confidence intervals. There are, however, limitations to this approach, which underscore the importance of the simulation-based approaches seen in Chapters 3 and 4. These include: There are lots of statistics, like medians and standard deviations, that do not have known standard error formulas, and do not follow symmetric bell-shaped distributions. In more advanced and complicated models, it is common to encounter statistics of interest with unknown sampling distributions. In these cases, we can estimate p-values and build confidence intervals via simulation, even if we cannot identify the distribution by name. Even for statistics with known standard error formulas, the t-test is only appropriate when the sampling distribution for \\(b_j\\) is symmetric and bell-shaped. While there is probability theory that shows this will happen when the sample size is “large enough,” there is no set sample size that guarantees this. Datasets with heavier skewness in the response variable will require larger sample sizes than datasets with less skewness in the response. The simulation-based approaches provide valuable insight to the logic behind hypothesis tests. When we permute values of an explanatory variable in a hypothesis test it is clear that we are simulating a situation where the null hypothesis is true. Likewise, when we simulate taking many samples in bootstrapping, it is clear that we are assessing the variability in a statistic across samples. Simply jumping to the t-based approximations of these distributions makes it easy to lose our sense of what they actually represent, and thus increases the likelihood of interpreting them incorrectly. In fact prominent statistician R.A. Fisher wrote of simulation-based methods in 1936: ``Actually, the statistician does not carry out this very simple and very tedious process, but his conclusions have no justification beyond the fact that they agree with those which could have been arrived at by this elementary method.\" Fisher’s comment emphasizes the fact that probability-based tests, like the t-test are simply approximations to what we would obtain via simulation-based approaches, which were not possible in his day, but are now. Proponents of simulation-based inference include Tim Hesterberg, Senior Statistician at Instacart, and former Senior Statistician at Google, which heavily used simulation-based tests associated with computer experiments associated with their search settings. Hesterberg wrote a 2015 paper, arguing for the use and teaching of simulation-based techniques. We will move forward by using probability-based inference where appropriate, while understanding that we are merely approximating what we would obtain via simulation. Meanwhile, we’ll continue to employ simulation-based approaches where probability-based techniques are inappropriate or unavailable. 5.3 F-Distributions Just as we’ve seen that the ratio of a regression statistic to its standard error follows a t-distribution when can be thought of as having come from a process that can be approximated with the normal error regression model, F-statistics also follow a known probability distribution under this assumption. 5.3.1 F-Distribution An F distribution is a right-skewed distribution. It is defined by two parameters, \\(\\nu_1, \\nu_2\\), called numerator and denominator degrees of freedom. Important Fact: If \\(Y_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2{X_i2} + \\ldots + \\beta_qX_{iq} + \\epsilon_i\\), with \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\), and \\(Y_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2{X_i2} + \\ldots + \\beta_qX_{iq} + \\beta_{q+1}X_{i{q+1}} \\ldots + \\beta_pX_{ip}+ \\epsilon_i\\), is another proposed model, then \\[ F=\\frac{\\frac{\\text{Unexplained Variability in Reduced Model}-\\text{Unexplained Variability in Full Model}}{p-q}}{\\frac{\\text{Unexplained Variability in Full Model}}{n-(p+1)}} \\] follows an F-distribution. The numerator and denominator degrees of freedom are given by \\(p-q\\) and \\(n-(p+1)\\), respectively. These are the same values we divided by when computing the F-statistic. 5.3.2 House Condition Example Recall our F-statistic for comparing prices of houses in either very good, good, or average condition, calculated in Section 2.5.3 and the simulation-based F-test associated with this statistic that we performed in Section 3.2.5. Null Hypothesis: There is no difference in average prices between houses of the three different conditions, among all houses in King County, WA. Alternative Hypothesis: There is a difference in average prices between houses of the three different conditions, among all houses in King County, WA. Reduced Model: \\(\\text{Price}= \\beta_0 + \\epsilon_i , \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\\) Full Model: \\(\\text{Price}= \\beta_0+ \\beta_1 \\times\\text{good condition}+ \\beta_2\\times\\text{very good condition} + \\epsilon_i , \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\\) \\[ \\begin{aligned} F &amp;= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\\\ &amp;=\\frac{\\frac{69,045,634-68,195,387}{2-0}}{\\frac{68,195,387}{100-(2+1)}} \\\\ \\end{aligned} \\] ((SST - SSR_cond)/(2-0))/(SSR_cond/(100-(2+1))) ## [1] 0.6046888 The results of the simulation-based hypothesis test are shown below. House_Cond_SimulationResults_Plot simulation-based p-value: mean(FSim &gt; Fstat) ## [1] 0.5568 Now, we calculate the p-value using the probability-based F-distribution. ts=0.605 gf_dist(&quot;f&quot;, df1=2, df2=97, geom = &quot;area&quot;, fill = ~ (abs(x)&lt; abs(ts)), show.legend=FALSE) + geom_vline(xintercept=c(ts), color=&quot;red&quot;) + xlab(&quot;F&quot;) p-value: 1-pf(ts, df1=2, df2=97) ## [1] 0.5481219 The p-value we obtained is very similar to the one we obtained using the simulation-based test. We can obtain this p-value directly using the anova command. M_cond &lt;- lm(data=Houses, price ~ condition) M0 &lt;- lm(data=Houses, price ~ 1) anova(M0, M_cond) ## Analysis of Variance Table ## ## Model 1: price ~ 1 ## Model 2: price ~ condition ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 99 69045634 ## 2 97 68195387 2 850247 0.6047 0.5483 5.3.3 Interaction Example We can also use an F-test to compare a model predicting house prices with an interaction term to one without one. Reduced Model: \\(\\text{Price}= \\beta_0+ \\beta_1 \\times\\text{sqft_living} + \\beta_2\\times\\text{Waterfront} + \\epsilon_i , \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\\) Full Model: \\(\\text{Price}= \\beta_0+ \\beta_1 \\times\\text{sqft_living}+ \\beta_2\\times\\text{Waterfront} + \\beta_3\\times\\text{sqft_living}\\times\\text{Waterfront} + \\epsilon_i , \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\\) \\[ \\begin{aligned} F &amp;= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\\\ &amp;=\\frac{\\frac{16,521,296-10,139,974}{3-2}}{\\frac{10,139,974}{100-(3+1)}} \\\\ \\end{aligned} \\] ((SSR_wf_sqft-SSR_int)/(3-2))/((SSR_int)/(100-(3+1))) ## [1] 60.41505 ts=60.41505 gf_dist(&quot;f&quot;, df1=1, df2=96, geom = &quot;area&quot;, fill = ~ (abs(x)&gt;! abs(ts)), show.legend=FALSE) + geom_vline(xintercept=c(ts), color=&quot;red&quot;) + xlab(&quot;F&quot;) p-value: 1-pf(ts, df1=1, df2=96) ## [1] 0.000000000008572476 The probability-based F-test is used in the anova command. M_wf_SqFt &lt;- lm(data=Houses, price~sqft_living + waterfront) M_House_Int &lt;- lm(data=Houses, price~sqft_living * waterfront) anova(M_wf_SqFt, M_House_Int) ## Analysis of Variance Table ## ## Model 1: price ~ sqft_living + waterfront ## Model 2: price ~ sqft_living * waterfront ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 97 16521296 ## 2 96 10139974 1 6381323 60.415 0.000000000008572 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notice that this p-value is identical to the one we obtained in the previous section, using the lm command. 5.4 Regression Model Assumptions 5.4.1 Regression Assumptions Let’s think carefully about what we are assuming in order to use the hypothesis tests and confidence intervals associated with the normal error regression model. The statement \\(Y_i = \\beta_0 + \\beta_1X_{i1}+ \\ldots + \\beta_pX_{ip} + \\epsilon_i\\), with \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\) implies the following: Linearity: the expected value of \\(Y\\) is a linear function of \\(X_1, X_2, \\ldots, X_p\\). (This assumption is only relevent for models including at least one quantitative explanatory variable.) Normality: Given the values of \\(X_1, X_2, \\ldots, X_p\\), \\(Y\\) follows a normal distribution. Constant Variance: Regardless of the values of \\(X_1, X_2, \\ldots, X_p\\), the variance (or standard deviation) in the normal distribution for \\(Y\\) is the same. Independence: each observation is independent of the rest. Illustration of Model Assumptions We know that these assumptions held true in the ice cream example, because we generated the data in a way that was consistent with these. In practice, we will have only the data, without knowing the exact mechanism that produced it. We should only rely on the t-distribution based p-values and confidence intervals in the R output if these appear to be reasonable assumptions. Of course, these assumptions will almost never be truly satisfied, but they should at least be a reasonable approximation if we are to draw meaningful conclusions. 5.4.2 Checking Model Assumptions The following plots are useful when assessing the appropriateness of the normal error regression model. Scatterplot of residuals against predicted values Histogram of standardized residuals heavy skewness indicates a problem with normality assumption Normal quantile plot severe departures from diagonal line indicate problem with normality assumption Residual vs Predicted Plots A residual vs predicted plot is useful for detecting issues with the linearity or constant variance assumption. curvature indicates a problem with linearity assumption “funnel” or “megaphone” shape indicates problem with constant variance assumption P1 &lt;- ggplot(data=Violations, aes(y=no_viol_Model$residuals, x=no_viol_Model$fitted.values)) + geom_point() + ggtitle(&quot;No Violation&quot;) + xlab(&quot;Predicted Values&quot;) + ylab(&quot;Residuals&quot;) P2 &lt;- ggplot(data=Violations, aes(y=lin_viol_Model$residuals, x=no_viol_Model$fitted.values)) + geom_point() + ggtitle(&quot;Violation of Linearity Assumption&quot;)+ xlab(&quot;Predicted Values&quot;) + ylab(&quot;Residuals&quot;) P3 &lt;- ggplot(data=Violations, aes(y=cvar_viol_Model$residuals, x=no_viol_Model$fitted.values)) + geom_point() + ggtitle(&quot;Violation of Constant Variance Assumption&quot;)+ xlab(&quot;Predicted Values&quot;) + ylab(&quot;Residuals&quot;) grid.arrange(P1, P2, P3, ncol=3) If there is only one explanatory variable, plotting the residuals against that variable reveals the same information as a residual vs predicted plot. Histogram of Residuals A histogram of the residuals is useful for assessing the normality assumption. Severe skewness indicates violation of normality assumption P1 &lt;- ggplot(data=Violations, aes(x=no_viol_Model$residuals)) + geom_histogram() + ggtitle(&quot;No Violation&quot;) +xlab(&quot;Residual&quot;) P2 &lt;- ggplot(data=Violations, aes(x=norm_viol_Model$residuals)) + geom_histogram() + ggtitle(&quot;Violation of Normality Assumption&quot;) + xlab(&quot;Residual&quot;) grid.arrange(P1, P2, ncol=2) Normal Quantile-Quantile (QQ) Plot Sometimes histograms can be inconclusive, especially when sample size is smaller. A Normal quantile-quantile plot displays quantiles of the residuals against the expected quantiles of a normal distribution. Severe departures from diagonal line indicate a problem with normality assumption. P1 &lt;- ggplot(data=Violations, aes(sample = scale(no_viol_Model$residuals))) + stat_qq() + stat_qq_line() + xlab(&quot;Normal Quantiles&quot;) + ylab(&quot;Residual Quantiles&quot;) + ggtitle(&quot;No Violation&quot;) + xlim(c(-4,4)) + ylim(c(-4,4)) P2 &lt;- ggplot(data=Violations, aes(sample = scale(norm_viol_Model$residuals))) + stat_qq() + stat_qq_line() + xlab(&quot;Normal Quantiles&quot;) + ylab(&quot;Residual Quantiles&quot;) + ggtitle(&quot;Violation of Normality Assumption&quot;) + xlim(c(-4,4)) + ylim(c(-4,4)) grid.arrange(P1, P2, ncol=2) Checking Model Assumptions - Independence Independence is often difficult to assess through plots of data, but it is important to think about whether there were factors in the data collection that would cause some observations to be more highly correlated than others. For example: People in the study who are related. Some plants grown in the same greenhouse and others in different greenhouses. Some observations taken in same time period and others at different times. All of these require more complicated models that account for correlation using spatial and time structure. 5.4.3 Summary of Checks for Model Assumptions Model assumption How to detect violation Linearity Curvature in residual plot Constant Variance Funnel shape in residual plot Normality Skewness in histogram of residuals or departure from diag. line in QQ plot Independence No graphical check, carefully examine data collection 5.4.4 Example: N v S Lakes Recall our sample of 53 Florida Lakes, 33 in the north, and 20 in the south. \\(\\text{Mercury}_i = \\beta_0 + \\beta_1\\times\\text{I}_{\\text{South}_i} + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\\). LakesBP When we use the normal error regression model, we are assuming the following: Linearity: there is an expected mercury concentration for lakes in North Florida, and another for lakes in South Florida. Normality: mercury concentrations of individual lakes in the north are normally distributed, and so are mercury concentrations in the south. These normal distributions might have different means. Constant Variance: the normal distribution for mercury concentrations in North Florida has the same standard deviation as the normal distribution for mercury concentrations in South Florida Independence: no two lakes are any more alike than any others, except for being in the north or south, which we account for in the model. We might have concerns about this, do to some lakes being geographically closer to each other than others. We should only use the p-values and confidence intervals provided by R, which depend on the t-distribution approximation, if we believe these assumptions are reasonable. A residual by predicted plot, histogram of residuals, and normal quantile-quantile plot are shown below. P1 &lt;- ggplot(data=FloridaLakes, aes(y=Lakes_M$residuals, x=Lakes_M$fitted.values)) + geom_point() + ggtitle(&quot;Residual vs Predicted Plot&quot;) + xlab(&quot;Predicted Values&quot;) + ylab(&quot;Residuals&quot;) P2 &lt;- ggplot(data=FloridaLakes, aes(x=Lakes_M$residuals)) + geom_histogram() + ggtitle(&quot;Histogram of Residuals&quot;) + xlab(&quot;Residual&quot;) P3 &lt;- ggplot(data=FloridaLakes, aes(sample = scale(Lakes_M$residuals))) + stat_qq() + stat_qq_line() + xlab(&quot;Normal Quantiles&quot;) + ylab(&quot;Residual Quantiles&quot;) + ggtitle(&quot;Normal QQ Plot&quot;) grid.arrange(P1, P2, P3, ncol=3) Notice that we see two lines of predicted values and residuals. This makes sense since all lakes in North Florida will have the same predicted value, as will all lakes in Southern Florida. There appears to be a little more variability in residuals for Southern Florida (on the right), than Northern Florida, causing some concern about the constant variance assumption. Overall, though, the assumptions seem mostly reasonable. We shouldn’t be concerned about using theory-based hypothesis tests or confidence intervals for the mean mercury level or difference in mean mercury levels. There might be some concern that prediction intervals could be either too wide or too narrow, but this is not a major concern, since the constant variance assumption is not severe. 5.4.5 Example: pH Model Recall the regression line estimating the relationship between a lake’s mercury level and pH. \\(\\text{Mercury}_i = \\beta_0 + \\beta_1\\times\\text{pH}_i + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\\). The model assumes: Linearity: the expected mercury level of a lake is a linear function of pH. Normality: for any given pH, the mercury levels of lakes with that pH follow a normal distribution. For example, mercury levels for lakes with pH of 6 is are normally distributed, and mercury levels for lakes with pH of 9 are normally distributed, though these normal distributions may have different means. Constant Variance: the variance (or standard deviation) in the normal distribution for mercury level is the same for each pH. For example, there is the same amount of variability associated with lakes with pH level 6, as pH level 8. Independence: no two lakes are any more alike than any others, except with respect to pH, which is accounted for in the model. This may not be a reasonable assumption, but it’s unclear what the effects of such a violation would be. We should only use the p-values and confidence intervals provided by R, which depend on the t-distribution approximation, if we believe these assumptions are reasonable. The plots for checking these assumptions are shown below. P1 &lt;- ggplot(data=FloridaLakes, aes(y=M_pH$residuals, x=M_pH$fitted.values)) + geom_point() + ggtitle(&quot;Residual vs Predicted Plot&quot;) + xlab(&quot;Predicted Values&quot;) + ylab(&quot;Residuals&quot;) P2 &lt;- ggplot(data=FloridaLakes, aes(x=M_pH$residuals)) + geom_histogram() + ggtitle(&quot;Histogram of Residuals&quot;) + xlab(&quot;Residual&quot;) P3 &lt;- ggplot(data=FloridaLakes, aes(sample = scale(M_pH$residuals))) + stat_qq() + stat_qq_line() + xlab(&quot;Normal Quantiles&quot;) + ylab(&quot;Residual Quantiles&quot;) + ggtitle(&quot;Normal QQ Plot&quot;) grid.arrange(P1, P2, P3, ncol=3) The residual vs predicted plot does not show any linear trend, and variability appears to be about the same for low predicted values as for high ones. Thus, the linearity and constant variance assumptions appear reasonable. The histogram shows some right-skewness, and the right-most points on the normal-qq plot are above the line, indicating a possible concern with the normality assumption. There is some evidence of right-skewness, which might impact the appropriatness of the normal error regression model. Nevertheless, we obtained similar results using the simulation-based results as the normal error regression model, suggesting that the concern about normality did not have much impact on the estimation of \\(\\beta_1\\). It is possible that this concern could have implications for other kinds of inference, such as confidence intervals for an expected response, and prediction intervals, which we’ll explore later in the chapter. 5.4.6 Example: House Prices Recall the model for estimating price of a house, using size, waterfront status, and an interaction term. \\(\\text{Price}_i = \\beta_0 + \\beta_1\\text{Sq.Ft.}_{i}+ \\beta_2\\text{Waterfront}_{i}+ \\beta_3\\times\\text{Sq.Ft.}_i\\times\\text{Waterfront}_{i} + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\). The model assumes: Linearity: the expected price of a house is a linear function of its size. The slope and intercept of this function may be different for houses on the waterfront, compared to houses not on the waterfront. Normality: prices of houses of a given size and waterfront status are normally distributed. Constant Variance: the variance (or standard deviation) in the normal distribution for prices is the same for all sizes and waterfront statuses. Independence: no two houses are any more alike than any others, except with respect to size and waterfront status. We should only use the p-values and confidence intervals provided by R, which depend on the t-distribution approximation, if we believe these assumptions are reasonable. Several reasons come to mind that might cause us to doubt the validity of these assumptions, but let’s investigate them emperically, using our data on 100 houses. The plots for checking these assumptions are shown below. P1 &lt;- ggplot(data=Houses, aes(y=M_House_Int$residuals, x=M_House_Int$fitted.values)) + geom_point() + ggtitle(&quot;Residual vs Predicted Plot&quot;) + xlab(&quot;Predicted Values&quot;) + ylab(&quot;Residuals&quot;) P2 &lt;- ggplot(data=Houses, aes(x=M_House_Int$residuals)) + geom_histogram() + ggtitle(&quot;Histogram of Residuals&quot;) + xlab(&quot;Residual&quot;) P3 &lt;- ggplot(data=Houses, aes(sample = scale(M_House_Int$residuals))) + stat_qq() + stat_qq_line() + xlab(&quot;Normal Quantiles&quot;) + ylab(&quot;Residual Quantiles&quot;) + ggtitle(&quot;Normal QQ Plot&quot;) grid.arrange(P1, P2, P3, ncol=3) Although we might have had some initial concerns about the model assumptions, the plots do not raise any serious concerns. There is no sign of a nonlinear relationship in the residual vs predicted plot, so the linearity assumption appears reasonable. There is possibly more variability associated with prices or more expensive houses than less expensive ones, so we might have some concerns about constant variance, but since there are only a few very high-priced houses, and the increasing variance is not too severe, this may not be much of a concern. There are a few houses on each end of the normal qq plot that deviate from their expected line, but not very many. It’s not uncommon to have a few points deviate from the line on the end, so we do not have severe concerns about normality. The histogram of residuals is roughly symmetric. Thus, the normal error regression model appears to be reasonable for these data. 5.5 Intervals for Expected Response 5.5.1 Parameter Values and Expected Responses Recall that in Chapter 4, we saw two different types of confidence intervals. One type was for regression parameters, \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\), using estimate \\(b_0, b_1, \\ldots, b_p\\). The other type was for expected responses, which involved estimating linear functions of these parameters, for example \\(\\beta_0 + 7\\beta_1\\). Under the assumptions of a normal error regression model, we an approximate 95% confidence interval for regression parameter \\(\\beta_j\\) is given by \\[ b_j + \\pm t^*\\text{SE}(b_j), \\] where \\(t^*\\approx 2\\). We’ve seen that in R, confidence intervals for regression parameters can be obtained through the confint() command. A 95% confidence interval for an expected response \\(E(Y_i|X_{i1}=x_{i1}, \\ldots X_{ip}=x_{ip}) = \\beta_0 + \\beta_1x_{i1} + \\ldots + \\beta_px_{ip}\\) is estimated by \\[ b_0 + b_1x_{i1} + \\ldots + b_px_{ip} + \\pm t^*\\text{SE}(b_0 + b_1x_{i1} + \\ldots + b_px_{ip}), \\] In this section, we’ll look more into confidence intervals for expected responses and also another kind of interval involving expected responses, called a prediction interval. For notational purposes, well sometimes write \\(E(Y_i|X_{i1}=x_{i1}, \\ldots X_{ip}=x_{ip})\\) as \\(E(Y|X)\\) 5.5.2 Estimation and Prediction Recall the ice cream dispenser that is known to dispense ice cream at a rate of 2 oz. per second on average, with individual amounts varying according to a normal distribution with mean 0 and standard deviation 0.5 Consider the following two questions: On average, how much ice cream will be dispensed for people who press the dispenser for 1.5 seconds? If a single person presses the dispenser for 1.5 seconds, how much ice cream will be dispensed? The first question is one of estimation. The second pertains to prediction. When estimating expected responses and making predictions on new observations, there are two sources of variability we must consider. We are using data to estimate \\(\\beta_0\\) and \\(\\beta_1\\), which introduces sampling variability. Even if we did know \\(\\beta_0\\) and \\(\\beta_1\\), there is variability in individual observations, which follows a \\(\\mathcal{N}(0, \\sigma)\\) distribution. In an estimation problem, we only need to think about (1). When predicting the value of a single new observation, we need to think about both (1) and (2). Thus, intervals for predictions of individual observations carry more uncertainty and are wider than confidence intervals for \\(E(Y|X)\\). 5.5.3 Estimation and Prediction in SLR In the estimation setting, we are trying to determine the location of the regression line for the entire population. Uncertainty comes from the fact that we only have data from a sample. In the ice cream example, we can see that the blue line, fit to our data, is a good approximation of the “true” regression line that pertains to the mechanism from which the data were generated. It does, however, vary from the red line slightly due to sampling variability. ggplot(data=Icecream1, aes(x=time, y=amount)) + geom_point() + ggtitle(&quot;Icecream Dispensed&quot;) + xlab(&quot;Time Pressing dispenser&quot;) + ylab(&quot;Amount Dispensed&quot;) + geom_abline(slope=2, intercept=0, color=&quot;red&quot;) + stat_smooth(method=&quot;lm&quot;) summary(IC_Model) ## ## Call: ## lm(formula = lm(amount ~ time), data = Icecream1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.8645 -0.3553 0.0685 0.2252 0.6963 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.1299 0.3968 -0.327 0.749 ## time 2.0312 0.1598 12.714 0.0000000104 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4527 on 13 degrees of freedom ## Multiple R-squared: 0.9256, Adjusted R-squared: 0.9198 ## F-statistic: 161.6 on 1 and 13 DF, p-value: 0.00000001042 b0 &lt;- IC_Model$coefficients[1] b1 &lt;- IC_Model$coefficients[2] s &lt;- sigma(IC_Model) The first question: “On average, how much ice cream will be dispensed for people who press the dispenser for 1.5 seconds?” is a question of estimation. It is of the form, for a given \\(X\\), on average what do we expect to be true of \\(Y\\). In the ice cream question, we can answer this exactly, since we know \\(\\beta_0\\) and \\(\\beta_1\\). In a real situation, we don’t know these and have to estimate them from the data, which introduces uncertainty. Confidence interval for \\(E(Y | (X=x))\\): \\[ \\begin{aligned} &amp; b_0+b_1x^* \\pm t^*SE(\\hat{Y}|X=x^*) \\\\ &amp; b_0+b_1x^* \\pm t^*\\sqrt{\\widehat{Var}(\\hat{Y}|X=x^*)} \\end{aligned} \\] The second question is a question of prediction. Even if we knew the true values of \\(beta_0\\) and \\(\\beta_1\\), we would not be able to given the exact amount dispensed for an individual user, since this varies between users. Prediction interval for \\(E(Y | (X=x))\\): \\[ \\begin{aligned} &amp; b_0+b_1x^* \\pm t^*\\sqrt{\\widehat{Var}(\\hat{Y}|X=x^*) + s^2} \\end{aligned} \\] The extra \\(s^2\\) in the calculation of prediction variance comes from the uncertainty associated with individual observations. 5.5.4 Intervals in R In R, we can obtain confidence intervals for an expected response and prediction intervals for an individual response using the predict command, with either interval=\"confidence\" or interval=\"prediction\". predict(IC_Model, newdata=data.frame(time=1.5), interval = &quot;confidence&quot;, level=0.95) ## fit lwr upr ## 1 2.916965 2.523728 3.310201 We are 95% confident that the mean amount of ice cream dispensed when the dispenser is held for 1.5 seconds is between 2.52 and 3.31 oz. predict(IC_Model, newdata=data.frame(time=1.5), interval = &quot;prediction&quot;, level=0.95) ## fit lwr upr ## 1 2.916965 1.862832 3.971097 We are 95% confident that in individual who holds the dispenser for 1.5 seconds will get between 1.86 and 3.97 oz of ice cream. The prediction interval (in red) is wider than the confidence interval (in blue), since it must account for variability between individuals, in addition to sampling variability. 5.5.5 SLR Calculations (Optional) In simple linear regression, \\[ \\begin{aligned} SE(\\hat{Y}|X=x^*) = \\sqrt{\\frac{1}{n}+ \\frac{(x^*-\\bar{x})^2}{\\displaystyle\\sum_{i=1}^n(x_i-\\bar{x})^2}} \\end{aligned} \\] Thus a confidence interval for \\(E(Y | (X=x))\\) is: \\[ \\begin{aligned} &amp; b_0+b_1x^* \\pm t^*SE(\\hat{Y}|X=x^*) \\\\ &amp; = b_0+b_1x^* \\pm 2s\\sqrt{\\frac{1}{n}+ \\frac{(x^*-\\bar{x})^2}{\\displaystyle\\sum_{i=1}^n(x_i-\\bar{x})^2}} \\ \\end{aligned} \\] A prediction interval for \\(E(Y | (X=x))\\) is: \\[\\beta_0 + \\beta_1x^* \\pm t^* s\\sqrt{\\left(\\frac{1}{n}+ \\frac{(x^*-\\bar{x})^2}{\\displaystyle\\sum_{i=1}^n(x_i-\\bar{x})^2}\\right) + 1} \\] Calculations in Icecream example For \\(x=1.5\\), a confidence interval is: \\[ \\begin{aligned} &amp; b_0+b_1x^* \\pm t^*SE(\\hat{Y}|X=x^*) \\\\ &amp; = b_0+b_1x^* \\pm 2s\\sqrt{\\frac{1}{n}+ \\frac{(x^*-\\bar{x})^2}{\\displaystyle\\sum_{i=1}^n(x_i-\\bar{x})^2}} \\\\ &amp; = -0.1299087 + 2.0312489 \\pm 20.4527185 \\sqrt{\\frac{1}{15}+ \\frac{(1.5-2.3733)^2}{8.02933}} \\end{aligned} \\] A prediction interval is: \\[ \\begin{aligned} &amp; b_0+b_1x^* \\pm t^*SE(\\hat{Y}|X=x^*) \\\\ &amp; = b_0+b_1x^* \\pm 2s\\sqrt{\\frac{1}{n}+ \\frac{(x^*-\\bar{x})^2}{\\displaystyle\\sum_{i=1}^n(x_i-\\bar{x})^2}} \\\\ &amp; = -0.1299087 + 2.0312489 \\pm 20.4527185 \\sqrt{\\left(\\frac{1}{15}+ \\frac{(1.5-2.3733)^2}{8.02933}\\right)+1} \\end{aligned} \\] 5.5.6 Car Price and Acceleration Time We consider data from the Kelly Blue Book, pertaining to new cars, released in 2015. We’ll investigate the relationship between price, length, and time it takes to accelerate from 0 to 60 mph. Price represents the price of a standard (non-luxury) model of a car. Acc060 represents time it takes to accelerate from 0 to 60 mph. CarsA060 &lt;- ggplot(data=Cars2015, aes(x=Acc060, y=Price)) + geom_point() CarsA060 + stat_smooth(method=&quot;lm&quot;, se=FALSE) \\(Price = \\beta_0 + \\beta_1\\times\\text{Acc. Time} , \\text{where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\\) The model assumes expected price is a linear function of acceleration time. Parameter Interpretations: \\(\\beta_0\\) represents intercept of regression line, i.e. expected price of a car that can accelerate from 0 to 60 mph in no time. This is not a meaningful interpretation in context. \\(\\beta_1\\) represents slope of regression line, i.e. expected change in price for each additional second it takes to accelerate from 0 to 60 mph. Cars_M_A060 &lt;- lm(data=Cars2015, Price~Acc060) summary(Cars_M_A060) ## ## Call: ## lm(formula = Price ~ Acc060, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.512 -6.544 -1.265 4.759 27.195 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 89.9036 5.0523 17.79 &lt;0.0000000000000002 *** ## Acc060 -7.1933 0.6234 -11.54 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.71 on 108 degrees of freedom ## Multiple R-squared: 0.5521, Adjusted R-squared: 0.548 ## F-statistic: 133.1 on 1 and 108 DF, p-value: &lt; 0.00000000000000022 ** Model Interpretations** \\(\\widehat{Price} = b_0 + b_1\\times\\text{Acc. Time}\\) \\(\\widehat{Price} = 89.90 - 7.193\\times\\text{Acc. Time}\\) Intercept \\(b_0\\) might be interpreted as the price of a car that can accelerate from 0 to 60 in no time, but this is not a meaningful interpretation since there are no such cars. \\(b_1=-7.1933\\) tells us that on average, the price of a car is expected to decrease by 7.19 thousand dollars for each additional second it takes to accelerate from 0 to 60 mph. \\(R^2 = 0.5521\\) tells us that 55% of the variation in price is explained by the linear model using acceleration time as the explanatory variable. What is a reasonable range for the average price of all new 2015 cars that can accelerate from 0 to 60 mph in 7 seconds? If a car I am looking to buy can accelerate from 0 to 60 mph in 7 seconds, what price range should I expect? What is a reasonable range for the average price of all new 2015 cars that can accelerate from 0 to 60 mph in 7 seconds? predict(Cars_M_A060, newdata=data.frame(Acc060=7), interval=&quot;confidence&quot;, level=0.95) ## fit lwr upr ## 1 39.5502 37.21856 41.88184 We are 95% confident that the average price of new 2015 cars that accelerate from 0 to 60 mph in 7 seconds is between 37.2 and 41.9 thousand dollars. Note: this is a confidence interval for \\(\\beta_0 -7\\beta_1\\). If a car I am looking to buy can accelerate from 0 to 60 mph in 7 seconds, what price range should I expect? predict(Cars_M_A060, newdata=data.frame(Acc060=7), interval=&quot;prediction&quot;, level=0.95) ## fit lwr upr ## 1 39.5502 18.19826 60.90215 We are 95% confident that a single new 2015 car that accelerates from 0 to 60 mph in 7 seconds will cost between 18.2 and 60.9 thousand dollars. 5.5.7 Florida Lakes Est. and Pred. Calculate an interval that we are 95% confident contains the mean mercury concentration for all lakes in Northern Florida. Do the same for Southern Florida. Calculate an interval that we are 95% confident contains the mean mercury concentration for an individual lake in Northern Florida. Do the same for a lake in Southern Florida. predict(Lakes_M, newdata=data.frame(Location=c(&quot;N&quot;, &quot;S&quot;)), interval=&quot;confidence&quot;, level=0.95) ## fit lwr upr ## 1 0.4245455 0.3137408 0.5353501 ## 2 0.6965000 0.5541689 0.8388311 We are 95% confident that the mean mercury level in North Florida is between 0.31 and 0.54 ppm. We are 95% confident that the mean mercury level in South Florida is between 0.55 and 0.84 ppm. Note: these are confidence intervals for \\(\\beta_0\\), and \\(\\beta_0 + \\beta_1\\), respectively. predict(Lakes_M, newdata=data.frame(Location=c(&quot;N&quot;, &quot;S&quot;)), interval=&quot;prediction&quot;, level=0.95) ## fit lwr upr ## 1 0.4245455 -0.22155101 1.070642 ## 2 0.6965000 0.04425685 1.348743 We are 95% confident that an individual lake in North Florida will have mercury level between 0 and 1.07 ppm. We are 95% confident that the mean mercury level in South Florida is between 0.04 and 1.35 ppm. Note that the normality assumption, which allows for negative mercury levels leads to a somewhat nonsensical result. 5.6 Transformations In this section, we’ll explore an approach we can sometimes use when model assumptions are not satisfied. 5.6.1 Cars Assumptions Check P1 &lt;- ggplot(data=Cars2015, aes(y=Cars_M_A060$residuals, x=Cars_M_A060$fitted.values)) + geom_point() + ggtitle(&quot;Residual Plot&quot;) + xlab(&quot;Predicted Values&quot;) + ylab(&quot;Residuals&quot;) P2 &lt;- ggplot(data=Cars2015, aes(x=Cars_M_A060$residuals)) + geom_histogram() + ggtitle(&quot;Histogram of Residuals&quot;) + xlab(&quot;Residual&quot;) P3 &lt;- ggplot(data=Cars2015, aes(sample = scale(Cars_M_A060$residuals))) + stat_qq() + stat_qq_line() + xlab(&quot;Normal Quantiles&quot;) + ylab(&quot;Residual Quantiles&quot;) + ggtitle(&quot;Normal QQ Plot&quot;) grid.arrange(P1, P2, P3, ncol=3) There is a funnel-shape in the residual plot, indicating a concern about the constant variance assumption. There appears to be more variability in prices for more expensive cars than for cheaper cars. There is also some concern about the normality assumption, as the histogram and QQ plot indicate right-skew in the residuals. 5.6.2 Log Transformation When residual plots yield model inadequacy, we might try to correct these by applying a transformation to the response variable. When working a nonnegative, right-skewed response variable, it is often helpful to work with the logarithm of the response variable. Note: In R, log() denotes the natural (base e) logarithm, often denoted ln(). We can actually use any logarithm, but the natural logarithm is commonly used. 5.6.3 Log Transform for Car Prices \\[ \\text{Log Price} = \\beta_0 + \\beta_1\\times \\text{Acc060} + \\epsilon_i , \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma) \\] ggplot(data=Cars2015, aes(x=Acc060, y=log(Price))) + geom_point() + xlab(&quot;Acceleration Time&quot;) + ylab(&quot;Log of Price&quot;) + ggtitle(&quot;Acceleration Time and Log Price&quot;) + stat_smooth(method=&quot;lm&quot;, se=FALSE) Cars_M_Log &lt;- lm(data=Cars2015, log(Price)~Acc060) summary(Cars_M_Log) ## ## Call: ## lm(formula = log(Price) ~ Acc060, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.84587 -0.19396 0.00908 0.18615 0.53350 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.13682 0.13021 39.45 &lt;0.0000000000000002 *** ## Acc060 -0.22064 0.01607 -13.73 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.276 on 108 degrees of freedom ## Multiple R-squared: 0.6359, Adjusted R-squared: 0.6325 ## F-statistic: 188.6 on 1 and 108 DF, p-value: &lt; 0.00000000000000022 Log Transformation Model - What We’re Assuming Linearity: the log of expected price of a car is a linear function of its acceleration time. Normality: for any given acceleration time, the log of prices of actual cars follow a normal distribution. Constant Variance: the normal distribution for log of price is the same for all acceleration times. Independence: no two cars are any more alike than any others. We should only use the p-values and confidence intervals provided by R, which depend on the t-distribution approximation, if we believe these assumptions are reasonable. Assumption Check for Model on Log Price P1 &lt;- ggplot(data=Cars2015, aes(y=Cars_M_Log$residuals, x=Cars_M_Log$fitted.values)) + geom_point() + ggtitle(&quot;Cars Log Model Residual Plot&quot;) + xlab(&quot;Predicted Values&quot;) + ylab(&quot;Residuals&quot;) P2 &lt;- ggplot(data=Cars2015, aes(x=Cars_M_Log$residuals)) + geom_histogram() + ggtitle(&quot;Histogram of Residuals&quot;) + xlab(&quot;Residual&quot;) P3 &lt;- ggplot(data=Cars2015, aes(sample = scale(Cars_M_Log$residuals))) + stat_qq() + stat_qq_line() + xlab(&quot;Normal Quantiles&quot;) + ylab(&quot;Residual Quantiles&quot;) + ggtitle(&quot;Cars Model QQ Plot&quot;) grid.arrange(P1, P2, P3, ncol=3) There is still some concern about constant variance, though perhaps not as much. The normality assumption appears more reasonable. 5.6.4 Interpretations in Log Model \\[ \\text{Log Price} = \\beta_0 + \\beta_1\\times \\text{Acc060} + \\epsilon_i , \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma) \\] Thus 5.6.5 Log Model Predictions Prediction Equation: \\[ \\begin{aligned} \\widehat{\\text{Price}} &amp; = e^{5.13582-0.22064 \\times \\text{Acc060}} \\end{aligned} \\] Predicted price for car that takes 7 seconds to accelerate: \\[ \\begin{aligned} \\widehat{\\text{Price}} &amp; = e^{5.13582-0.22064 \\times \\text{7}} = 36.3 \\end{aligned} \\] Predicted price for car that takes 10 seconds to accelerate: \\[ \\begin{aligned} \\widehat{\\text{Price}} &amp; = e^{5.13582-0.22064 \\times \\text{10}}= 18.7 \\end{aligned} \\] Predictions are for log(Price), so we need to exponentiate. predict(Cars_M_Log, newdata=data.frame(Acc060=c(7))) ## 1 ## 3.592343 exp(predict(Cars_M_Log, newdata=data.frame(Acc060=c(7)))) ## 1 ## 36.31908 A car that accelerates from 0 to 60 mph in 7 seconds is expected to cost 36.3 thousand dollars. 5.6.6 Log Model Interpretations \\[ \\begin{aligned} \\text{ Expected Price} &amp; = e^{\\beta_0 + \\beta_1\\times \\text{Acc060} } \\\\ &amp; e^{\\beta_0}e^{\\beta_1 \\times \\text{Acc060}} \\\\ &amp; e^{\\beta_0}(e^{\\beta_1})^\\text{Acc060} \\end{aligned} \\] \\(e^{\\beta_0}\\) is theoretically the expected price of a car that can accelerate from 0 to 60 mph in no time, but this is not a meaningful interpretation. For each additional second it takes a car to accelerate, price is expected to multiply by a factor of \\(e^{b_1}\\). For each additional second in acceleration time, price is expected to multiply by a a factor of \\(e^{-0.22} = 0.80\\). Thus, each 1-second increase in acceleration time is estimated to be associated with a 20% drop in price, on average. 5.6.7 Log Model CI for \\(\\beta_0\\), \\(\\beta_1\\) confint(Cars_M_Log) ## 2.5 % 97.5 % ## (Intercept) 4.8787105 5.3949208 ## Acc060 -0.2524862 -0.1887916 We are 95% confident that the price of a car changes, on average, by multiplicative factor between \\(e^{-0.252} = 0.7773\\) and \\(e^{-0.189}=0.828\\) for each additional second in acceleration time. That is, we believe the price decreases between 17% and 23% on average for each additional second in acceleration time. 5.6.8 Log Model CI for Expected Response predict(Cars_M_Log, newdata=data.frame(Acc060=c(7)), interval=&quot;confidence&quot;) ## fit lwr upr ## 1 3.592343 3.53225 3.652436 exp(predict(Cars_M_Log, newdata=data.frame(Acc060=c(7)), interval=&quot;confidence&quot;)) ## fit lwr upr ## 1 36.31908 34.20083 38.56852 We are 95% confident that the mean price amoung all cars that accelerate from 0 to 60 mph in 7 seconds is between \\(e^{3.53225} =34.2\\) and \\(e^{3.652436}=38.6\\) thousand dollars. 5.6.9 Log Model Prediction Interval predict(Cars_M_Log, newdata=data.frame(Acc060=c(7)), interval=&quot;prediction&quot;) ## fit lwr upr ## 1 3.592343 3.042041 4.142645 exp(predict(Cars_M_Log, newdata=data.frame(Acc060=c(7)), interval=&quot;prediction&quot;)) ## fit lwr upr ## 1 36.31908 20.94796 62.96917 We are 95% confident that the expected price for a car that accelerates from 0 to 60 mph in 7 seconds is between \\(e^{3.04} =20.9\\) and \\(e^{4.14}=63.9\\) thousand dollars. 5.6.10 Confidence Interval Comparison 95% Confidence interval for average price of cars that take 7 seconds to accelerate: Original Model: predict(Cars_M_A060, newdata=data.frame(Acc060=7), interval=&quot;confidence&quot;, level=0.95) ## fit lwr upr ## 1 39.5502 37.21856 41.88184 Transformed Model: exp(predict(Cars_M_Log, newdata=data.frame(Acc060=c(7)), interval=&quot;confidence&quot;, level=0.95)) ## fit lwr upr ## 1 36.31908 34.20083 38.56852 5.6.11 Prediction Interval Comparison 95% Prediction interval for price of an individual car that takes 7 seconds to accelerate: Original Model: predict(Cars_M_A060, newdata=data.frame(Acc060=7), interval=&quot;prediction&quot;, level=0.95) ## fit lwr upr ## 1 39.5502 18.19826 60.90215 Transformed Model: exp(predict(Cars_M_Log, newdata=data.frame(Acc060=c(7)), interval=&quot;prediction&quot;, level=0.95)) ## fit lwr upr ## 1 36.31908 20.94796 62.96917 Notice that the transformed interval is not symmetric and allows for a longer “tail” on the right than the left. 5.6.12 Log Model Visualization The log model suggests an nonlinear trend in price with respect to acceleration time and gives wider confidence and prediction intervals for cars that accelerate faster and tend to be more expensive. It also gives non-symmetric intervals. These results appear to be consistent with the observed data. 5.6.13 Comments on Transformations We could have used another transformation, such as \\(\\sqrt{\\text{Price}}\\) The log tranform leads to a nice interpretation involving percent change. Other transformations might yield better predictions, but are often hard to interpret. There is often a tradeoff between model complexity and interpretability. We’ll talk more about this. We did an example of a transformation in a model with a single explanatory variable. If the explanatory variable is categorical: - \\(e^{\\beta_0}\\) represents the expected response in the baseline category - \\(e^{\\beta_j}\\) represents the number of times larger the expected response in category \\(j\\) is, compared to the baseline category. When working with multiple regression models, it is still important to mention holding other variables constant when interpreting parameters associated with one of the variables. 5.7 Case Studies In this section, we’ll examine three case studies and think about what we should or shouldn’t conclude in each situation. 5.7.1 Flights from NY to CHI A traveler lives in New York and wants to fly to Chicago. They consider flying out of two New York airports: Newark (EWR) LaGuardia (LGA) We have data on the times of flights from both airports to Chicago’s O’Hare airport from 2013 (more than 14,000 flights). Assuming these flights represent a random sample of all flights from these airports to Chicago, consider how the traveler might use this information to decide which airport to fly out of. library(nycflights13) data(flights) flights$origin &lt;- as.factor(flights$origin) flights$dest &lt;- as.factor(flights$dest) Flights_NY_CHI &lt;- flights %&gt;% filter(origin %in% c(&quot;EWR&quot;, &quot;LGA&quot;) &amp; dest ==&quot;ORD&quot;) %&gt;% select(origin, dest, air_time) p1 &lt;- ggplot(data=Flights_NY_CHI, aes(x=air_time, fill=origin, color=origin)) + geom_density(alpha=0.2) + ggtitle(&quot;Flight Time&quot;) p2 &lt;- ggplot(data=Flights_NY_CHI, aes(x=air_time, y=origin)) + geom_boxplot() + ggtitle(&quot;Flight Time&quot;) grid.arrange(p1, p2, ncol=2) library(knitr) T &lt;- Flights_NY_CHI %&gt;% group_by(origin) %&gt;% summarize(Mean_Airtime = mean(air_time, na.rm=TRUE), SD = sd(air_time, na.rm=TRUE), n=sum(!is.na(air_time))) kable(T) origin Mean_Airtime SD n EWR 113.2603 9.987122 5828 LGA 115.7998 9.865270 8507 Model for Flights Data M_Flights &lt;- lm(data=Flights_NY_CHI, air_time~origin) summary(M_Flights) ## ## Call: ## lm(formula = air_time ~ origin, data = Flights_NY_CHI) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.26 -7.26 -1.26 5.20 84.74 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 113.2603 0.1299 872.06 &lt;0.0000000000000002 *** ## originLGA 2.5395 0.1686 15.06 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.915 on 14333 degrees of freedom ## (622 observations deleted due to missingness) ## Multiple R-squared: 0.01558, Adjusted R-squared: 0.01551 ## F-statistic: 226.9 on 1 and 14333 DF, p-value: &lt; 0.00000000000000022 Confidence Interval confint(M_Flights) ## 2.5 % 97.5 % ## (Intercept) 113.00572 113.514871 ## originLGA 2.20905 2.869984 Question: If you were flying from New York to Chicago, would this information influence which airport you would fly out of? If so, which would you be more likely to choose? 5.7.2 Smoking During Pregnancy Many studies have shown that a mother’s smoking during pregnancy puts a baby at risk of low birth weight. We have data on the from a sample of 80 babies born in North Carolina in 2004. Thirty of the mothers were smokers, and fifty were nonsmokers. p1 &lt;- ggplot(data=NCBirths, aes(x=weight, fill=habit, color=habit)) + geom_density(alpha=0.2) + ggtitle(&quot;Birthweight and Smoking&quot;) p2 &lt;- ggplot(data=NCBirths, aes(x=weight, y=habit)) + geom_boxplot() + ggtitle(&quot;Birthweight and Smoking&quot;) grid.arrange(p1, p2, ncol=2) library(knitr) T &lt;- NCBirths %&gt;% group_by(habit) %&gt;% summarize(Mean_Weight = mean(weight), SD = sd(weight), n=n()) kable(T) habit Mean_Weight SD n nonsmoker 7.039200 1.709388 50 smoker 6.616333 1.106418 30 Model for Birth Weights Data M_Birthwt &lt;- lm(data=NCBirths, weight~habit) summary(M_Birthwt) ## ## Call: ## lm(formula = weight ~ habit, data = NCBirths) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.0392 -0.6763 0.2372 0.8280 2.4437 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.0392 0.2140 32.89 &lt;0.0000000000000002 *** ## habitsmoker -0.4229 0.3495 -1.21 0.23 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.514 on 78 degrees of freedom ## Multiple R-squared: 0.01842, Adjusted R-squared: 0.005834 ## F-statistic: 1.464 on 1 and 78 DF, p-value: 0.23 Confidence Interval confint(M_Birthwt) ## 2.5 % 97.5 % ## (Intercept) 6.613070 7.4653303 ## habitsmoker -1.118735 0.2730012 Question: What should we conclude about the effect of smoking on birth weight based on these results? 5.7.3 Smoking During Pregnancy (cont) In fact, this sample of 80 babies is part of a larger dataset, consisting of 1,000 babies born in NC in 2004. When we consider the full dataset, notice that the difference between the groups is similar, but the p-value is much smaller, providing stronger evidence of a relationship between a mother’s smoking and lower birthweight. M_Birthwt_Full &lt;- lm(data=ncbirths, weight~habit) summary(M_Birthwt_Full) ## ## Call: ## lm(formula = weight ~ habit, data = ncbirths) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.1443 -0.7043 0.1657 0.9157 4.6057 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.14427 0.05086 140.472 &lt;0.0000000000000002 *** ## habitsmoker -0.31554 0.14321 -2.203 0.0278 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.503 on 997 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.004846, Adjusted R-squared: 0.003848 ## F-statistic: 4.855 on 1 and 997 DF, p-value: 0.02779 5.7.4 Exam Scores Exam 1 vs Exam 2 scores for intro stat students at another college. The blue line represents the least squares regression line, while the red line represents the line \\(y=x\\). Notice that out of the seven highest scoring students on Exam 1 (above 90), only two improved on Exam 2. Meanwhile, out of the six students who score lowest (below 70) on Exam 1, five improved on Exam 2. One conclusion would be that the students who did best on Exam 1 didn’t study as hard and weren’t as prepared for Exam 2 as they were for Exam 1, while the students who scored lower worked harder and were better prepared for Exam 2 than they were for Exam 1. While the above explanation is plausible, there is another possible explanation for this behavior. Think about what it might be. 5.7.5 Simulating the Regression Effect We simulate test scores under the assumption that each person’s score on an exam is a combination of their true level of understanding, and a “luck” factor, simulated from a normal distribution. We assume that each student’s true level of understanding is exactly the same on the second exam as on the first. The numbers on the graph indicate the student’s true level of understanding. set.seed(10172023) Understanding &lt;-runif(50,55,95) Score1 &lt;- Understanding + rnorm(50, 0, 5) Score2 &lt;- Understanding + rnorm(50, 0, 5) Understanding &lt;- round(Understanding,0) Score1 &lt;- round(Score1,0) Score2 &lt;- round(Score2,0) TestSim &lt;- data.frame(Understanding, Score1, Score2) ggplot(data=TestSim, aes(y=Score2, x=Score1))+ geom_point() + stat_smooth(method=&quot;lm&quot;) + geom_abline(slope=1, color=&quot;red&quot;) + geom_text(aes(label=Understanding), vjust = 0, nudge_y = 0.5) + xlim(c(50,105)) + ylim(c(50,105)) Highest Exam 1 Scores kable(head(TestSim%&gt;%arrange(desc(Score1)))) Understanding Score1 Score2 93 97 92 90 97 85 88 95 85 93 94 103 95 94 93 79 89 81 Lowest Exam 1 Scores kable(head(TestSim%&gt;%arrange(Score1))) Understanding Score1 Score2 55 49 61 60 54 57 60 56 59 58 57 52 66 60 71 63 61 62 This phenomenon is called the regression effect. 5.7.6 NFL Wins Wins by NFL teams in 2021 and 2022 5.8 Impact of Model Assumption Violations In this chapter, we’ve studied the normal error regression model and its underlying assumptions. We’ve seen that when these assumptions are realistic, we can use distributions derived from probability theory, such as t and F distributions to approximate sampling distributions, in place of the simulation-based methods seen in Chapters 3 and 4. Of course, real data don’t come exactly from processes like the fictional ice cream dispenser described in Section 5.1, so it’s really a question of whether this model is a realistic approximation (or simplification) of the true mechanism that led to the data we observe. We can use diagnostics like residual and Normal-QQ plots, as well as our intuition and background knowledge to assess whether the normal error regression model is a reasonable approximation. The p-values provided by the lm summary output, and anova commands, and the and intervals produced by the confint, and predict command, as well as many other R commands, depend on the assumptions of the normal error regression model, and should only be used when these assumptions are reasonable. In situations where some model assumptions appear to be violated, we might be okay using certain tests/intervals, but not others. In general, we should proceed with caution in these situations. The table below provides guidance on the potential impact of model assumption violation on predicted values, confidence intervals, and prediction intervals. Model assumption Violated Predicted Values Confidence Intervals Prediction Intervals Linearity Unreliable Unreliable Unreliable Constant Variance Reliable Somewhat unreliable - Some too wide, others too narrow Very unreliable - Some too wide, others too narrow Normality Reliable Possibly unreliable - might be symmetric when they shouldn’t be. Might be okay when skewness isn’t bad and sample size is large. Very unreliable - will be symmetric when they shouldn’t be Independence might be reliable unreliable - either too wide or too narrow unreliable - either too wide or too narrow When model assumptions are a concern, consider a using a transformation of the data, a more advanced model, or a more flexible technique, such as a nonparametric approach or statistical machine learning algorithm. "],["building-models-for-interpretation.html", "Chapter 6 Building Models for Interpretation 6.1 Model Building 6.2 Modeling Car Price", " Chapter 6 Building Models for Interpretation Learning Outcomes: Explain the differences in the ways we construct statistical models when we are focused primarily on interpretation. Describe the ways that multicollinearity influences the interpretability of regression models. Recognize situations where confounding and Simpson’s paradox might influence conclusions we draw from a model, and make appropriate interpretations in these situations. Evaluate the appropriateness of models using plots of residuals vs explanatory variables. Recognize when it is appropriate to use polynomials or other nonlinear functions in a statistical model, and interpret corresponding estimates of regression coefficients. Decide which variables to include in a statistical model, and justify your decision. 6.1 Model Building 6.1.1 Modeling for Interpretation So far, we’ve dealt with models with 2 or fewer variables. Some real questions require accounting for more than two variables. In these situations, we’ll need to develop a model that is complex enough to capture the important aspects of the mechanism we’re modeling, but also simple enough for us to be able to explain and interpret. We’ll need to decide how many variables to include in the model, and whether to use transformations, or to include interaction terms. We’ll examine strategies for modeling in two different contexts. In this chapter, we’ll focus on building models for situations when we want to make interpretations and draw conclusions about relationships between variables. In Chapter 7, we focus on modeling solely for the purpose of prediction, when we are not interested in making interpretations or conclusions about relationships between variables. When building a model for the purpose of interpretation, we are typically interested in investigating a research question pertaining to relationships between explanatory and response variables. We’ll need to think about things like: which explanatory variables should we include in the model, and how many? should we include any interaction terms? should we use any nonlinear terms? should we use a transformation of the response variable? We’ll go through a couple example to see how we can address these questions in building a model. Keep in mind, there is no single correct model, but there are common characteristics of a good model. While two statisticians might use different models for a given set of data, they will hopefully lead to reasonably similar conclusions if constructed carefully. 6.1.2 SAT Scores Dataset We’ll now look at a dataset containing education data on all 50 states. It includes the following variables. state - a factor with names of each state expend - expenditure per pupil in average daily attendance in public elementary and secondary schools, 1994-95 (in thousands of US dollars) ratio - average pupil/teacher ratio in public elementary and secondary schools, Fall 1994 salary - estimated average annual salary of teachers in public elementary and secondary schools, 1994-95 (in thousands of US dollars) frac - percentage of all eligible students taking the SAT, 1994-95 sat - average total SAT score, 1994-95 region - region of the country library(mosaicData) data(SAT) SAT &lt;- SAT %&gt;% dplyr::select(-c(verbal, math)) library(Lock5Data) data(&quot;USStates&quot;) SAT &lt;- SAT %&gt;% left_join(USStates %&gt;% select(State, Region), by=c(&quot;state&quot;=&quot;State&quot;)) %&gt;% rename(region = Region) SAT ## state expend ratio salary frac sat region ## 1 Alabama 4.405 17.2 31.144 8 1029 S ## 2 Alaska 8.963 17.6 47.951 47 934 W ## 3 Arizona 4.778 19.3 32.175 27 944 W ## 4 Arkansas 4.459 17.1 28.934 6 1005 S ## 5 California 4.992 24.0 41.078 45 902 W ## 6 Colorado 5.443 18.4 34.571 29 980 W ## 7 Connecticut 8.817 14.4 50.045 81 908 NE ## 8 Delaware 7.030 16.6 39.076 68 897 NE ## 9 Florida 5.718 19.1 32.588 48 889 S ## 10 Georgia 5.193 16.3 32.291 65 854 S ## 11 Hawaii 6.078 17.9 38.518 57 889 W ## 12 Idaho 4.210 19.1 29.783 15 979 W ## 13 Illinois 6.136 17.3 39.431 13 1048 MW ## 14 Indiana 5.826 17.5 36.785 58 882 MW ## 15 Iowa 5.483 15.8 31.511 5 1099 MW ## 16 Kansas 5.817 15.1 34.652 9 1060 MW ## 17 Kentucky 5.217 17.0 32.257 11 999 MW ## 18 Louisiana 4.761 16.8 26.461 9 1021 S ## 19 Maine 6.428 13.8 31.972 68 896 NE ## 20 Maryland 7.245 17.0 40.661 64 909 NE ## 21 Massachusetts 7.287 14.8 40.795 80 907 NE ## 22 Michigan 6.994 20.1 41.895 11 1033 MW ## 23 Minnesota 6.000 17.5 35.948 9 1085 MW ## 24 Mississippi 4.080 17.5 26.818 4 1036 S ## 25 Missouri 5.383 15.5 31.189 9 1045 MW ## 26 Montana 5.692 16.3 28.785 21 1009 W ## 27 Nebraska 5.935 14.5 30.922 9 1050 MW ## 28 Nevada 5.160 18.7 34.836 30 917 W ## 29 New Hampshire 5.859 15.6 34.720 70 935 NE ## 30 New Jersey 9.774 13.8 46.087 70 898 NE ## 31 New Mexico 4.586 17.2 28.493 11 1015 W ## 32 New York 9.623 15.2 47.612 74 892 NE ## 33 North Carolina 5.077 16.2 30.793 60 865 S ## 34 North Dakota 4.775 15.3 26.327 5 1107 MW ## 35 Ohio 6.162 16.6 36.802 23 975 MW ## 36 Oklahoma 4.845 15.5 28.172 9 1027 S ## 37 Oregon 6.436 19.9 38.555 51 947 W ## 38 Pennsylvania 7.109 17.1 44.510 70 880 NE ## 39 Rhode Island 7.469 14.7 40.729 70 888 NE ## 40 South Carolina 4.797 16.4 30.279 58 844 S ## 41 South Dakota 4.775 14.4 25.994 5 1068 MW ## 42 Tennessee 4.388 18.6 32.477 12 1040 S ## 43 Texas 5.222 15.7 31.223 47 893 S ## 44 Utah 3.656 24.3 29.082 4 1076 W ## 45 Vermont 6.750 13.8 35.406 68 901 NE ## 46 Virginia 5.327 14.6 33.987 65 896 S ## 47 Washington 5.906 20.2 36.151 48 937 W ## 48 West Virginia 6.107 14.8 31.944 17 932 S ## 49 Wisconsin 6.930 15.9 37.746 9 1073 MW ## 50 Wyoming 6.160 14.9 31.285 10 1001 W Note that the dataset is quite old (from 1994-95), so the financial information may be out of date. Nevertheless, it is useful for exploring relationships between SAT scores and other variables. 6.1.3 Research Question A good statistical research question should be one that has practical implications that people would care about. It should be complex enough to be worth investigating. If the answer is obvious, then there would be no need to use statistics, or scientific reasoning in general. For the SAT score dataset, we’ll focus on the question: Do students in states that prioritize education spending achieve better SAT scores? While this may seem like a straightforward question, we’ll see that answering it properly requires careful thought and analysis. 6.1.4 Teacher Salary and SAT score One way to measure a state’s investment in education is in how much it pays its teachers. The plot displays average SAT score against average teacher salary for all 50 US states. ggplot(data=SAT, aes(y=sat, x=salary)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) + ggtitle(&quot;Average SAT score vs Average Teacher Salary&quot;) + xlab(&quot;Average Teacher Salary in Thousands&quot;) Fitting a simple linear regression model to the data, we obtain the following: SAT_M1 &lt;- lm(data=SAT, sat~salary) summary(SAT_M1) ## ## Call: ## lm(formula = sat ~ salary, data = SAT) ## ## Residuals: ## Min 1Q Median 3Q Max ## -147.125 -45.354 4.073 42.193 125.279 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1158.859 57.659 20.098 &lt; 0.0000000000000002 *** ## salary -5.540 1.632 -3.394 0.00139 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 67.89 on 48 degrees of freedom ## Multiple R-squared: 0.1935, Adjusted R-squared: 0.1767 ## F-statistic: 11.52 on 1 and 48 DF, p-value: 0.001391 On average, SAT score is expected to decrease by about 5.5 points for each additional one thousand dollars in average teacher salary in the state. The low p-value suggests a relationship like this is unlikely to occur by chance, though the practical importance of a 5-point decrease in SAT score (out of 1600) seems minimal. Furthermore, only 19% of the total variation in SAT score is explained by teaching salary. Nevertheless, a person looking to argue against raising teacher salaries might use the negative estimate and low p-value as a justification for their position. 6.1.5 A Deeper Investigation Notice that there are large discrepancies in the frac variable, representing the percentage of students taking the SAT. In Connecticut, 81% of high school students took the SAT, compared to only 6% in Arkansas. Let’s break the data down by the percentage of students who take the SAT. We’ll (somewhat arbitrarily), divide the states into Low = 0%-22% Medium = 22-49% High = 49-81% SAT &lt;- mutate(SAT, fracgrp = cut(frac, breaks=c(0, 22, 49, 81), labels=c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;))) Plotting SAT score against average teacher salary in each state, we see that the picture changes. ggplot(data=SAT, aes( y=sat, x=salary )) +geom_point() + facet_wrap(facets = ~fracgrp) + stat_smooth(method=&quot;lm&quot;, se=FALSE) + xlab(&quot;Average Teacher Salary in Thousands&quot;) There appears to be a slight positive relationship between teacher salary and SAT score in each state. While breaking up the data into these three groups helps us visualize, we’ll simply add the frac variable to the model as a quantitative variable, rather than breaking it into these arbitrary categories. SAT_M2 &lt;- lm(data=SAT, sat~salary+frac) summary(SAT_M2) ## ## Call: ## lm(formula = sat ~ salary + frac, data = SAT) ## ## Residuals: ## Min 1Q Median 3Q Max ## -78.313 -26.731 3.168 18.951 75.590 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 987.9005 31.8775 30.991 &lt; 0.0000000000000002 *** ## salary 2.1804 1.0291 2.119 0.0394 * ## frac -2.7787 0.2285 -12.163 0.0000000000000004 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 33.69 on 47 degrees of freedom ## Multiple R-squared: 0.8056, Adjusted R-squared: 0.7973 ## F-statistic: 97.36 on 2 and 47 DF, p-value: &lt; 0.00000000000000022 For each one thousand dollar increase in average teacher salary, a state’s average SAT score is expected to increase by 2.18 points, assuming percentage of students taking the test is the same. For each one percent increase in percentage of students taking the SAT, a state’s average score is expected to decrease by 2.78 points, assuming average teacher salary is the same. Both of these estimates are associated with low p-values. While the effect of a 2 point increase per $1,000 in average teacher salary might seem small, the ~3 point decrease for each percentage point of students taking the exam is quite meaningful. According to the model, if the percentage of students taking the SAT is 10 percentage points higher than another, and the states pay their teachers the same, then the state with more people taking the exam is expected to have an average score almost 30 points lower. Adding percentage of students taking the exam increased the \\(R^2\\) value substantially. We see that the relationship between SAT score and salary appears to reverse when we account for percentage of students taking the test. States with low percentages of people taking the SAT tend to get higher scores, as the people taking the test tend to be those who are best prepared and have strong incentive for taking it, perhaps because they are trying to get into an elite college. At the same time, states that pay their teachers more tend to have higher percentages of people taking the SAT. This may be because states that prioritize education are more likely to cover the cost of students taking the test, or even to require it. It may also be that many of the states that require the SAT are coastal states, where cost of living, and thus teacher salaries, tend to be higher in general. Thus, it appears initially that teacher salaries are negatively correlated with SAT scores, but after accounting for percentage taking the test, the trend reverses. Situations where an apparent trend disappears or reverses after accounting for another variable are called Simpson’s Paradox. 6.1.6 Student-to-Teacher Ratio Let’s see what other possible explanatory variables we might want to add to the model. Keep in mind that our goal is to understand the relationship between teacher salary and SAT scores in the state, so we should only use variables that help us understand this relationship. In addition to teacher salaries, student-to-teacher ratio might be an indication of a state’s investment in education. We’ll add student-to-teacher ratio to the model and explore whether there is evidence that hiring enough teachers to keep student-to-teacher ratio low has a benefit, in terms of SAT score. SAT_M3 &lt;- lm(data=SAT, sat~salary+frac+ratio) summary(SAT_M3) ## ## Call: ## lm(formula = sat ~ salary + frac + ratio, data = SAT) ## ## Residuals: ## Min 1Q Median 3Q Max ## -89.244 -21.485 -0.798 17.685 68.262 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1057.8982 44.3287 23.865 &lt;0.0000000000000002 *** ## salary 2.5525 1.0045 2.541 0.0145 * ## frac -2.9134 0.2282 -12.764 &lt;0.0000000000000002 *** ## ratio -4.6394 2.1215 -2.187 0.0339 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 32.41 on 46 degrees of freedom ## Multiple R-squared: 0.8239, Adjusted R-squared: 0.8124 ## F-statistic: 71.72 on 3 and 46 DF, p-value: &lt; 0.00000000000000022 Interpretations On average, a $1,000 dollar increase in average teacher salary is associated with a 2.5 point increase in average SAT score assuming fraction of students taking the SAT, and student to teacher ratio are held constant. On average, a 1% increase in percentage of students taking the SAT is associated with a 2.9 point decrease in average SAT score assuming average teacher salary, and student to teacher ratio are held constant. On average, a 1 student per teacher increase in student to teacher ratio is associated with a 4.6 point from in average SAT score, assuming average teacher salary, and percentage of students taking the SAT are held constant. We see that student to teacher ratio is negatively associated with SAT score, with an expected drop of about 4.6 points in average SAT score for each additional student per teacher, assuming average teacher salary and percentage of students taking the exam are held constant. This suggests that states should try to keep student to teacher ratios low. We see teacher salary remains positively correlated with SAT score and percentage taking the test remains negatively correlated, after accounting for student to teacher ratio. 6.1.7 Multicollinearity Next, let’s add the variable expend, which measures the state’s expenditure per pupil. SAT_M4 &lt;- lm(data=SAT, sat~salary+frac+ratio+expend) summary(SAT_M4) ## ## Call: ## lm(formula = sat ~ salary + frac + ratio + expend, data = SAT) ## ## Residuals: ## Min 1Q Median 3Q Max ## -90.531 -20.855 -1.746 15.979 66.571 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1045.9715 52.8698 19.784 &lt; 0.0000000000000002 *** ## salary 1.6379 2.3872 0.686 0.496 ## frac -2.9045 0.2313 -12.559 0.000000000000000261 *** ## ratio -3.6242 3.2154 -1.127 0.266 ## expend 4.4626 10.5465 0.423 0.674 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 32.7 on 45 degrees of freedom ## Multiple R-squared: 0.8246, Adjusted R-squared: 0.809 ## F-statistic: 52.88 on 4 and 45 DF, p-value: &lt; 0.00000000000000022 It may be surprising to see that after accounting for expenditure per student, teacher salary is still positively correlated with SAT score, but that the p-value associated with teacher salary is quite large. Likewise, while student-to-teacher ratio is still negatively associated, it too has a large p-value. Also notice that \\(R^2\\) barely increased when accounting for total expenditures. This happens because expenditures are highly correlated with teacher salary. States that pay their teacher more also spend more on education per pupil. The scatterplot matrix below shows a strong correlation of 0.87 between teacher salary and expenditures. SAT_Num &lt;- select_if(SAT, is.numeric) C &lt;- cor(SAT_Num, use = &quot;pairwise.complete.obs&quot;) round(C,2) ## expend ratio salary frac sat ## expend 1.00 -0.37 0.87 0.59 -0.38 ## ratio -0.37 1.00 0.00 -0.21 0.08 ## salary 0.87 0.00 1.00 0.62 -0.44 ## frac 0.59 -0.21 0.62 1.00 -0.89 ## sat -0.38 0.08 -0.44 -0.89 1.00 library(corrplot) corrplot(C) Because these variables are highly correlated, it doesn’t make sense to talk about the effect of increasing teacher salary, while holding expenditure constant, or vice-versa. Notice the standard error on the salary line in model SAT_M4 (which includes expenditures) is more than twice as high as in SAT_M3, which did not. This happens because the model is not able to separate the effect of salary from the effect of expenditure, and thus becomes very uncertain of the effect of both, resulting in high standard errors. In addition to reducing the t-statistic, and increasing the p-value, this leads to much wider and less informative confidence intervals associated with the effect of teacher salary. Confidence intervals for model involving teacher salary, percentage taking the test, and student-to-teacher ratio. confint(SAT_M3) ## 2.5 % 97.5 % ## (Intercept) 968.6691802 1147.1271438 ## salary 0.5304797 4.5744605 ## frac -3.3727807 -2.4539197 ## ratio -8.9098147 -0.3690414 Confidence intervals for model with above variables plus expenditure. confint(SAT_M4) ## 2.5 % 97.5 % ## (Intercept) 939.486374 1152.456698 ## salary -3.170247 6.446081 ## frac -3.370262 -2.438699 ## ratio -10.100417 2.851952 ## expend -16.779204 25.704393 Models with highly correlated explanatory variables suffer from multicollinearity, which increases standard errors, making the effect of variables harder to discern. When we have explanatory variables that are highly correlated (usually with correlation greater than 0.8), we should pick out just one to include in the model. In this case, we’ll stick with teacher salary. 6.1.8 Check Model Assumptions Let’s return to the model with salary, ratio, and fraction taking test. We use residual plots to assess model assumptions. P1 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(y=SAT_M3$residuals, x=SAT_M3$fitted.values)) + geom_point() + ggtitle(&quot;Residual Plot&quot;) + xlab(&quot;Predicted Values&quot;) + ylab(&quot;Residuals&quot;) P2 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(x=SAT_M3$residuals)) + geom_histogram() + ggtitle(&quot;Histogram of Residuals&quot;) + xlab(&quot;Residual&quot;) P3 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(sample = scale(SAT_M3$residuals))) + stat_qq() + stat_qq_line() + xlab(&quot;Normal Quantiles&quot;) + ylab(&quot;Residual Quantiles&quot;) + ggtitle(&quot;QQ Plot&quot;) grid.arrange(P1, P2, P3, ncol=3) There is some sign of a quadratic trend in the residual plot, creating concern about the linearity assumption. In models with multiple explanatory variables, it is helpful to also plot our residuals against the explanatory variables to see whether the model is properly accounting for relationships involving each variable. If we see nonlinear trends, we should consider adding a nonlinear function of that explanatory variable. P1 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(y=SAT_M3$residuals, x=SAT_M3$model$salary)) + geom_point() + ggtitle(&quot;Residual by Predictor Plot&quot;) + xlab(&quot;Salary&quot;) + ylab(&quot;Residuals&quot;) P2 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(y=SAT_M3$residuals, x=SAT_M3$model$frac)) + geom_point() + ggtitle(&quot;Residual by Predictor Plot&quot;) + xlab(&quot;Fraction Taking Test&quot;) + ylab(&quot;Residuals&quot;) P3 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(y=SAT_M3$residuals, x=SAT_M3$model$ratio)) + geom_point() + ggtitle(&quot;Residual by Predictor Plot&quot;) + xlab(&quot;Student to Teach Ratio&quot;) + ylab(&quot;Residuals&quot;) grid.arrange(P1, P2, P3, ncol=3) There is also a quadratic trend in the plot involving the fraction variable. We might account for this by adding a quadratic term for frac to the model. 6.1.9 Quadratic Term SAT_M5 &lt;- lm(data=SAT, sat~salary+frac+I(frac^2)+ratio ) summary(SAT_M5) ## ## Call: ## lm(formula = sat ~ salary + frac + I(frac^2) + ratio, data = SAT) ## ## Residuals: ## Min 1Q Median 3Q Max ## -66.09 -15.20 -4.64 15.06 52.77 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1039.21242 36.28206 28.643 &lt; 0.0000000000000002 *** ## salary 1.80708 0.83150 2.173 0.0351 * ## frac -6.64001 0.77668 -8.549 0.0000000000555 *** ## I(frac^2) 0.05065 0.01025 4.942 0.0000111676728 *** ## ratio -0.04058 1.96174 -0.021 0.9836 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 26.38 on 45 degrees of freedom ## Multiple R-squared: 0.8858, Adjusted R-squared: 0.8757 ## F-statistic: 87.28 on 4 and 45 DF, p-value: &lt; 0.00000000000000022 We notice a small p-value associated with the quadratic term, indicating SAT scores do indeed show evidence of a quadratic trend with respect to the percentage of students taking the test. We now examine residual plots for the model that includes the quadratic term for frac. P1 &lt;- ggplot(data=data.frame(SAT_M5$residuals), aes(y=SAT_M5$residuals, x=SAT_M5$fitted.values)) + geom_point() + ggtitle(&quot;Residual Plot&quot;) + xlab(&quot;Predicted Values&quot;) + ylab(&quot;Residuals&quot;) P2 &lt;- ggplot(data=data.frame(SAT_M5$residuals), aes(x=SAT_M5$residuals)) + geom_histogram() + ggtitle(&quot;Histogram of Residuals&quot;) + xlab(&quot;Residual&quot;) P3 &lt;- ggplot(data=data.frame(SAT_M5$residuals), aes(sample = scale(SAT_M5$residuals))) + stat_qq() + stat_qq_line() + xlab(&quot;Normal Quantiles&quot;) + ylab(&quot;Residual Quantiles&quot;) + ggtitle(&quot;QQ Plot&quot;) grid.arrange(P1, P2, P3, ncol=3) P1 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(y=SAT_M5$residuals, x=SAT_M5$model$salary)) + geom_point() + ggtitle(&quot;Residual by Predictor Plot&quot;) + xlab(&quot;Salary&quot;) + ylab(&quot;Residuals&quot;) P2 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(y=SAT_M5$residuals, x=SAT_M5$model$frac)) + geom_point() + ggtitle(&quot;Residual by Predictor Plot&quot;) + xlab(&quot;Fraction Taking Test&quot;) + ylab(&quot;Residuals&quot;) P3 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(y=SAT_M5$residuals, x=SAT_M5$model$ratio)) + geom_point() + ggtitle(&quot;Residual by Predictor Plot&quot;) + xlab(&quot;Student to Teach Ratio&quot;) + ylab(&quot;Residuals&quot;) grid.arrange(P1, P2, P3, ncol=3) The quadratic trend in the residual by predicted plot and second residual by fraction plot appear to have disappeared, suggesting this model has properly accounted for the quadratic trend. Interpretations for Model with Quadratic Term On average, a $1,000 dollar increase in average teacher salary is associated with a 1.8 point increase in average SAT score assuming fraction of students taking the SAT, and student to teacher ratio are held constant. On average, a 1 student per teacher increase in student to teacher ratio is associated with a 0.05 point from in average SAT score, assuming average teacher salary, and percentage of students taking the SAT are held constant. We cannot give a clear interpretation of the fraction variable, since it occurs in both linear and quadratic terms. In fact, the vertex of the parabola given by \\(y=-6.64x + 0.05x^2\\) occurs at \\(x=\\frac{6.64}{2(0.05)}\\approx 66\\). So the model estimates that SAT score decreases in a quadratic fashion with respect to fraction taking the test, until that fraction reaches 66 percent of student, then is expected to increase. ggplot(data=SAT, aes(x=frac, y=sat)) + geom_point() + stat_smooth(se=FALSE) We do see some possible quadratic trend, but we should be really careful about extrapolation. Although the trend does seem to level off in a quadratic way, we wouldn’t expect SAT scores to start to increase if more than 80 percent of students took the exam! 6.1.10 Account for Region? So far, we’ve considered only quantitative explanatory variables. What if we add region of the country to the model. SAT_M6 &lt;- lm(data=SAT, sat~salary+frac+I(frac^2)+ratio + region ) summary(SAT_M6) ## ## Call: ## lm(formula = sat ~ salary + frac + I(frac^2) + ratio + region, ## data = SAT) ## ## Residuals: ## Min 1Q Median 3Q Max ## -45.309 -15.407 -1.996 13.852 41.859 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1085.46629 37.25849 29.133 &lt; 0.0000000000000002 *** ## salary 0.19485 0.86256 0.226 0.822378 ## frac -6.12514 0.84697 -7.232 0.00000000679 *** ## I(frac^2) 0.04762 0.01217 3.914 0.000327 *** ## ratio 0.83366 1.99687 0.417 0.678452 ## regionNE -11.53616 18.18986 -0.634 0.529384 ## regionS -40.07482 11.14606 -3.595 0.000845 *** ## regionW -15.89290 11.77634 -1.350 0.184386 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 23.58 on 42 degrees of freedom ## Multiple R-squared: 0.9148, Adjusted R-squared: 0.9007 ## F-statistic: 64.46 on 7 and 42 DF, p-value: &lt; 0.00000000000000022 We find that on average, SAT scores were lower in the NE, S, and W regions, compared to the baseline region of MW, though only in the S is the difference large enough to yield a small p-value. Notice that the effect of teacher salary and student-to-teacher ratio are no longer statistically significant. This happens because now we are only comparing states in the same region of the country. The p-value associated with teacher salary is now testing the null hypothesis “There is no relationship between average teacher salary and SAT score among states in the same region of the country, with the same percentage of students taking the test, and same student to teacher ratio.” Because we only have 50 states to begin with, breaking down by region results in small sample sizes, which contributes to the large p-values. Furthermore, it is unclear why we would need to account for region here. If our goal is to assess the impact of educational spending on SAT scores, it is probably okay to compare states in different regions of the country. Unless we have some reason for wanting to compare states in the same region, we shouldn’t include region as an explanatory variable (even though including it did raise our \\(R^2\\) value above 0.9). In general, we should only include variables if they help us address our research question. In this case, it’s not clear that accounting for region helps us better understand the relationship between a state’s investment in education, and its students average SAT scores. It is important to note that we should not decide whether to include a variable based on whether or not it yielded a small p-value. Adding or deleting variables from a model until we get a desired p-value on a variable we’re interested in can lead to Confirmation bias (that is choosing our model in a way that intentionally confirms what we expected or hoped to be true), and to detecting spurious correlations that will not be replicable in future studies. This phenomenon, known as p-hacking has led to incorrect and unethical conclusions. We should make modeling decisions about which variables to include in a model before looking at the p-values and then draw conclusions based on the results we see, keeping in mind that p-values are only a part of the picture. 6.1.11 Predictions and Intervals Going back to Model M5, which did not include region, we can make confidence and predictions intervals corresponding to hypothetical states. newstate &lt;- data.frame(salary = 45, frac=0.5, ratio=15) predict(SAT_M5, newdata = newstate, interval=&quot;confidence&quot;, conf.level=0.95) ## fit lwr upr ## 1 1116.615 1085.93 1147.3 We are 95% confident that the average of average SAT scores among all states with average teacher salary of 45 thousand dollars, where 50% of students take the SAT and having student-to-teacher ratio of 15 is between 1085 and 1147. predict(SAT_M5, newdata = newstate, interval=&quot;prediction&quot;, conf.level=0.95) ## fit lwr upr ## 1 1116.615 1055.256 1177.973 We are 95% confident that an individual state with average teacher salary of 45 thousand dollars, where 50% of students take the SAT and having student-to-teacher ratio of 15 will have an average SAT score between 1055 and 1178. 6.2 Modeling Car Price 6.2.1 Model for Price of 2015 Cars We’ll build a model for the price of a new 2015 car, to help us understand what factors are related to the price of a car. #data(Cars2015) #Cars2015 &lt;- Cars2015 %&gt;% rename(Price=LowPrice) Cars2015 &lt;- Cars2015%&gt;% select(-HighPrice) glimpse(Cars2015) ## Rows: 110 ## Columns: 19 ## $ Make &lt;fct&gt; Chevrolet, Hyundai, Kia, Mitsubishi, Nissan, Dodge, Chevrole… ## $ Model &lt;fct&gt; Spark, Accent, Rio, Mirage, Versa Note, Dart, Cruze LS, 500L… ## $ Type &lt;fct&gt; Hatchback, Hatchback, Sedan, Hatchback, Hatchback, Sedan, Se… ## $ Price &lt;dbl&gt; 12.270, 14.745, 13.990, 12.995, 14.180, 16.495, 16.170, 19.3… ## $ Drive &lt;fct&gt; FWD, FWD, FWD, FWD, FWD, FWD, FWD, FWD, FWD, FWD, FWD, AWD, … ## $ CityMPG &lt;int&gt; 30, 28, 28, 37, 31, 23, 24, 24, 28, 30, 27, 27, 25, 27, 30, … ## $ HwyMPG &lt;int&gt; 39, 37, 36, 44, 40, 35, 36, 33, 38, 35, 33, 36, 36, 37, 39, … ## $ FuelCap &lt;dbl&gt; 9.0, 11.4, 11.3, 9.2, 10.9, 14.2, 15.6, 13.1, 12.4, 11.1, 11… ## $ Length &lt;int&gt; 145, 172, 172, 149, 164, 184, 181, 167, 179, 154, 156, 180, … ## $ Width &lt;int&gt; 63, 67, 68, 66, 67, 72, 71, 70, 72, 67, 68, 69, 70, 68, 69, … ## $ Wheelbase &lt;int&gt; 94, 101, 101, 97, 102, 106, 106, 103, 104, 99, 98, 104, 104,… ## $ Height &lt;int&gt; 61, 57, 57, 59, 61, 58, 58, 66, 58, 59, 58, 58, 57, 58, 59, … ## $ UTurn &lt;int&gt; 34, 37, 37, 32, 37, 38, 38, 37, 39, 34, 35, 38, 37, 36, 37, … ## $ Weight &lt;int&gt; 2345, 2550, 2575, 2085, 2470, 3260, 3140, 3330, 2990, 2385, … ## $ Acc030 &lt;dbl&gt; 4.4, 3.7, 3.5, 4.4, 4.0, 3.4, 3.7, 3.9, 3.4, 3.9, 3.9, 3.7, … ## $ Acc060 &lt;dbl&gt; 12.8, 10.3, 9.5, 12.1, 10.9, 9.3, 9.8, 9.5, 9.2, 10.8, 11.1,… ## $ QtrMile &lt;dbl&gt; 19.4, 17.8, 17.3, 19.0, 18.2, 17.2, 17.6, 17.4, 17.1, 18.3, … ## $ PageNum &lt;int&gt; 123, 148, 163, 188, 196, 128, 119, 131, 136, 216, 179, 205, … ## $ Size &lt;fct&gt; Small, Small, Small, Small, Small, Small, Small, Small, Smal… Exploratory Analysis We’ll look at a summary of the categorical variables in the dataset. Cars_Cat &lt;- select_if(Cars2015, is.factor) summary(Cars_Cat) ## Make Model Type Drive Size ## Chevrolet: 8 CTS : 2 7Pass :15 AWD:25 Large :29 ## Ford : 7 2 Touring : 1 Hatchback:11 FWD:63 Midsized:34 ## Hyundai : 7 200 : 1 Sedan :46 RWD:22 Small :47 ## Toyoto : 7 3 i Touring: 1 Sporty :11 ## Audi : 6 3 Series GT: 1 SUV :18 ## Nissan : 6 300 : 1 Wagon : 9 ## (Other) :69 (Other) :103 We examine the correlation matrix of quantitative variables. Cars_Num &lt;- select_if(Cars2015, is.numeric) C &lt;- cor(Cars_Num, use = &quot;pairwise.complete.obs&quot;) round(C,2) ## Price CityMPG HwyMPG FuelCap Length Width Wheelbase Height UTurn ## Price 1.00 -0.65 -0.59 0.57 0.47 0.48 0.46 0.02 0.40 ## CityMPG -0.65 1.00 0.93 -0.77 -0.72 -0.78 -0.69 -0.39 -0.73 ## HwyMPG -0.59 0.93 1.00 -0.75 -0.64 -0.75 -0.64 -0.54 -0.68 ## FuelCap 0.57 -0.77 -0.75 1.00 0.82 0.85 0.79 0.58 0.76 ## Length 0.47 -0.72 -0.64 0.82 1.00 0.81 0.92 0.46 0.84 ## Width 0.48 -0.78 -0.75 0.85 0.81 1.00 0.76 0.62 0.77 ## Wheelbase 0.46 -0.69 -0.64 0.79 0.92 0.76 1.00 0.49 0.81 ## Height 0.02 -0.39 -0.54 0.58 0.46 0.62 0.49 1.00 0.55 ## UTurn 0.40 -0.73 -0.68 0.76 0.84 0.77 0.81 0.55 1.00 ## Weight 0.55 -0.83 -0.84 0.91 0.82 0.91 0.81 0.71 0.80 ## Acc030 -0.76 0.64 0.51 -0.47 -0.38 -0.41 -0.31 0.21 -0.36 ## Acc060 -0.74 0.68 0.52 -0.49 -0.47 -0.46 -0.38 0.21 -0.41 ## QtrMile -0.76 0.65 0.49 -0.45 -0.42 -0.41 -0.35 0.25 -0.37 ## PageNum -0.23 0.28 0.15 -0.15 -0.23 -0.20 -0.24 0.06 -0.22 ## Weight Acc030 Acc060 QtrMile PageNum ## Price 0.55 -0.76 -0.74 -0.76 -0.23 ## CityMPG -0.83 0.64 0.68 0.65 0.28 ## HwyMPG -0.84 0.51 0.52 0.49 0.15 ## FuelCap 0.91 -0.47 -0.49 -0.45 -0.15 ## Length 0.82 -0.38 -0.47 -0.42 -0.23 ## Width 0.91 -0.41 -0.46 -0.41 -0.20 ## Wheelbase 0.81 -0.31 -0.38 -0.35 -0.24 ## Height 0.71 0.21 0.21 0.25 0.06 ## UTurn 0.80 -0.36 -0.41 -0.37 -0.22 ## Weight 1.00 -0.41 -0.43 -0.39 -0.20 ## Acc030 -0.41 1.00 0.95 0.95 0.25 ## Acc060 -0.43 0.95 1.00 0.99 0.26 ## QtrMile -0.39 0.95 0.99 1.00 0.26 ## PageNum -0.20 0.25 0.26 0.26 1.00 library(corrplot) C &lt;- corrplot(C) Note the high correlation between many variables in the dataset. We’ll need to be careful about multicollinearity. 6.2.2 Acc. and Qrt. Mile Time We saw in Section 5.6 that it was better to model log(Price) than price itself, so we’ll continue modeling logprice here. Model Using Just Acceleration Time First, we fit a model using only the time it takes to accelerate from 0 to 60 mph as an explanatory variable. Cars_M1 &lt;- lm(data=Cars2015, log(Price) ~ Acc060) summary(Cars_M1) ## ## Call: ## lm(formula = log(Price) ~ Acc060, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.84587 -0.19396 0.00908 0.18615 0.53350 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.13682 0.13021 39.45 &lt;0.0000000000000002 *** ## Acc060 -0.22064 0.01607 -13.73 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.276 on 108 degrees of freedom ## Multiple R-squared: 0.6359, Adjusted R-squared: 0.6325 ## F-statistic: 188.6 on 1 and 108 DF, p-value: &lt; 0.00000000000000022 Confidence Interval for Effect of Acceleration Time: exp(confint(Cars_M1)) ## 2.5 % 97.5 % ## (Intercept) 131.4610408 220.284693 ## Acc060 0.7768669 0.827959 We are 95% confident that a 1-second increase in acceleration time is associated with an average price decrease between 17% and 22.5%. Model Using Just Quarter Mile Time Now, let’s fit a different model using only the time it takes to drive a quarter mile as an explanatory variable. Cars_M2 &lt;- lm(data=Cars2015, log(Price) ~ QtrMile) summary(Cars_M2) ## ## Call: ## lm(formula = log(Price) ~ QtrMile, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.91465 -0.19501 0.02039 0.17538 0.60073 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.8559 0.3248 24.19 &lt;0.0000000000000002 *** ## QtrMile -0.2776 0.0201 -13.81 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.275 on 108 degrees of freedom ## Multiple R-squared: 0.6385, Adjusted R-squared: 0.6351 ## F-statistic: 190.7 on 1 and 108 DF, p-value: &lt; 0.00000000000000022 Confidence Interval for Effect of Quarter Mile Time: exp(confint(Cars_M2)) ## 2.5 % 97.5 % ## (Intercept) 1355.8297704 4913.077313 ## QtrMile 0.7279941 0.788385 We are 95% confident that a 1-second increase in quarter mile time is associated with a price decrease between 21% and 27%, on average. Model Using Both Acceleration and Quarter Mile Time Cars_M3 &lt;- lm(data=Cars2015, log(Price) ~ QtrMile + Acc060) summary(Cars_M3) ## ## Call: ## lm(formula = log(Price) ~ QtrMile + Acc060, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.89124 -0.20030 0.01001 0.17576 0.57462 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.83974 1.54354 4.431 0.0000227 *** ## QtrMile -0.17316 0.15640 -1.107 0.271 ## Acc060 -0.08389 0.12455 -0.673 0.502 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2757 on 107 degrees of freedom ## Multiple R-squared: 0.64, Adjusted R-squared: 0.6332 ## F-statistic: 95.1 on 2 and 107 DF, p-value: &lt; 0.00000000000000022 Confidence Intervals from 2-variable Model exp(confint(Cars_M3)) ## 2.5 % 97.5 % ## (Intercept) 43.8095999 19922.799158 ## QtrMile 0.6168071 1.146686 ## Acc060 0.7183525 1.177065 It does not make sense to talk about holding QtrMile constant as Acc060 increases, or vice-versa. Trying to do so leads to nonsensical answers. We are 95% confident that a 1-second increase in quarter mile time is associated with an average price change between a 38% decrease and 15% increase, assuming acceleration time is held constant. We are 95% confident that a 1-second increase in acceleration time is associated with an average price change between a 28% decrease and 18% increase, assuming quarter mile time is held constant. Because these variables are so highly correlated, it the model cannot separate the effect of one from the other, and thus is uncertain about both. Notice the very large standard errors associated with both regression coefficients, which lead to very wide confidence intervals. In fact, if two variables are perfectly correlated, it will be impossible to fit them both in a model, and you will get an error message. Impact on Prediction Suppose we want to predict the price of a car that can accelerate from 0 to 60 mph in 9.5 seconds, and completes a quarter mile in 17.3 seconds. exp(predict(Cars_M1, newdata = data.frame(Acc060=9.5, QtrMile=17.3))) ## 1 ## 20.92084 exp(predict(Cars_M2, newdata = data.frame(Acc060=9.5, QtrMile=17.3))) ## 1 ## 21.18223 exp(predict(Cars_M3, newdata = data.frame(Acc060=9.5, QtrMile=17.3))) ## 1 ## 21.05489 The predicted values are similar. Multicollinearity does not hurt predictions, only interpretations. 6.2.3 Adding Weight to Model We could use either quarter mile time or acceleration time as an explanatory variable, but we shouldn’t use both. We’ll proceed with quarter mile time. Cars_M4 &lt;- lm(data=Cars2015, log(Price) ~ QtrMile + Weight) summary(Cars_M4) ## ## Call: ## lm(formula = log(Price) ~ QtrMile + Weight, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.79365 -0.13931 -0.01368 0.15773 0.42234 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.21326823 0.33491778 18.552 &lt; 0.0000000000000002 *** ## QtrMile -0.22482146 0.01748563 -12.858 &lt; 0.0000000000000002 *** ## Weight 0.00020606 0.00002641 7.803 0.0000000000043 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2206 on 107 degrees of freedom ## Multiple R-squared: 0.7696, Adjusted R-squared: 0.7653 ## F-statistic: 178.7 on 2 and 107 DF, p-value: &lt; 0.00000000000000022 \\(R^2\\) went up from 0.64 to 0.76! We might consider adding an interaction term between quarter mile time and weight. This would mean that we think the effect of quarter mile time on price of a car is different for heavier cars than for lighter cars. It’s not clear to me why that would be the case. Cars_M5 &lt;- lm(data=Cars2015, log(Price) ~ QtrMile * Weight) summary(Cars_M5) ## ## Call: ## lm(formula = log(Price) ~ QtrMile * Weight, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.82013 -0.12076 -0.01464 0.14717 0.41928 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.1114189 1.3270870 3.098 0.00249 ** ## QtrMile -0.0963226 0.0804413 -1.197 0.23381 ## Weight 0.0008110 0.0003707 2.188 0.03089 * ## QtrMile:Weight -0.0000373 0.0000228 -1.636 0.10482 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2188 on 106 degrees of freedom ## Multiple R-squared: 0.7752, Adjusted R-squared: 0.7689 ## F-statistic: 121.9 on 3 and 106 DF, p-value: &lt; 0.00000000000000022 p-value on interaction is not that small. \\(R^2\\) didn’t go up much. There doesn’t seem to be much reason to complicate the model by adding an interaction term. 6.2.4 Adding More Variables We’ll consider adding highway MPG to the model. Cars_M6 &lt;- lm(data=Cars2015, log(Price) ~ QtrMile + Weight + HwyMPG) summary(Cars_M6) ## ## Call: ## lm(formula = log(Price) ~ QtrMile + Weight + HwyMPG, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.82308 -0.14513 -0.01922 0.16732 0.41390 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.54954436 0.42196132 15.522 &lt; 0.0000000000000002 *** ## QtrMile -0.21699008 0.01843615 -11.770 &lt; 0.0000000000000002 *** ## Weight 0.00015922 0.00004456 3.573 0.000532 *** ## HwyMPG -0.00961141 0.00737658 -1.303 0.195410 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2198 on 106 degrees of freedom ## Multiple R-squared: 0.7732, Adjusted R-squared: 0.7668 ## F-statistic: 120.5 on 3 and 106 DF, p-value: &lt; 0.00000000000000022 HwyMPG doesn’t make change \\(R^2\\) much, and has a high correlation with weight. Let’s not include it. Next, we’ll consider adding categorical explanatory variables Size, and Drive. P1 &lt;- ggplot(data=Cars2015, aes(x=log(Price), y=Size)) + geom_boxplot() + ggtitle(&quot;Price by Size&quot;) P2 &lt;- ggplot(data=Cars2015, aes(x=log(Price), y=Drive)) + geom_boxplot() + ggtitle(&quot;Price by Drive&quot;) grid.arrange(P1, P2, ncol=2) Information about size is already included, through the weight variable. Let’s add drive type to the model. Cars_M7 &lt;- lm(data=Cars2015, log(Price) ~ QtrMile + Weight + Drive) summary(Cars_M7) ## ## Call: ## lm(formula = log(Price) ~ QtrMile + Weight + Drive, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.72386 -0.10882 0.01269 0.13306 0.45304 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.81406789 0.33961789 17.119 &lt; 0.0000000000000002 *** ## QtrMile -0.19007439 0.01959554 -9.700 0.000000000000000289 *** ## Weight 0.00020496 0.00002583 7.936 0.000000000002420675 *** ## DriveFWD -0.22403222 0.05704513 -3.927 0.000154 *** ## DriveRWD -0.13884399 0.06227709 -2.229 0.027913 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2077 on 105 degrees of freedom ## Multiple R-squared: 0.7995, Adjusted R-squared: 0.7919 ## F-statistic: 104.7 on 4 and 105 DF, p-value: &lt; 0.00000000000000022 We found evidence of differences in price between front-wheel drive and rear-wheel drive, compared to all wheel drive cars. Next, we’ll explore adding size to the model. Cars_M8 &lt;- lm(data=Cars2015, log(Price) ~ QtrMile + Weight + Drive + Size) summary(Cars_M8) ## ## Call: ## lm(formula = log(Price) ~ QtrMile + Weight + Drive + Size, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.71092 -0.12126 0.01355 0.11831 0.44439 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.66594310 0.37169625 15.243 &lt; 0.0000000000000002 *** ## QtrMile -0.19256547 0.02000711 -9.625 0.000000000000000505 *** ## Weight 0.00023978 0.00004101 5.847 0.000000059416930182 *** ## DriveFWD -0.21598794 0.05780323 -3.737 0.000306 *** ## DriveRWD -0.15259183 0.06410851 -2.380 0.019142 * ## SizeMidsized 0.04699095 0.06271499 0.749 0.455398 ## SizeSmall 0.08875861 0.08105810 1.095 0.276071 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2085 on 103 degrees of freedom ## Multiple R-squared: 0.8018, Adjusted R-squared: 0.7903 ## F-statistic: 69.46 on 6 and 103 DF, p-value: &lt; 0.00000000000000022 Adding size barely increased \\(R^2\\) at all. We find no evidence of differences in price between the three sizes, after accounting for the other variables. Note: Information about car size is already being taken into account through the Weight variable. We could keep looking at other variables to add, but at this point, we have a model that gives us a good sense of the factors related to price of a car, capturing 80% of total variability in car price, and is still easy to interpret. For our research purposes, this model is good enough. 6.2.5 Check of Model Assumptions We’ll use residuals to check the model assumptions. Residual by Predicted Plot, Histogram of Residuals, and Normal Quantile-Quantile Plot P1 &lt;- ggplot(data=data.frame(Cars_M7$residuals), aes(y=Cars_M7$residuals, x=Cars_M7$fitted.values)) + geom_point() + ggtitle(&quot;Residual Plot&quot;) + xlab(&quot;Predicted Values&quot;) + ylab(&quot;Residuals&quot;) P2 &lt;- ggplot(data=data.frame(Cars_M7$residuals), aes(x=Cars_M7$residuals)) + geom_histogram() + ggtitle(&quot;Histogram of Residuals&quot;) + xlab(&quot;Residual&quot;) P3 &lt;- ggplot(data=data.frame(Cars_M7$residuals), aes(sample = scale(Cars_M7$residuals))) + stat_qq() + stat_qq_line() + xlab(&quot;Normal Quantiles&quot;) + ylab(&quot;Residual Quantiles&quot;) + ggtitle(&quot;QQ Plot&quot;) grid.arrange(P1, P2, P3, ncol=3) There is slight concern about constant variance, but otherwise, the model assumptions look good. Residual by Predictor Plots P1 &lt;- ggplot(data=data.frame(Cars_M7$residuals), aes(y=Cars_M7$residuals, x=Cars_M7$model$QtrMile)) + geom_point() + ggtitle(&quot;Residual by Predictor Plot&quot;) + xlab(&quot;QtrMile&quot;) + ylab(&quot;Residuals&quot;) P2 &lt;- ggplot(data=data.frame(Cars_M7$residuals), aes(y=Cars_M7$residuals, x=Cars_M7$model$Weight)) + geom_point() + ggtitle(&quot;Residual by Predictor Plot&quot;) + xlab(&quot;Weight&quot;) + ylab(&quot;Residuals&quot;) P3 &lt;- ggplot(data=data.frame(Cars_M7$residuals), aes(y=Cars_M7$residuals, x=Cars_M7$model$Drive)) + geom_point() + ggtitle(&quot;Residual by Predictor Plot&quot;) + xlab(&quot;Drive&quot;) + ylab(&quot;Residuals&quot;) grid.arrange(P1, P2, P3, ncol=3) These plots don’t raise any concerns. 6.2.6 Coefficients and Exponentiation The model coefficients are shown below. Cars_M7$coefficients ## (Intercept) QtrMile Weight DriveFWD DriveRWD ## 5.8140678915 -0.1900743859 0.0002049586 -0.2240322171 -0.1388439916 Since we used a log transformation, we should interpret \\(e^{b_j}\\) rather than \\(b_j\\) itself. exp(Cars_M7$coefficients) ## (Intercept) QtrMile Weight DriveFWD DriveRWD ## 334.9790161 0.8268976 1.0002050 0.7992894 0.8703638 The price of a car is expected to decrease by 17% for each additional second it takes to drive a quartermile, assuming weight, and drive type are held constant. The price of a car is expected to increase by 0.02% for each additional pound, assuming quarter mile time, and drive type are held constant. Thus, a 100 lb increase is associated with an expected 2% increase in price, assuming quarter mile time, and drive type are held constant. FWD cars are expected to cost 20% less than AWD cars, assuming quarter mile time and weight are held constant. RWD cars are expected to cost 13% less than AWD cars, assuming quarter mile time and weight are held constant. 6.2.7 Confidence and Prediction Intevals We’ll use our model to estimate the average price with the following characteristics, and also to predict the price of a new car with the given characteristics. newcar &lt;- data.frame(QtrMile = 18, Weight=2400, Drive = &quot;AWD&quot;) This is an interval for log(Price). predict(Cars_M7, newdata=newcar, interval=&quot;confidence&quot;, level=0.95) ## fit lwr upr ## 1 2.884629 2.741423 3.027836 Exponentiating, we obtain exp(predict(Cars_M7, newdata=newcar, interval=&quot;confidence&quot;, level=0.95)) ## fit lwr upr ## 1 17.89693 15.50904 20.65249 We are 95% Confident that the average price of all new 2015 cars that weigh 2400 lbs, drive a quarter mile in 18 seconds on a fast track, and have all wheel drive is between 15.5 thousand and 20.7 thousand dollars. Next, we calculate a prediction interval for an individual car with these characteristics. exp(predict(Cars_M7, newdata=newcar, interval=&quot;prediction&quot;, level=0.95)) ## fit lwr upr ## 1 17.89693 11.57309 27.6763 We are 95% Confident that the price of an individual new 2015 car that weighs 2400 lbs, drives a quarter mile in 18 seconds on a fast track, and has all wheel drive will be between 11.6 thousand and 27.7 thousand dollars. 6.2.8 Model Building Summary Consider the following when building a model for the purpose of interpreting parameters and understanding and drawing conclusions about a population or process. Model driven by research question Include variables of interest Include potential confounders (like in SAT example) Avoid including highly correlated explanatory variables Avoid messy transformations and interactions where possible Use residual plots to assess appropriateness of model assumptions Aim for high \\(R^2\\) but not highest Aim for model complex enough to capture nature of data, but simple enough to give clear interpretations "],["predictive-modeling.html", "Chapter 7 Predictive Modeling", " Chapter 7 Predictive Modeling "],["classification-and-logistic-regression.html", "Chapter 8 Classification and Logistic Regression", " Chapter 8 Classification and Logistic Regression "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
