<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Predictive Modeling | Stat 255: Statistics for Data Science Notes</title>
  <meta name="description" content="Chapter 7 Predictive Modeling | Stat 255: Statistics for Data Science Notes" />
  <meta name="generator" content="bookdown 0.31 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Predictive Modeling | Stat 255: Statistics for Data Science Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Predictive Modeling | Stat 255: Statistics for Data Science Notes" />
  
  
  

<meta name="author" content="Andrew Sage - Lawrence University" />


<meta name="date" content="2023-11-05" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="building-models-for-interpretation.html"/>
<link rel="next" href="logistic-regression-and-classification.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">STAT 255 Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html"><i class="fa fa-check"></i><b>1</b> Exploratory Data Analysis</a>
<ul>
<li class="chapter" data-level="1.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#getting-started-in-r"><i class="fa fa-check"></i><b>1.1</b> Getting Started in R</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#previewing-the-data"><i class="fa fa-check"></i><b>1.1.1</b> Previewing the Data</a></li>
<li class="chapter" data-level="1.1.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#modifying-the-data"><i class="fa fa-check"></i><b>1.1.2</b> Modifying the Data</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#data-visualization"><i class="fa fa-check"></i><b>1.2</b> Data Visualization</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#histogram"><i class="fa fa-check"></i><b>1.2.1</b> Histogram</a></li>
<li class="chapter" data-level="1.2.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#density-plot"><i class="fa fa-check"></i><b>1.2.2</b> Density Plot</a></li>
<li class="chapter" data-level="1.2.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boxplot"><i class="fa fa-check"></i><b>1.2.3</b> Boxplot</a></li>
<li class="chapter" data-level="1.2.4" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#violin-plot"><i class="fa fa-check"></i><b>1.2.4</b> Violin Plot</a></li>
<li class="chapter" data-level="1.2.5" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#scatterplot"><i class="fa fa-check"></i><b>1.2.5</b> Scatterplot</a></li>
<li class="chapter" data-level="1.2.6" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#bar-graph"><i class="fa fa-check"></i><b>1.2.6</b> Bar Graph</a></li>
<li class="chapter" data-level="1.2.7" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#stacked-and-side-by-side-bar-graphs"><i class="fa fa-check"></i><b>1.2.7</b> Stacked and Side-by-Side Bar Graphs</a></li>
<li class="chapter" data-level="1.2.8" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#correlation-plot"><i class="fa fa-check"></i><b>1.2.8</b> Correlation Plot</a></li>
<li class="chapter" data-level="1.2.9" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#scatterplot-matrix"><i class="fa fa-check"></i><b>1.2.9</b> Scatterplot Matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#summary-tables"><i class="fa fa-check"></i><b>1.3</b> Summary Tables</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#calculating-summary-statistics"><i class="fa fa-check"></i><b>1.3.1</b> Calculating Summary Statistics</a></li>
<li class="chapter" data-level="1.3.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#grouped-summaries"><i class="fa fa-check"></i><b>1.3.2</b> Grouped Summaries</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html"><i class="fa fa-check"></i><b>2</b> Introduction to Statistical Models</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#fitting-models-to-data"><i class="fa fa-check"></i><b>2.1</b> Fitting Models to Data</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#terminology"><i class="fa fa-check"></i><b>2.1.1</b> Terminology</a></li>
<li class="chapter" data-level="2.1.2" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#model-with-quantitative-explanatory-variable"><i class="fa fa-check"></i><b>2.1.2</b> Model with Quantitative Explanatory Variable</a></li>
<li class="chapter" data-level="2.1.3" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#model-with-categorical-variable"><i class="fa fa-check"></i><b>2.1.3</b> Model with Categorical Variable</a></li>
<li class="chapter" data-level="2.1.4" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#model-with-multiple-explanatory-variables"><i class="fa fa-check"></i><b>2.1.4</b> Model with Multiple Explanatory Variables</a></li>
<li class="chapter" data-level="2.1.5" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#model-with-no-explanatory-variable"><i class="fa fa-check"></i><b>2.1.5</b> Model with No Explanatory Variable</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#variability-explained-by-a-model"><i class="fa fa-check"></i><b>2.2</b> Variability Explained by a Model</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#quantifying-variability"><i class="fa fa-check"></i><b>2.2.1</b> Quantifying Variability</a></li>
<li class="chapter" data-level="2.2.2" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#total-variability"><i class="fa fa-check"></i><b>2.2.2</b> Total Variability</a></li>
<li class="chapter" data-level="2.2.3" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#residuals"><i class="fa fa-check"></i><b>2.2.3</b> Residuals</a></li>
<li class="chapter" data-level="2.2.4" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#variability-explained-by-sq.-ft.-model"><i class="fa fa-check"></i><b>2.2.4</b> Variability Explained by Sq. Ft. Model</a></li>
<li class="chapter" data-level="2.2.5" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#linear-correlation-coefficient"><i class="fa fa-check"></i><b>2.2.5</b> Linear Correlation Coefficient</a></li>
<li class="chapter" data-level="2.2.6" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#variability-explained-by-waterfront-model"><i class="fa fa-check"></i><b>2.2.6</b> Variability Explained by Waterfront Model</a></li>
<li class="chapter" data-level="2.2.7" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#variability-explained-by-multiple-regression-model"><i class="fa fa-check"></i><b>2.2.7</b> Variability Explained by Multiple Regression Model</a></li>
<li class="chapter" data-level="2.2.8" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#summary-sst-ssr-ssm-r2"><i class="fa fa-check"></i><b>2.2.8</b> Summary: SST, SSR, SSM, <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="2.2.9" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#r2-visually"><i class="fa fa-check"></i><b>2.2.9</b> <span class="math inline">\(R^2\)</span> Visually</a></li>
<li class="chapter" data-level="2.2.10" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#model-comparison-summary"><i class="fa fa-check"></i><b>2.2.10</b> Model Comparison Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#models-with-interaction"><i class="fa fa-check"></i><b>2.3</b> Models with Interaction</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#definition-of-interaction"><i class="fa fa-check"></i><b>2.3.1</b> Definition of Interaction</a></li>
<li class="chapter" data-level="2.3.2" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#interaction-term"><i class="fa fa-check"></i><b>2.3.2</b> Interaction Term</a></li>
<li class="chapter" data-level="2.3.3" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#interaction-models-in-r"><i class="fa fa-check"></i><b>2.3.3</b> Interaction Models in R</a></li>
<li class="chapter" data-level="2.3.4" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#r2-for-interaction-model"><i class="fa fa-check"></i><b>2.3.4</b> <span class="math inline">\(R^2\)</span> for Interaction Model</a></li>
<li class="chapter" data-level="2.3.5" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#considerations-for-using-interactions"><i class="fa fa-check"></i><b>2.3.5</b> Considerations for Using Interactions</a></li>
<li class="chapter" data-level="2.3.6" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#interaction-vs-correlation"><i class="fa fa-check"></i><b>2.3.6</b> Interaction vs Correlation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#least-squares-estimation-lse"><i class="fa fa-check"></i><b>2.4</b> Least Squares Estimation (LSE)</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#estimating-regression-coefficients"><i class="fa fa-check"></i><b>2.4.1</b> Estimating Regression Coefficients</a></li>
<li class="chapter" data-level="2.4.2" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#mathematics-of-lse-for-slr"><i class="fa fa-check"></i><b>2.4.2</b> Mathematics of LSE for SLR</a></li>
<li class="chapter" data-level="2.4.3" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#lse-for-categorical-variable"><i class="fa fa-check"></i><b>2.4.3</b> LSE for Categorical Variable</a></li>
<li class="chapter" data-level="2.4.4" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#lse-more-generally"><i class="fa fa-check"></i><b>2.4.4</b> LSE More Generally</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#analysis-of-variance"><i class="fa fa-check"></i><b>2.5</b> ANalysis Of VAriance</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#submodels"><i class="fa fa-check"></i><b>2.5.1</b> Submodels</a></li>
<li class="chapter" data-level="2.5.2" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#f-statistics"><i class="fa fa-check"></i><b>2.5.2</b> F-Statistics</a></li>
<li class="chapter" data-level="2.5.3" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#comparing-3-or-more-categories"><i class="fa fa-check"></i><b>2.5.3</b> Comparing 3 or More Categories</a></li>
<li class="chapter" data-level="2.5.4" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#f-statistic-illustration"><i class="fa fa-check"></i><b>2.5.4</b> F-Statistic Illustration</a></li>
<li class="chapter" data-level="2.5.5" data-path="introduction-to-statistical-models.html"><a href="introduction-to-statistical-models.html#alternative-f-statistic-formula"><i class="fa fa-check"></i><b>2.5.5</b> Alternative F-Statistic Formula</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="hypothesis-testing-via-permutation.html"><a href="hypothesis-testing-via-permutation.html"><i class="fa fa-check"></i><b>3</b> Hypothesis Testing via Permutation</a>
<ul>
<li class="chapter" data-level="3.1" data-path="hypothesis-testing-via-permutation.html"><a href="hypothesis-testing-via-permutation.html#test-for-difference-in-means"><i class="fa fa-check"></i><b>3.1</b> Test for Difference in Means</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="hypothesis-testing-via-permutation.html"><a href="hypothesis-testing-via-permutation.html#mercury-levels-in-florida-lakes"><i class="fa fa-check"></i><b>3.1.1</b> Mercury Levels in Florida Lakes</a></li>
<li class="chapter" data-level="3.1.2" data-path="hypothesis-testing-via-permutation.html"><a href="hypothesis-testing-via-permutation.html#model-for-mercury-level"><i class="fa fa-check"></i><b>3.1.2</b> Model for Mercury Level</a></li>
<li class="chapter" data-level="3.1.3" data-path="hypothesis-testing-via-permutation.html"><a href="hypothesis-testing-via-permutation.html#hypotheses-and-key-question"><i class="fa fa-check"></i><b>3.1.3</b> Hypotheses and Key Question</a></li>
<li class="chapter" data-level="3.1.4" data-path="hypothesis-testing-via-permutation.html"><a href="hypothesis-testing-via-permutation.html#permutation-test-for-difference-in-means"><i class="fa fa-check"></i><b>3.1.4</b> Permutation Test for Difference in Means</a></li>
<li class="chapter" data-level="3.1.5" data-path="hypothesis-testing-via-permutation.html"><a href="hypothesis-testing-via-permutation.html#five-permutations-in-r"><i class="fa fa-check"></i><b>3.1.5</b> Five Permutations in R</a></li>
<li class="chapter" data-level="3.1.6" data-path="hypothesis-testing-via-permutation.html"><a href="hypothesis-testing-via-permutation.html#r-code-for-permutation-test"><i class="fa fa-check"></i><b>3.1.6</b> R Code for Permutation Test</a></li>
<li class="chapter" data-level="3.1.7" data-path="hypothesis-testing-via-permutation.html"><a href="hypothesis-testing-via-permutation.html#p-values"><i class="fa fa-check"></i><b>3.1.7</b> p-values</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="hypothesis-testing-via-permutation.html"><a href="hypothesis-testing-via-permutation.html#general-permutation-tests"><i class="fa fa-check"></i><b>3.2</b> General Permutation Tests</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="hypothesis-testing-via-permutation.html"><a href="hypothesis-testing-via-permutation.html#other-test-statistics"><i class="fa fa-check"></i><b>3.2.1</b> Other Test Statistics</a></li>
<li class="chapter" data-level="3.2.2" data-path="hypothesis-testing-via-permutation.html"><a href="hypothesis-testing-via-permutation.html#general-permutation-test-procedure"><i class="fa fa-check"></i><b>3.2.2</b> General Permutation Test Procedure</a></li>
<li class="chapter" data-level="3.2.3" data-path="hypothesis-testing-via-permutation.html"><a href="hypothesis-testing-via-permutation.html#difference-in-standard-deviation"><i class="fa fa-check"></i><b>3.2.3</b> Difference in Standard Deviation</a></li>
<li class="chapter" data-level="3.2.4" data-path="hypothesis-testing-via-permutation.html"><a href="hypothesis-testing-via-permutation.html#permutation-test-for-slope"><i class="fa fa-check"></i><b>3.2.4</b> Permutation Test for Slope</a></li>
<li class="chapter" data-level="3.2.5" data-path="hypothesis-testing-via-permutation.html"><a href="hypothesis-testing-via-permutation.html#f-statistic"><i class="fa fa-check"></i><b>3.2.5</b> F-Statistic</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="hypothesis-testing-via-permutation.html"><a href="hypothesis-testing-via-permutation.html#responsible-hypothesis-testing"><i class="fa fa-check"></i><b>3.3</b> Responsible Hypothesis Testing</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html"><i class="fa fa-check"></i><b>4</b> Bootstrap Interval Estimation</a>
<ul>
<li class="chapter" data-level="4.1" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#sampling-distributions"><i class="fa fa-check"></i><b>4.1</b> Sampling Distributions</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#sampling-from-a-population"><i class="fa fa-check"></i><b>4.1.1</b> Sampling From a Population</a></li>
<li class="chapter" data-level="4.1.2" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1.2</b> Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#bootstrapping"><i class="fa fa-check"></i><b>4.2</b> Bootstrapping</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#mercury-levels-in-florida-lakes-1"><i class="fa fa-check"></i><b>4.2.1</b> Mercury Levels in Florida Lakes</a></li>
<li class="chapter" data-level="4.2.2" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#bootstrap-sampling"><i class="fa fa-check"></i><b>4.2.2</b> Bootstrap Sampling</a></li>
<li class="chapter" data-level="4.2.3" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#bootstrap-samples-of-lakes"><i class="fa fa-check"></i><b>4.2.3</b> Bootstrap Samples of Lakes</a></li>
<li class="chapter" data-level="4.2.4" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#bootstrap-distribution"><i class="fa fa-check"></i><b>4.2.4</b> Bootstrap Distribution</a></li>
<li class="chapter" data-level="4.2.5" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#bootstrap-se-confidence-interval"><i class="fa fa-check"></i><b>4.2.5</b> Bootstrap SE Confidence Interval</a></li>
<li class="chapter" data-level="4.2.6" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#bootstrap-distribution-vs-sampling-distribution"><i class="fa fa-check"></i><b>4.2.6</b> Bootstrap Distribution vs Sampling Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#bootstrap-confidence-interval-example"><i class="fa fa-check"></i><b>4.3</b> Bootstrap Confidence Interval Example</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#bootstrapping-other-statistics"><i class="fa fa-check"></i><b>4.3.1</b> Bootstrapping Other Statistics</a></li>
<li class="chapter" data-level="4.3.2" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#ci-for-mean"><i class="fa fa-check"></i><b>4.3.2</b> CI for Mean</a></li>
<li class="chapter" data-level="4.3.3" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#ci-for-standard-deviation"><i class="fa fa-check"></i><b>4.3.3</b> CI for Standard Deviation</a></li>
<li class="chapter" data-level="4.3.4" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#ci-for-median"><i class="fa fa-check"></i><b>4.3.4</b> CI for Median</a></li>
<li class="chapter" data-level="4.3.5" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#ci-for-difference-in-means"><i class="fa fa-check"></i><b>4.3.5</b> CI for Difference in Means</a></li>
<li class="chapter" data-level="4.3.6" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#ci-for-regression-slope"><i class="fa fa-check"></i><b>4.3.6</b> CI for Regression Slope</a></li>
<li class="chapter" data-level="4.3.7" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#ci-for-regression-response"><i class="fa fa-check"></i><b>4.3.7</b> CI for Regression Response</a></li>
<li class="chapter" data-level="4.3.8" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#more-cis-in-regression"><i class="fa fa-check"></i><b>4.3.8</b> More CIâ€™s in Regression</a></li>
<li class="chapter" data-level="4.3.9" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#bootstrapping-cautions"><i class="fa fa-check"></i><b>4.3.9</b> Bootstrapping Cautions</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#estimating-standard-error"><i class="fa fa-check"></i><b>4.4</b> Estimating Standard Error</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#standard-error-vs-standard-deviation"><i class="fa fa-check"></i><b>4.4.1</b> Standard Error vs Standard Deviation</a></li>
<li class="chapter" data-level="4.4.2" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#sample-size-and-standard-error"><i class="fa fa-check"></i><b>4.4.2</b> Sample Size and Standard Error</a></li>
<li class="chapter" data-level="4.4.3" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#standard-error-formulas"><i class="fa fa-check"></i><b>4.4.3</b> Standard Error Formulas</a></li>
<li class="chapter" data-level="4.4.4" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#one-sample-mean-example"><i class="fa fa-check"></i><b>4.4.4</b> One-Sample Mean Example</a></li>
<li class="chapter" data-level="4.4.5" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#difference-in-means-example"><i class="fa fa-check"></i><b>4.4.5</b> Difference in Means Example</a></li>
<li class="chapter" data-level="4.4.6" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#regression-example"><i class="fa fa-check"></i><b>4.4.6</b> Regression Example</a></li>
<li class="chapter" data-level="4.4.7" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#theory-based-confidence-intervals"><i class="fa fa-check"></i><b>4.4.7</b> Theory-Based Confidence Intervals</a></li>
<li class="chapter" data-level="4.4.8" data-path="bootstrap-interval-estimation.html"><a href="bootstrap-interval-estimation.html#ci-method-comparison"><i class="fa fa-check"></i><b>4.4.8</b> CI Method Comparison</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html"><i class="fa fa-check"></i><b>5</b> Normal Error Regression Model</a>
<ul>
<li class="chapter" data-level="5.1" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#the-normal-error-regression-model"><i class="fa fa-check"></i><b>5.1</b> The Normal Error Regression Model</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#example-ice-cream-dispenser"><i class="fa fa-check"></i><b>5.1.1</b> Example: Ice Cream dispenser</a></li>
<li class="chapter" data-level="5.1.2" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#signal-and-noise"><i class="fa fa-check"></i><b>5.1.2</b> Signal and Noise</a></li>
<li class="chapter" data-level="5.1.3" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#normal-distribution"><i class="fa fa-check"></i><b>5.1.3</b> Normal Distribution</a></li>
<li class="chapter" data-level="5.1.4" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#signal-and-noise-in-icecream-example"><i class="fa fa-check"></i><b>5.1.4</b> Signal and Noise in Icecream Example</a></li>
<li class="chapter" data-level="5.1.5" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#normal-error-regression-model-1"><i class="fa fa-check"></i><b>5.1.5</b> Normal Error Regression Model</a></li>
<li class="chapter" data-level="5.1.6" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#examples-of-normal-error-regression-model"><i class="fa fa-check"></i><b>5.1.6</b> Examples of Normal Error Regression Model</a></li>
<li class="chapter" data-level="5.1.7" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#implications-of-normal-error-regression-model"><i class="fa fa-check"></i><b>5.1.7</b> Implications of Normal Error Regression Model</a></li>
<li class="chapter" data-level="5.1.8" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#philosophical-question"><i class="fa fa-check"></i><b>5.1.8</b> Philosophical Question</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#inference-in-normal-error-regression-model"><i class="fa fa-check"></i><b>5.2</b> Inference in Normal Error Regression Model</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#lm-summary-output"><i class="fa fa-check"></i><b>5.2.1</b> <code>lm</code> <code>summary</code> Output</a></li>
<li class="chapter" data-level="5.2.2" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#t-distribution"><i class="fa fa-check"></i><b>5.2.2</b> t-distribution</a></li>
<li class="chapter" data-level="5.2.3" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#difference-in-means-example-1"><i class="fa fa-check"></i><b>5.2.3</b> Difference in Means Example</a></li>
<li class="chapter" data-level="5.2.4" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#simple-linear-regression-example"><i class="fa fa-check"></i><b>5.2.4</b> Simple Linear Regression Example</a></li>
<li class="chapter" data-level="5.2.5" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#multiple-regression-example"><i class="fa fa-check"></i><b>5.2.5</b> Multiple Regression Example</a></li>
<li class="chapter" data-level="5.2.6" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#mr-with-interaction-example"><i class="fa fa-check"></i><b>5.2.6</b> MR with Interaction Example</a></li>
<li class="chapter" data-level="5.2.7" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#limitations"><i class="fa fa-check"></i><b>5.2.7</b> Limitations</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#f-distributions"><i class="fa fa-check"></i><b>5.3</b> F-Distributions</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#f-distribution"><i class="fa fa-check"></i><b>5.3.1</b> F-Distribution</a></li>
<li class="chapter" data-level="5.3.2" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#house-condition-example"><i class="fa fa-check"></i><b>5.3.2</b> House Condition Example</a></li>
<li class="chapter" data-level="5.3.3" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#interaction-example"><i class="fa fa-check"></i><b>5.3.3</b> Interaction Example</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#regression-model-assumptions"><i class="fa fa-check"></i><b>5.4</b> Regression Model Assumptions</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#regression-assumptions"><i class="fa fa-check"></i><b>5.4.1</b> Regression Assumptions</a></li>
<li class="chapter" data-level="5.4.2" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#checking-model-assumptions"><i class="fa fa-check"></i><b>5.4.2</b> Checking Model Assumptions</a></li>
<li class="chapter" data-level="5.4.3" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#summary-of-checks-for-model-assumptions"><i class="fa fa-check"></i><b>5.4.3</b> Summary of Checks for Model Assumptions</a></li>
<li class="chapter" data-level="5.4.4" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#example-n-v-s-lakes"><i class="fa fa-check"></i><b>5.4.4</b> Example: N v S Lakes</a></li>
<li class="chapter" data-level="5.4.5" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#example-ph-model"><i class="fa fa-check"></i><b>5.4.5</b> Example: pH Model</a></li>
<li class="chapter" data-level="5.4.6" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#example-house-prices"><i class="fa fa-check"></i><b>5.4.6</b> Example: House Prices</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#intervals-for-expected-response"><i class="fa fa-check"></i><b>5.5</b> Intervals for Expected Response</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#parameter-values-and-expected-responses"><i class="fa fa-check"></i><b>5.5.1</b> Parameter Values and Expected Responses</a></li>
<li class="chapter" data-level="5.5.2" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#estimation-and-prediction"><i class="fa fa-check"></i><b>5.5.2</b> Estimation and Prediction</a></li>
<li class="chapter" data-level="5.5.3" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#estimation-and-prediction-in-slr"><i class="fa fa-check"></i><b>5.5.3</b> Estimation and Prediction in SLR</a></li>
<li class="chapter" data-level="5.5.4" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#intervals-in-r"><i class="fa fa-check"></i><b>5.5.4</b> Intervals in R</a></li>
<li class="chapter" data-level="5.5.5" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#slr-calculations-optional"><i class="fa fa-check"></i><b>5.5.5</b> SLR Calculations (Optional)</a></li>
<li class="chapter" data-level="5.5.6" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#car-price-and-acceleration-time"><i class="fa fa-check"></i><b>5.5.6</b> Car Price and Acceleration Time</a></li>
<li class="chapter" data-level="5.5.7" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#florida-lakes-est.-and-pred."><i class="fa fa-check"></i><b>5.5.7</b> Florida Lakes Est. and Pred.</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#transformations"><i class="fa fa-check"></i><b>5.6</b> Transformations</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#cars-assumptions-check"><i class="fa fa-check"></i><b>5.6.1</b> Cars Assumptions Check</a></li>
<li class="chapter" data-level="5.6.2" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#log-transformation"><i class="fa fa-check"></i><b>5.6.2</b> Log Transformation</a></li>
<li class="chapter" data-level="5.6.3" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#log-transform-for-car-prices"><i class="fa fa-check"></i><b>5.6.3</b> Log Transform for Car Prices</a></li>
<li class="chapter" data-level="5.6.4" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#log-model-predictions"><i class="fa fa-check"></i><b>5.6.4</b> Log Model Predictions</a></li>
<li class="chapter" data-level="5.6.5" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#log-model-interpretations"><i class="fa fa-check"></i><b>5.6.5</b> Log Model Interpretations</a></li>
<li class="chapter" data-level="5.6.6" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#log-model-ci-for-beta_0-beta_1"><i class="fa fa-check"></i><b>5.6.6</b> Log Model CI for <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span></a></li>
<li class="chapter" data-level="5.6.7" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#log-model-ci-for-expected-response"><i class="fa fa-check"></i><b>5.6.7</b> Log Model CI for Expected Response</a></li>
<li class="chapter" data-level="5.6.8" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#log-model-prediction-interval"><i class="fa fa-check"></i><b>5.6.8</b> Log Model Prediction Interval</a></li>
<li class="chapter" data-level="5.6.9" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#confidence-interval-comparison"><i class="fa fa-check"></i><b>5.6.9</b> Confidence Interval Comparison</a></li>
<li class="chapter" data-level="5.6.10" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#prediction-interval-comparison"><i class="fa fa-check"></i><b>5.6.10</b> Prediction Interval Comparison</a></li>
<li class="chapter" data-level="5.6.11" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#log-model-visualization"><i class="fa fa-check"></i><b>5.6.11</b> Log Model Visualization</a></li>
<li class="chapter" data-level="5.6.12" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#comments-on-transformations"><i class="fa fa-check"></i><b>5.6.12</b> Comments on Transformations</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#case-studies"><i class="fa fa-check"></i><b>5.7</b> Case Studies</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#flights-from-ny-to-chi"><i class="fa fa-check"></i><b>5.7.1</b> Flights from NY to CHI</a></li>
<li class="chapter" data-level="5.7.2" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#smoking-during-pregnancy"><i class="fa fa-check"></i><b>5.7.2</b> Smoking During Pregnancy</a></li>
<li class="chapter" data-level="5.7.3" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#smoking-during-pregnancy-cont"><i class="fa fa-check"></i><b>5.7.3</b> Smoking During Pregnancy (cont)</a></li>
<li class="chapter" data-level="5.7.4" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#exam-scores"><i class="fa fa-check"></i><b>5.7.4</b> Exam Scores</a></li>
<li class="chapter" data-level="5.7.5" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#simulating-the-regression-effect"><i class="fa fa-check"></i><b>5.7.5</b> Simulating the Regression Effect</a></li>
<li class="chapter" data-level="5.7.6" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#nfl-wins"><i class="fa fa-check"></i><b>5.7.6</b> NFL Wins</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="normal-error-regression-model.html"><a href="normal-error-regression-model.html#impact-of-model-assumption-violations"><i class="fa fa-check"></i><b>5.8</b> Impact of Model Assumption Violations</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="building-models-for-interpretation.html"><a href="building-models-for-interpretation.html"><i class="fa fa-check"></i><b>6</b> Building Models for Interpretation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="building-models-for-interpretation.html"><a href="building-models-for-interpretation.html#model-building---sat-scores"><i class="fa fa-check"></i><b>6.1</b> Model Building - SAT Scores</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="building-models-for-interpretation.html"><a href="building-models-for-interpretation.html#modeling-for-interpretation"><i class="fa fa-check"></i><b>6.1.1</b> Modeling for Interpretation</a></li>
<li class="chapter" data-level="6.1.2" data-path="building-models-for-interpretation.html"><a href="building-models-for-interpretation.html#sat-scores-dataset"><i class="fa fa-check"></i><b>6.1.2</b> SAT Scores Dataset</a></li>
<li class="chapter" data-level="6.1.3" data-path="building-models-for-interpretation.html"><a href="building-models-for-interpretation.html#research-question"><i class="fa fa-check"></i><b>6.1.3</b> Research Question</a></li>
<li class="chapter" data-level="6.1.4" data-path="building-models-for-interpretation.html"><a href="building-models-for-interpretation.html#teacher-salary-and-sat-score"><i class="fa fa-check"></i><b>6.1.4</b> Teacher Salary and SAT score</a></li>
<li class="chapter" data-level="6.1.5" data-path="building-models-for-interpretation.html"><a href="building-models-for-interpretation.html#a-deeper-investigation"><i class="fa fa-check"></i><b>6.1.5</b> A Deeper Investigation</a></li>
<li class="chapter" data-level="6.1.6" data-path="building-models-for-interpretation.html"><a href="building-models-for-interpretation.html#student-to-teacher-ratio"><i class="fa fa-check"></i><b>6.1.6</b> Student-to-Teacher Ratio</a></li>
<li class="chapter" data-level="6.1.7" data-path="building-models-for-interpretation.html"><a href="building-models-for-interpretation.html#multicollinearity"><i class="fa fa-check"></i><b>6.1.7</b> Multicollinearity</a></li>
<li class="chapter" data-level="6.1.8" data-path="building-models-for-interpretation.html"><a href="building-models-for-interpretation.html#check-model-assumptions"><i class="fa fa-check"></i><b>6.1.8</b> Check Model Assumptions</a></li>
<li class="chapter" data-level="6.1.9" data-path="building-models-for-interpretation.html"><a href="building-models-for-interpretation.html#quadratic-term"><i class="fa fa-check"></i><b>6.1.9</b> Quadratic Term</a></li>
<li class="chapter" data-level="6.1.10" data-path="building-models-for-interpretation.html"><a href="building-models-for-interpretation.html#account-for-region"><i class="fa fa-check"></i><b>6.1.10</b> Account for Region?</a></li>
<li class="chapter" data-level="6.1.11" data-path="building-models-for-interpretation.html"><a href="building-models-for-interpretation.html#predictions-and-intervals"><i class="fa fa-check"></i><b>6.1.11</b> Predictions and Intervals</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="building-models-for-interpretation.html"><a href="building-models-for-interpretation.html#modeling-car-price"><i class="fa fa-check"></i><b>6.2</b> Modeling Car Price</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="building-models-for-interpretation.html"><a href="building-models-for-interpretation.html#model-for-price-of-2015-cars"><i class="fa fa-check"></i><b>6.2.1</b> Model for Price of 2015 Cars</a></li>
<li class="chapter" data-level="6.2.2" data-path="building-models-for-interpretation.html"><a href="building-models-for-interpretation.html#acc.-and-qrt.-mile-time"><i class="fa fa-check"></i><b>6.2.2</b> Acc. and Qrt. Mile Time</a></li>
<li class="chapter" data-level="6.2.3" data-path="building-models-for-interpretation.html"><a href="building-models-for-interpretation.html#adding-weight-to-model"><i class="fa fa-check"></i><b>6.2.3</b> Adding Weight to Model</a></li>
<li class="chapter" data-level="6.2.4" data-path="building-models-for-interpretation.html"><a href="building-models-for-interpretation.html#adding-more-variables"><i class="fa fa-check"></i><b>6.2.4</b> Adding More Variables</a></li>
<li class="chapter" data-level="6.2.5" data-path="building-models-for-interpretation.html"><a href="building-models-for-interpretation.html#check-of-model-assumptions"><i class="fa fa-check"></i><b>6.2.5</b> Check of Model Assumptions</a></li>
<li class="chapter" data-level="6.2.6" data-path="building-models-for-interpretation.html"><a href="building-models-for-interpretation.html#coefficients-and-exponentiation"><i class="fa fa-check"></i><b>6.2.6</b> Coefficients and Exponentiation</a></li>
<li class="chapter" data-level="6.2.7" data-path="building-models-for-interpretation.html"><a href="building-models-for-interpretation.html#confidence-and-prediction-intevals"><i class="fa fa-check"></i><b>6.2.7</b> Confidence and Prediction Intevals</a></li>
<li class="chapter" data-level="6.2.8" data-path="building-models-for-interpretation.html"><a href="building-models-for-interpretation.html#model-building-summary"><i class="fa fa-check"></i><b>6.2.8</b> Model Building Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="predictive-modeling.html"><a href="predictive-modeling.html"><i class="fa fa-check"></i><b>7</b> Predictive Modeling</a>
<ul>
<li class="chapter" data-level="7.1" data-path="predictive-modeling.html"><a href="predictive-modeling.html#modeling-for-prediction"><i class="fa fa-check"></i><b>7.1</b> Modeling for Prediction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="predictive-modeling.html"><a href="predictive-modeling.html#overview"><i class="fa fa-check"></i><b>7.1.1</b> Overview</a></li>
<li class="chapter" data-level="7.1.2" data-path="predictive-modeling.html"><a href="predictive-modeling.html#illustration-of-predictive-modeling"><i class="fa fa-check"></i><b>7.1.2</b> Illustration of Predictive Modeling</a></li>
<li class="chapter" data-level="7.1.3" data-path="predictive-modeling.html"><a href="predictive-modeling.html#predicting-new-data"><i class="fa fa-check"></i><b>7.1.3</b> Predicting New Data</a></li>
<li class="chapter" data-level="7.1.4" data-path="predictive-modeling.html"><a href="predictive-modeling.html#evaluating-predictions---rmspe"><i class="fa fa-check"></i><b>7.1.4</b> Evaluating Predictions - RMSPE</a></li>
<li class="chapter" data-level="7.1.5" data-path="predictive-modeling.html"><a href="predictive-modeling.html#training-data-error"><i class="fa fa-check"></i><b>7.1.5</b> Training Data Error</a></li>
<li class="chapter" data-level="7.1.6" data-path="predictive-modeling.html"><a href="predictive-modeling.html#graph-of-rmspe"><i class="fa fa-check"></i><b>7.1.6</b> Graph of RMSPE</a></li>
<li class="chapter" data-level="7.1.7" data-path="predictive-modeling.html"><a href="predictive-modeling.html#best-model"><i class="fa fa-check"></i><b>7.1.7</b> Best Model</a></li>
<li class="chapter" data-level="7.1.8" data-path="predictive-modeling.html"><a href="predictive-modeling.html#model-complexity-training-error-and-test-error"><i class="fa fa-check"></i><b>7.1.8</b> Model Complexity, Training Error, and Test Error</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="predictive-modeling.html"><a href="predictive-modeling.html#variance-bias-tradeoff"><i class="fa fa-check"></i><b>7.2</b> Variance-Bias Tradeoff</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="predictive-modeling.html"><a href="predictive-modeling.html#what-contributes-to-prediction-error"><i class="fa fa-check"></i><b>7.2.1</b> What Contributes to Prediction Error?</a></li>
<li class="chapter" data-level="7.2.2" data-path="predictive-modeling.html"><a href="predictive-modeling.html#variance-and-bias"><i class="fa fa-check"></i><b>7.2.2</b> Variance and Bias</a></li>
<li class="chapter" data-level="7.2.3" data-path="predictive-modeling.html"><a href="predictive-modeling.html#variance-bias-tradeoff-1"><i class="fa fa-check"></i><b>7.2.3</b> Variance-Bias Tradeoff</a></li>
<li class="chapter" data-level="7.2.4" data-path="predictive-modeling.html"><a href="predictive-modeling.html#modeling-for-prediction-1"><i class="fa fa-check"></i><b>7.2.4</b> Modeling for Prediction</a></li>
<li class="chapter" data-level="7.2.5" data-path="predictive-modeling.html"><a href="predictive-modeling.html#cross-validation"><i class="fa fa-check"></i><b>7.2.5</b> Cross-Validation</a></li>
<li class="chapter" data-level="7.2.6" data-path="predictive-modeling.html"><a href="predictive-modeling.html#cross-validation-illustration"><i class="fa fa-check"></i><b>7.2.6</b> Cross-Validation Illustration</a></li>
<li class="chapter" data-level="7.2.7" data-path="predictive-modeling.html"><a href="predictive-modeling.html#cv-in-r"><i class="fa fa-check"></i><b>7.2.7</b> CV in R</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="predictive-modeling.html"><a href="predictive-modeling.html#ridge-regression"><i class="fa fa-check"></i><b>7.3</b> Ridge Regression</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="predictive-modeling.html"><a href="predictive-modeling.html#complexity-in-model-coefficients"><i class="fa fa-check"></i><b>7.3.1</b> Complexity in Model Coefficients</a></li>
<li class="chapter" data-level="7.3.2" data-path="predictive-modeling.html"><a href="predictive-modeling.html#ridge-regression-penalty"><i class="fa fa-check"></i><b>7.3.2</b> Ridge Regression Penalty</a></li>
<li class="chapter" data-level="7.3.3" data-path="predictive-modeling.html"><a href="predictive-modeling.html#choosing-lambda"><i class="fa fa-check"></i><b>7.3.3</b> Choosing <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="7.3.4" data-path="predictive-modeling.html"><a href="predictive-modeling.html#ridge-regression-on-housing-dataset"><i class="fa fa-check"></i><b>7.3.4</b> Ridge Regression on Housing Dataset</a></li>
<li class="chapter" data-level="7.3.5" data-path="predictive-modeling.html"><a href="predictive-modeling.html#ridge-vs-ols"><i class="fa fa-check"></i><b>7.3.5</b> Ridge vs OLS</a></li>
<li class="chapter" data-level="7.3.6" data-path="predictive-modeling.html"><a href="predictive-modeling.html#lasso-and-elastic-net"><i class="fa fa-check"></i><b>7.3.6</b> Lasso and Elastic Net</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="predictive-modeling.html"><a href="predictive-modeling.html#decision-trees"><i class="fa fa-check"></i><b>7.4</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="predictive-modeling.html"><a href="predictive-modeling.html#basics-of-decision-trees"><i class="fa fa-check"></i><b>7.4.1</b> Basics of Decision Trees</a></li>
<li class="chapter" data-level="7.4.2" data-path="predictive-modeling.html"><a href="predictive-modeling.html#partitioning-in-a-decision-tree"><i class="fa fa-check"></i><b>7.4.2</b> Partitioning in A Decision Tree</a></li>
<li class="chapter" data-level="7.4.3" data-path="predictive-modeling.html"><a href="predictive-modeling.html#next-splits"><i class="fa fa-check"></i><b>7.4.3</b> Next Splits</a></li>
<li class="chapter" data-level="7.4.4" data-path="predictive-modeling.html"><a href="predictive-modeling.html#recursive-partitioning"><i class="fa fa-check"></i><b>7.4.4</b> Recursive Partitioning</a></li>
<li class="chapter" data-level="7.4.5" data-path="predictive-modeling.html"><a href="predictive-modeling.html#model-complexity-in-trees"><i class="fa fa-check"></i><b>7.4.5</b> Model Complexity in Trees</a></li>
<li class="chapter" data-level="7.4.6" data-path="predictive-modeling.html"><a href="predictive-modeling.html#cross-validation-on-housing-data"><i class="fa fa-check"></i><b>7.4.6</b> Cross-Validation on Housing Data</a></li>
<li class="chapter" data-level="7.4.7" data-path="predictive-modeling.html"><a href="predictive-modeling.html#comparing-ols-lasso-ridge-and-tree"><i class="fa fa-check"></i><b>7.4.7</b> Comparing OLS, Lasso, Ridge, and Tree</a></li>
<li class="chapter" data-level="7.4.8" data-path="predictive-modeling.html"><a href="predictive-modeling.html#random-forest"><i class="fa fa-check"></i><b>7.4.8</b> Random Forest</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="predictive-modeling.html"><a href="predictive-modeling.html#regression-splines"><i class="fa fa-check"></i><b>7.5</b> Regression Splines</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="predictive-modeling.html"><a href="predictive-modeling.html#regression-splines-1"><i class="fa fa-check"></i><b>7.5.1</b> Regression Splines</a></li>
<li class="chapter" data-level="7.5.2" data-path="predictive-modeling.html"><a href="predictive-modeling.html#two-models-with-high-bias"><i class="fa fa-check"></i><b>7.5.2</b> Two Models with High Bias</a></li>
<li class="chapter" data-level="7.5.3" data-path="predictive-modeling.html"><a href="predictive-modeling.html#cubic-splines"><i class="fa fa-check"></i><b>7.5.3</b> Cubic Splines</a></li>
<li class="chapter" data-level="7.5.4" data-path="predictive-modeling.html"><a href="predictive-modeling.html#predicting-test-data"><i class="fa fa-check"></i><b>7.5.4</b> Predicting Test Data</a></li>
<li class="chapter" data-level="7.5.5" data-path="predictive-modeling.html"><a href="predictive-modeling.html#implementation-of-splines"><i class="fa fa-check"></i><b>7.5.5</b> Implementation of Splines</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="predictive-modeling.html"><a href="predictive-modeling.html#summary-and-comparision"><i class="fa fa-check"></i><b>7.6</b> Summary and Comparision</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="predictive-modeling.html"><a href="predictive-modeling.html#modeling-with-ols"><i class="fa fa-check"></i><b>7.6.1</b> Modeling with OLS</a></li>
<li class="chapter" data-level="7.6.2" data-path="predictive-modeling.html"><a href="predictive-modeling.html#ridge-regression-with-housing-data"><i class="fa fa-check"></i><b>7.6.2</b> Ridge Regression with Housing Data</a></li>
<li class="chapter" data-level="7.6.3" data-path="predictive-modeling.html"><a href="predictive-modeling.html#decision-tree"><i class="fa fa-check"></i><b>7.6.3</b> Decision Tree</a></li>
<li class="chapter" data-level="7.6.4" data-path="predictive-modeling.html"><a href="predictive-modeling.html#comparing-performance"><i class="fa fa-check"></i><b>7.6.4</b> Comparing Performance</a></li>
<li class="chapter" data-level="7.6.5" data-path="predictive-modeling.html"><a href="predictive-modeling.html#predictions-on-new-data"><i class="fa fa-check"></i><b>7.6.5</b> Predictions on New Data</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="predictive-modeling.html"><a href="predictive-modeling.html#ethical-considerations-in-predictive-modeling"><i class="fa fa-check"></i><b>7.7</b> Ethical Considerations in Predictive Modeling</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="predictive-modeling.html"><a href="predictive-modeling.html#assumptions-in-predictive-models"><i class="fa fa-check"></i><b>7.7.1</b> Assumptions in Predictive Models</a></li>
<li class="chapter" data-level="7.7.2" data-path="predictive-modeling.html"><a href="predictive-modeling.html#amazon-hiring-algorithm"><i class="fa fa-check"></i><b>7.7.2</b> Amazon Hiring Algorithm</a></li>
<li class="chapter" data-level="7.7.3" data-path="predictive-modeling.html"><a href="predictive-modeling.html#facial-recognition"><i class="fa fa-check"></i><b>7.7.3</b> Facial Recognition</a></li>
<li class="chapter" data-level="7.7.4" data-path="predictive-modeling.html"><a href="predictive-modeling.html#comments"><i class="fa fa-check"></i><b>7.7.4</b> Comments</a></li>
<li class="chapter" data-level="7.7.5" data-path="predictive-modeling.html"><a href="predictive-modeling.html#modeling-for-prediction-2"><i class="fa fa-check"></i><b>7.7.5</b> Modeling for Prediction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html"><i class="fa fa-check"></i><b>8</b> Logistic Regression and Classification</a>
<ul>
<li class="chapter" data-level="8.1" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#logistic-regression"><i class="fa fa-check"></i><b>8.1</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#modeling-binary-response"><i class="fa fa-check"></i><b>8.1.1</b> Modeling Binary Response</a></li>
<li class="chapter" data-level="8.1.2" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#credit-card-dataset"><i class="fa fa-check"></i><b>8.1.2</b> Credit Card Dataset</a></li>
<li class="chapter" data-level="8.1.3" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#logistic-regression-model"><i class="fa fa-check"></i><b>8.1.3</b> Logistic Regression Model</a></li>
<li class="chapter" data-level="8.1.4" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#logistic-regression-model-for-default"><i class="fa fa-check"></i><b>8.1.4</b> Logistic Regression Model for Default</a></li>
<li class="chapter" data-level="8.1.5" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#where-do-the-bs-come-from"><i class="fa fa-check"></i><b>8.1.5</b> Where do the bâ€™s come from?</a></li>
<li class="chapter" data-level="8.1.6" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#odds-and-odds-ratio"><i class="fa fa-check"></i><b>8.1.6</b> Odds and Odds Ratio</a></li>
<li class="chapter" data-level="8.1.7" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#interpretation-of-beta_1"><i class="fa fa-check"></i><b>8.1.7</b> Interpretation of <span class="math inline">\(\beta_1\)</span></a></li>
<li class="chapter" data-level="8.1.8" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#hypothesis-tests-in-logistic-regression"><i class="fa fa-check"></i><b>8.1.8</b> Hypothesis Tests in Logistic Regression</a></li>
<li class="chapter" data-level="8.1.9" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#confidence-intervals-for-beta_1"><i class="fa fa-check"></i><b>8.1.9</b> Confidence Intervals for <span class="math inline">\(\beta_1\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>8.2</b> Multiple Logistic Regression</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#logistic-regression-models-with-multiple-explanatory-variables"><i class="fa fa-check"></i><b>8.2.1</b> Logistic Regression Models with Multiple Explanatory Variables</a></li>
<li class="chapter" data-level="8.2.2" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#multiple-logistic-regression-interpretations"><i class="fa fa-check"></i><b>8.2.2</b> Multiple Logistic Regression Interpretations</a></li>
<li class="chapter" data-level="8.2.3" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#hypothesis-tests-in-multiple-logistic-regression-model"><i class="fa fa-check"></i><b>8.2.3</b> Hypothesis Tests in Multiple Logistic Regression Model</a></li>
<li class="chapter" data-level="8.2.4" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#multiple-logistic-regression-model-with-interaction"><i class="fa fa-check"></i><b>8.2.4</b> Multiple Logistic Regression Model with Interaction</a></li>
<li class="chapter" data-level="8.2.5" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#interpretations-for-logistic-model-with-interaction"><i class="fa fa-check"></i><b>8.2.5</b> Interpretations for Logistic Model with Interaction</a></li>
<li class="chapter" data-level="8.2.6" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#logistic-regression-key-points"><i class="fa fa-check"></i><b>8.2.6</b> Logistic Regression Key Points</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#assessing-a-classifiers-performance"><i class="fa fa-check"></i><b>8.3</b> Assessing a Classifierâ€™s Performance</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#measuring-prediction-accuracy"><i class="fa fa-check"></i><b>8.3.1</b> Measuring Prediction Accuracy</a></li>
<li class="chapter" data-level="8.3.2" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#classification-accuracy"><i class="fa fa-check"></i><b>8.3.2</b> Classification Accuracy</a></li>
<li class="chapter" data-level="8.3.3" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#first-50-cases"><i class="fa fa-check"></i><b>8.3.3</b> First 50 Cases</a></li>
<li class="chapter" data-level="8.3.4" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#confusion-matrix"><i class="fa fa-check"></i><b>8.3.4</b> Confusion Matrix</a></li>
<li class="chapter" data-level="8.3.5" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#full-test-data-confusion-matrix"><i class="fa fa-check"></i><b>8.3.5</b> Full Test Data Confusion Matrix</a></li>
<li class="chapter" data-level="8.3.6" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#decision-tree-classifier"><i class="fa fa-check"></i><b>8.3.6</b> Decision Tree Classifier</a></li>
<li class="chapter" data-level="8.3.7" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#tree-confusion-matrix"><i class="fa fa-check"></i><b>8.3.7</b> Tree Confusion Matrix</a></li>
<li class="chapter" data-level="8.3.8" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#more-on-accuracy-rate"><i class="fa fa-check"></i><b>8.3.8</b> More on Accuracy Rate</a></li>
<li class="chapter" data-level="8.3.9" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#sensitivity-and-specificity"><i class="fa fa-check"></i><b>8.3.9</b> Sensitivity and Specificity</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#receiver-operating-characteristic-curve"><i class="fa fa-check"></i><b>8.4</b> Receiver Operating Characteristic Curve</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#roc-curve"><i class="fa fa-check"></i><b>8.4.1</b> ROC Curve</a></li>
<li class="chapter" data-level="8.4.2" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#lr-and-tree-roc-curves"><i class="fa fa-check"></i><b>8.4.2</b> LR and Tree ROC Curves</a></li>
<li class="chapter" data-level="8.4.3" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#constructing-roc-curve"><i class="fa fa-check"></i><b>8.4.3</b> Constructing ROC Curve</a></li>
<li class="chapter" data-level="8.4.4" data-path="logistic-regression-and-classification.html"><a href="logistic-regression-and-classification.html#construct-roc-example"><i class="fa fa-check"></i><b>8.4.4</b> Construct ROC Example</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Stat 255: Statistics for Data Science Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="predictive-modeling" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Predictive Modeling<a href="predictive-modeling.html#predictive-modeling" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><strong>Learning Outcomes:</strong></p>
<ol style="list-style-type: decimal">
<li>Explain how prediction error changes, depending on model complexity, for both training data and test data.<br />
</li>
<li>Explain the variance bias tradeoff, and identify situations when predictions might be impacted by either variance or bias.<br />
</li>
<li>Describe how overfitting can impact prediction accuracy.<br />
</li>
<li>Explain how and why we use cross-validation in predictive modeling.<br />
</li>
<li>Compare and contrast Ridge regression and ordinary least squares regression.<br />
</li>
<li>Explain the role of the parameter <span class="math inline">\(\lambda\)</span> is Ridge regression.<br />
</li>
<li>Given possible sets of regression coefficients, determine which would be optimal, using either ordinary least squares, Ridge regression, or Lasso regression.<br />
</li>
<li>Compare and contrast the assumptions made in decision trees, to those in the normal error regression model.<br />
</li>
<li>Given possible splits for a node in a decision tree, determine which is optimal.<br />
</li>
<li>Explain how the depth of a tree impacts prediction variance and bias, and overfitting.<br />
</li>
<li>Describe how decision trees in a random forest differ from one another.<br />
</li>
<li>Explain the process of fitting a polynomial spline to data, and the impact the the number of knots has on the process.<br />
</li>
<li>Explain the assumptions made in predictive modeling, and identify situations where these assumptions might be inappropriate or problematic from an ethical standpoint.</li>
</ol>
<div id="modeling-for-prediction" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Modeling for Prediction<a href="predictive-modeling.html#modeling-for-prediction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="overview" class="section level3 hasAnchor" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Overview<a href="predictive-modeling.html#overview" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Weâ€™ve previously learned how to build models for the purpose of interpretation, when our primary focus is on understanding relationships between variables in the model. In this chapter, weâ€™ll examine how to build models for situations when we are not interested in understanding relationships between variables, and instead care only about making the most accurate predictions possible.</p>
<p>Weâ€™ve seen that when we model for interpretation, we encounter a tradeoff between model complexity and interpretability. We wanted to choose a model that is complex enough to reasonably approximate the structure of the underlying data, but at the same time, not so complicated that it becomes hard to interpret. When modeling for prediction, we donâ€™t need to worry about interpretability, which can sometimes make more complex models more desirable. Nevertheless, weâ€™ll encounter a different kind of tradeoff, involving model complexity, that weâ€™ll have to think about, and weâ€™ll see that more complex models do not always lead to better predictions.</p>
<p><strong>Predictive Modeling Vocabulary</strong></p>
<ul>
<li><p>The new data on which we make predictions is called <strong>test data</strong>.</p></li>
<li><p>The data used to fit the model is called <strong>training data</strong>.</p></li>
</ul>
<p>In the training data, we know the values of the explanatory and response variables. In the test data, we know only the values of the explanatory variables and want to predict the values of the response variable.</p>
</div>
<div id="illustration-of-predictive-modeling" class="section level3 hasAnchor" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Illustration of Predictive Modeling<a href="predictive-modeling.html#illustration-of-predictive-modeling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The illustration shows observations from a simulated dataset consisting of 100 observations of a single explanatory variable <span class="math inline">\(x\)</span>, and response variable <span class="math inline">\(y\)</span>. We want to find a model that captures the trend in the data and will be best able to predict new values of y, for given x.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-525-1.png" width="768" /></p>
<p>Weâ€™ll fit several different polynomial models to the data, increasing in complexity from the most simple model we could possibly use, a constant model, to a very complex eighth degree polynomial model.</p>
<p><strong>Constant Model to Sample Data</strong></p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-526-1.png" width="768" /></p>
<p><strong>Linear Model to Sample Data</strong></p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-527-1.png" width="768" /></p>
<p><strong>Quadratic Model</strong></p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-528-1.png" width="768" /></p>
<p><strong>Degree 3, 4, and 8 Models</strong></p>
<p>We continue exploring higher order polynomial models. The blue curve represents a third degree (cubic) polynomial model, while the red curve represents a fourth degree (quartic) model and the green represents an eighth degree model.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-529-1.png" width="768" /></p>
<p>We see that the flexibility of the model increases as we add higher-order terms. The curve is allowed to have more twists and bends. For higher-order, more complex models, individual points have more influence on the shape of the curve. This can be both a good and bad thing, as it allows the model to better bend and fit the data, but also makes it susceptible to the influence of outliers.</p>
</div>
<div id="predicting-new-data" class="section level3 hasAnchor" number="7.1.3">
<h3><span class="header-section-number">7.1.3</span> Predicting New Data<a href="predictive-modeling.html#predicting-new-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now, suppose we have a new dataset of 100, x-values, and want to predict <span class="math inline">\(y\)</span>. The first 5 rows of the new dataset are shown</p>
<table>
<thead>
<tr class="header">
<th align="right">x</th>
<th align="left">Prediction</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">3.196237</td>
<td align="left">?</td>
</tr>
<tr class="even">
<td align="right">1.475586</td>
<td align="left">?</td>
</tr>
<tr class="odd">
<td align="right">5.278882</td>
<td align="left">?</td>
</tr>
<tr class="even">
<td align="right">5.529299</td>
<td align="left">?</td>
</tr>
<tr class="odd">
<td align="right">7.626731</td>
<td align="left">?</td>
</tr>
</tbody>
</table>
<p>We fit polynomial models of degree 0 through 8 to the data. Note that although we did not show the 5th through 7th degree models in our illustrations, weâ€™ll still fit these to the data.</p>
<div class="sourceCode" id="cb705"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb705-1"><a href="predictive-modeling.html#cb705-1" aria-hidden="true" tabindex="-1"></a>Sim_M0 <span class="ot">&lt;-</span><span class="fu">lm</span>(<span class="at">data=</span>Sampdf, y<span class="sc">~</span><span class="dv">1</span>)</span>
<span id="cb705-2"><a href="predictive-modeling.html#cb705-2" aria-hidden="true" tabindex="-1"></a>Sim_M1 <span class="ot">&lt;-</span><span class="fu">lm</span>(<span class="at">data=</span>Sampdf, y<span class="sc">~</span>x)</span>
<span id="cb705-3"><a href="predictive-modeling.html#cb705-3" aria-hidden="true" tabindex="-1"></a>Sim_M2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data=</span>Sampdf, y<span class="sc">~</span>x<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb705-4"><a href="predictive-modeling.html#cb705-4" aria-hidden="true" tabindex="-1"></a>Sim_M3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data=</span>Sampdf, y<span class="sc">~</span>x<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">2</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">3</span>))</span>
<span id="cb705-5"><a href="predictive-modeling.html#cb705-5" aria-hidden="true" tabindex="-1"></a>Sim_M4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data=</span>Sampdf, y<span class="sc">~</span>x<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">2</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">3</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">4</span>))</span>
<span id="cb705-6"><a href="predictive-modeling.html#cb705-6" aria-hidden="true" tabindex="-1"></a>Sim_M5 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data=</span>Sampdf, y<span class="sc">~</span>x<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">2</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">3</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">4</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">5</span>))</span>
<span id="cb705-7"><a href="predictive-modeling.html#cb705-7" aria-hidden="true" tabindex="-1"></a>Sim_M6 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data=</span>Sampdf, y<span class="sc">~</span>x<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">2</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">3</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">4</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">5</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">6</span>))</span>
<span id="cb705-8"><a href="predictive-modeling.html#cb705-8" aria-hidden="true" tabindex="-1"></a>Sim_M7 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data=</span>Sampdf, y<span class="sc">~</span>x<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">2</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">3</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">4</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">5</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">6</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">7</span>))</span>
<span id="cb705-9"><a href="predictive-modeling.html#cb705-9" aria-hidden="true" tabindex="-1"></a>Sim_M8 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data=</span>Sampdf, y<span class="sc">~</span>x<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">2</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">3</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">4</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">5</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">6</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">7</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">8</span>))</span></code></pre></div>
<p>We predict the values of the new observations, using each of the 9 models.</p>
<div class="sourceCode" id="cb706"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb706-1"><a href="predictive-modeling.html#cb706-1" aria-hidden="true" tabindex="-1"></a>Newdf<span class="sc">$</span>Deg0Pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(Sim_M0, <span class="at">newdata=</span>Newdf)</span>
<span id="cb706-2"><a href="predictive-modeling.html#cb706-2" aria-hidden="true" tabindex="-1"></a>Newdf<span class="sc">$</span>Deg1Pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(Sim_M1, <span class="at">newdata=</span>Newdf)</span>
<span id="cb706-3"><a href="predictive-modeling.html#cb706-3" aria-hidden="true" tabindex="-1"></a>Newdf<span class="sc">$</span>Deg2Pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(Sim_M2, <span class="at">newdata=</span>Newdf)</span>
<span id="cb706-4"><a href="predictive-modeling.html#cb706-4" aria-hidden="true" tabindex="-1"></a>Newdf<span class="sc">$</span>Deg3Pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(Sim_M3, <span class="at">newdata=</span>Newdf)</span>
<span id="cb706-5"><a href="predictive-modeling.html#cb706-5" aria-hidden="true" tabindex="-1"></a>Newdf<span class="sc">$</span>Deg4Pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(Sim_M4, <span class="at">newdata=</span>Newdf)</span>
<span id="cb706-6"><a href="predictive-modeling.html#cb706-6" aria-hidden="true" tabindex="-1"></a>Newdf<span class="sc">$</span>Deg5Pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(Sim_M5, <span class="at">newdata=</span>Newdf)</span>
<span id="cb706-7"><a href="predictive-modeling.html#cb706-7" aria-hidden="true" tabindex="-1"></a>Newdf<span class="sc">$</span>Deg6Pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(Sim_M6, <span class="at">newdata=</span>Newdf)</span>
<span id="cb706-8"><a href="predictive-modeling.html#cb706-8" aria-hidden="true" tabindex="-1"></a>Newdf<span class="sc">$</span>Deg7Pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(Sim_M7, <span class="at">newdata=</span>Newdf)</span>
<span id="cb706-9"><a href="predictive-modeling.html#cb706-9" aria-hidden="true" tabindex="-1"></a>Newdf<span class="sc">$</span>Deg8Pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(Sim_M8, <span class="at">newdata=</span>Newdf)</span></code></pre></div>
<p>In fact, since these data were simulated, we know the true value of <span class="math inline">\(y\)</span>, so we can compare the predicted values to the true ones.</p>
<div class="sourceCode" id="cb707"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb707-1"><a href="predictive-modeling.html#cb707-1" aria-hidden="true" tabindex="-1"></a><span class="fu">kable</span>(Newdf <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span><span class="fu">c</span>(samp)) <span class="sc">%&gt;%</span> <span class="fu">round</span>(<span class="dv">2</span>) <span class="sc">%&gt;%</span> <span class="fu">head</span>(<span class="dv">5</span>))</span></code></pre></div>
<table>
<colgroup>
<col width="5%" />
<col width="5%" />
<col width="6%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">x</th>
<th align="right">y</th>
<th align="right">Deg0Pred</th>
<th align="right">Deg1Pred</th>
<th align="right">Deg2Pred</th>
<th align="right">Deg3Pred</th>
<th align="right">Deg4Pred</th>
<th align="right">Deg5Pred</th>
<th align="right">Deg6Pred</th>
<th align="right">Deg7Pred</th>
<th align="right">Deg8Pred</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">108</td>
<td align="right">5.53</td>
<td align="right">5.49</td>
<td align="right">1.17</td>
<td align="right">1.05</td>
<td align="right">0.41</td>
<td align="right">-0.14</td>
<td align="right">-0.30</td>
<td align="right">0.10</td>
<td align="right">0.40</td>
<td align="right">0.25</td>
<td align="right">0.31</td>
</tr>
<tr class="even">
<td align="left">4371</td>
<td align="right">2.05</td>
<td align="right">3.92</td>
<td align="right">1.17</td>
<td align="right">1.89</td>
<td align="right">2.02</td>
<td align="right">3.66</td>
<td align="right">3.95</td>
<td align="right">4.26</td>
<td align="right">3.82</td>
<td align="right">3.37</td>
<td align="right">3.49</td>
</tr>
<tr class="odd">
<td align="left">4839</td>
<td align="right">3.16</td>
<td align="right">1.46</td>
<td align="right">1.17</td>
<td align="right">1.63</td>
<td align="right">1.29</td>
<td align="right">3.24</td>
<td align="right">3.35</td>
<td align="right">2.78</td>
<td align="right">2.24</td>
<td align="right">2.15</td>
<td align="right">2.07</td>
</tr>
<tr class="even">
<td align="left">6907</td>
<td align="right">2.06</td>
<td align="right">6.79</td>
<td align="right">1.17</td>
<td align="right">1.89</td>
<td align="right">2.02</td>
<td align="right">3.66</td>
<td align="right">3.95</td>
<td align="right">4.25</td>
<td align="right">3.80</td>
<td align="right">3.36</td>
<td align="right">3.47</td>
</tr>
<tr class="odd">
<td align="left">7334</td>
<td align="right">2.92</td>
<td align="right">-1.03</td>
<td align="right">1.17</td>
<td align="right">1.68</td>
<td align="right">1.42</td>
<td align="right">3.43</td>
<td align="right">3.60</td>
<td align="right">3.14</td>
<td align="right">2.51</td>
<td align="right">2.31</td>
<td align="right">2.25</td>
</tr>
</tbody>
</table>
</div>
<div id="evaluating-predictions---rmspe" class="section level3 hasAnchor" number="7.1.4">
<h3><span class="header-section-number">7.1.4</span> Evaluating Predictions - RMSPE<a href="predictive-modeling.html#evaluating-predictions---rmspe" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For quantitative response variables, we can evaluate the predictions by calculating the average of the squared differences between the true and predicted values. Often, we look at the square root of this quantity. This is called the Root Mean Square Prediction Error (RMSPE).</p>
<p><span class="math display">\[
\text{RMSPE} = \sqrt{\displaystyle\sum_{i=1}^{n&#39;}\frac{(y_i-\hat{y}_i)^2}{n&#39;}},
\]</span></p>
<p>where <span class="math inline">\(n&#39;\)</span> represents the number of new cases being predicted.</p>
<p>We calcuate RMSPE for each of the 9 models.</p>
<div class="sourceCode" id="cb708"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb708-1"><a href="predictive-modeling.html#cb708-1" aria-hidden="true" tabindex="-1"></a>RMSPE0 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((Newdf<span class="sc">$</span>y<span class="sc">-</span>Newdf<span class="sc">$</span>Deg0Pred)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb708-2"><a href="predictive-modeling.html#cb708-2" aria-hidden="true" tabindex="-1"></a>RMSPE1 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((Newdf<span class="sc">$</span>y<span class="sc">-</span>Newdf<span class="sc">$</span>Deg1Pred)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb708-3"><a href="predictive-modeling.html#cb708-3" aria-hidden="true" tabindex="-1"></a>RMSPE2 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((Newdf<span class="sc">$</span>y<span class="sc">-</span>Newdf<span class="sc">$</span>Deg2Pred)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb708-4"><a href="predictive-modeling.html#cb708-4" aria-hidden="true" tabindex="-1"></a>RMSPE3 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((Newdf<span class="sc">$</span>y<span class="sc">-</span>Newdf<span class="sc">$</span>Deg3Pred)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb708-5"><a href="predictive-modeling.html#cb708-5" aria-hidden="true" tabindex="-1"></a>RMSPE4 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((Newdf<span class="sc">$</span>y<span class="sc">-</span>Newdf<span class="sc">$</span>Deg4Pred)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb708-6"><a href="predictive-modeling.html#cb708-6" aria-hidden="true" tabindex="-1"></a>RMSPE5 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((Newdf<span class="sc">$</span>y<span class="sc">-</span>Newdf<span class="sc">$</span>Deg5Pred)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb708-7"><a href="predictive-modeling.html#cb708-7" aria-hidden="true" tabindex="-1"></a>RMSPE6 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((Newdf<span class="sc">$</span>y<span class="sc">-</span>Newdf<span class="sc">$</span>Deg6Pred)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb708-8"><a href="predictive-modeling.html#cb708-8" aria-hidden="true" tabindex="-1"></a>RMSPE7 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((Newdf<span class="sc">$</span>y<span class="sc">-</span>Newdf<span class="sc">$</span>Deg7Pred)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb708-9"><a href="predictive-modeling.html#cb708-9" aria-hidden="true" tabindex="-1"></a>RMSPE8 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((Newdf<span class="sc">$</span>y<span class="sc">-</span>Newdf<span class="sc">$</span>Deg8Pred)<span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">Degree</th>
<th align="right">RMSPE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0</td>
<td align="right">4.051309</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">3.849624</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">3.726767</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">3.256592</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">3.283513</td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="right">3.341336</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">3.346908</td>
</tr>
<tr class="even">
<td align="right">7</td>
<td align="right">3.370821</td>
</tr>
<tr class="odd">
<td align="right">8</td>
<td align="right">3.350198</td>
</tr>
</tbody>
</table>
<p>The third degree model did the best at predicting the new data.</p>
<p>Notice that making the model more complex beyond third degree not only didnâ€™t help, but actually hurt prediction accuracy.</p>
</div>
<div id="training-data-error" class="section level3 hasAnchor" number="7.1.5">
<h3><span class="header-section-number">7.1.5</span> Training Data Error<a href="predictive-modeling.html#training-data-error" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now, letâ€™s examine the behavior if we had fit the models to the data, instead of the test data.</p>
<div class="sourceCode" id="cb709"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb709-1"><a href="predictive-modeling.html#cb709-1" aria-hidden="true" tabindex="-1"></a>RMSE0 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(Sim_M0<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb709-2"><a href="predictive-modeling.html#cb709-2" aria-hidden="true" tabindex="-1"></a>RMSE1 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(Sim_M1<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb709-3"><a href="predictive-modeling.html#cb709-3" aria-hidden="true" tabindex="-1"></a>RMSE2 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(Sim_M2<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb709-4"><a href="predictive-modeling.html#cb709-4" aria-hidden="true" tabindex="-1"></a>RMSE3 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(Sim_M3<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb709-5"><a href="predictive-modeling.html#cb709-5" aria-hidden="true" tabindex="-1"></a>RMSE4 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(Sim_M4<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb709-6"><a href="predictive-modeling.html#cb709-6" aria-hidden="true" tabindex="-1"></a>RMSE5 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(Sim_M5<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb709-7"><a href="predictive-modeling.html#cb709-7" aria-hidden="true" tabindex="-1"></a>RMSE6 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(Sim_M6<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb709-8"><a href="predictive-modeling.html#cb709-8" aria-hidden="true" tabindex="-1"></a>RMSE7 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(Sim_M7<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb709-9"><a href="predictive-modeling.html#cb709-9" aria-hidden="true" tabindex="-1"></a>RMSE8 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(Sim_M8<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<div class="sourceCode" id="cb710"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb710-1"><a href="predictive-modeling.html#cb710-1" aria-hidden="true" tabindex="-1"></a>Degree <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">8</span></span>
<span id="cb710-2"><a href="predictive-modeling.html#cb710-2" aria-hidden="true" tabindex="-1"></a>Test <span class="ot">&lt;-</span> <span class="fu">c</span>(RMSPE0, RMSPE1, RMSPE2, RMSPE3, RMSPE4, RMSPE5, RMSPE6, RMSPE7, RMSPE8)</span>
<span id="cb710-3"><a href="predictive-modeling.html#cb710-3" aria-hidden="true" tabindex="-1"></a>Train <span class="ot">&lt;-</span> <span class="fu">c</span>(RMSE0, RMSE1, RMSE2, RMSE3, RMSE4, RMSE5, RMSE6, RMSE7, RMSE8)</span>
<span id="cb710-4"><a href="predictive-modeling.html#cb710-4" aria-hidden="true" tabindex="-1"></a>RMSPEdf <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(Degree, Test, Train)</span>
<span id="cb710-5"><a href="predictive-modeling.html#cb710-5" aria-hidden="true" tabindex="-1"></a>RMSPEdf</span></code></pre></div>
<pre><code>##   Degree     Test    Train
## 1      0 4.051309 3.431842
## 2      1 3.849624 3.366650
## 3      2 3.726767 3.296821
## 4      3 3.256592 2.913233
## 5      4 3.283513 2.906845
## 6      5 3.341336 2.838738
## 7      6 3.346908 2.809928
## 8      7 3.370821 2.800280
## 9      8 3.350198 2.799066</code></pre>
<p>Notice that the most complex model achieves the best performance on the training data, but not on the test data.</p>
<p>As the model complexity grows, the model will always fit the training data better, but that does not mean it will perform better on new data. It is possible to start modeling noise, rather than true signal in the training data, which hurts the accuracy of the model when applied to new data.</p>
</div>
<div id="graph-of-rmspe" class="section level3 hasAnchor" number="7.1.6">
<h3><span class="header-section-number">7.1.6</span> Graph of RMSPE<a href="predictive-modeling.html#graph-of-rmspe" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-540-1.png" width="672" /></p>
<ul>
<li>Training error decreases as model becomes more complex<br />
</li>
<li>Testing error is lowest for the 3rd degree model, then starts to increase again</li>
</ul>
</div>
<div id="best-model" class="section level3 hasAnchor" number="7.1.7">
<h3><span class="header-section-number">7.1.7</span> Best Model<a href="predictive-modeling.html#best-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Of the models we looked at, the third degree model does the best. The estimates of its coefficients are shown below.</p>
<div class="sourceCode" id="cb712"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb712-1"><a href="predictive-modeling.html#cb712-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(Sim_M3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x + I(x^2) + I(x^3), data = Sampdf)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.9451 -1.7976  0.1685  1.3988  6.8064 
## 
## Coefficients:
##             Estimate Std. Error t value   Pr(&gt;|t|)    
## (Intercept) -0.54165    1.26803  -0.427   0.670221    
## x            4.16638    1.09405   3.808   0.000247 ***
## I(x^2)      -1.20601    0.25186  -4.788 0.00000610 ***
## I(x^3)       0.08419    0.01622   5.191 0.00000117 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.973 on 96 degrees of freedom
## Multiple R-squared:  0.2794, Adjusted R-squared:  0.2569 
## F-statistic: 12.41 on 3 and 96 DF,  p-value: 0.0000006309</code></pre>
<p>In fact, the data were generated from the model <span class="math inline">\(y_i = 4.5x - 1.4x^2 + 0.1x^3 + \epsilon_i\)</span>, where <span class="math inline">\(\epsilon_i\sim\mathcal{N}(0,3)\)</span></p>
<p>We compare the true expected response curve (in yellow) to the estimates from the various polynomial models.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-543-1.png" width="768" /></p>
<p>The 8th degree model performs worse than the cubic. The extra terms cause the model to be â€œtoo flexible,â€ and it starts to model random fluctuations (noise) in the training data, that do not capture the true trend for the population. This is called <strong>overfitting.</strong></p>
</div>
<div id="model-complexity-training-error-and-test-error" class="section level3 hasAnchor" number="7.1.8">
<h3><span class="header-section-number">7.1.8</span> Model Complexity, Training Error, and Test Error<a href="predictive-modeling.html#model-complexity-training-error-and-test-error" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-544-1.png" width="960" /></p>
</div>
</div>
<div id="variance-bias-tradeoff" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Variance-Bias Tradeoff<a href="predictive-modeling.html#variance-bias-tradeoff" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="what-contributes-to-prediction-error" class="section level3 hasAnchor" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> What Contributes to Prediction Error?<a href="predictive-modeling.html#what-contributes-to-prediction-error" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose <span class="math inline">\(Y_i = f(x_i) + \epsilon_i\)</span>, where <span class="math inline">\(\epsilon_i\sim\mathcal{N}(0,\sigma)\)</span>.</p>
<p>Let <span class="math inline">\(\hat{f}\)</span> represent the function of our explanatory variable(s) <span class="math inline">\(x^*\)</span> used to predict the value of response variable <span class="math inline">\(y^*\)</span>. Thus <span class="math inline">\(\hat{y}^* = f(x^*)\)</span>.</p>
<p>There are three factors that contribute to the expected value of <span class="math inline">\(\left(y^* - \hat{y}\right)^2 = \left(y^* - \hat{f}(x^*)\right)^2\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Bias associated with fitting model:</strong> Model bias pertains to the difference between the true response function value <span class="math inline">\(f(x^*)\)</span>, and the average value of <span class="math inline">\(\hat{f}(x^*)\)</span> that would be obtained in the long run over many samples.<br />
- for example, if the true response function <span class="math inline">\(f\)</span> is cubic, then using a constant, linear, or quadratic model would result in biased predictions for most values of <span class="math inline">\(x^*\)</span>.</p></li>
<li><p><strong>Variance associated with fitting model:</strong> Individual observations in the training data are subject to random sampling variability. The more flexible a model is, the more weight is put on each individual observation increasing the variance associated with the model.</p></li>
<li><p><strong>Variability associated with prediction:</strong> Even if we knew the true value <span class="math inline">\(f(x^*)\)</span>, which represents the expected value of <span class="math inline">\(y^*\)</span> given <span class="math inline">\(x=x^*\)</span>, the actual value of <span class="math inline">\(y^*\)</span> will vary due to random noise (i.e.Â the <span class="math inline">\(\epsilon_i\sim\mathcal{N}(0,\sigma)\)</span> term).</p></li>
</ol>
</div>
<div id="variance-and-bias" class="section level3 hasAnchor" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Variance and Bias<a href="predictive-modeling.html#variance-and-bias" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The third source of variability cannot be controlled or eliminated. The first two, however are things we can control. If we could figure out how to minimize bias while also minimizing variance associated with a prediction, that would be great! Butâ€¦</p>
<p>The constant model suffers from high bias. Since it does not include a linear, quadratic, or cubic term, it cannot accurately approximate the true regression function.</p>
<p>The Eighth degree model suffers from high variance. Although it could, in theory, approximate the true regression function correctly, it is too flexible, and is thrown off because of the influence of individual points with high degrees of variability.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-545-1.png" width="960" /></p>
</div>
<div id="variance-bias-tradeoff-1" class="section level3 hasAnchor" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> Variance-Bias Tradeoff<a href="predictive-modeling.html#variance-bias-tradeoff-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As model complexity (flexibility) increases, bias decreases. Variance, however, increases.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-546-1.png" width="960" /></p>
<p>In fact, it can be shown that:</p>
<p><span class="math inline">\(\text{Expected RMSPE} = \text{Variance} + \text{Bias}^2\)</span></p>
<p>Our goal is the find the â€œsweetspotâ€ where expected RMSPE is minimized.</p>
</div>
<div id="modeling-for-prediction-1" class="section level3 hasAnchor" number="7.2.4">
<h3><span class="header-section-number">7.2.4</span> Modeling for Prediction<a href="predictive-modeling.html#modeling-for-prediction-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>When our purpose is purely prediction, we donâ€™t need to worry about keeping the model simple enough to interpret.<br />
</li>
<li>Goal is to fit data well enough to make good predictions on new data without modeling random noise in the training (overfitting)<br />
</li>
<li>A model that is too simple suffers from high bias<br />
</li>
<li>A model that is too complex suffers from high variance and is prone to overfitting<br />
</li>
<li>The right balance is different for every dataset<br />
</li>
<li>Measuring error on data used to fit the model (training data) does not accurately predict how well model will be able to predict new data (test data)</li>
</ul>
</div>
<div id="cross-validation" class="section level3 hasAnchor" number="7.2.5">
<h3><span class="header-section-number">7.2.5</span> Cross-Validation<a href="predictive-modeling.html#cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Weâ€™ve seen that training error is not an accurate approximation of test error. Instead, weâ€™ll approximate test error, by setting aside a set of the training data, and using it as if it were a test set. This process is called <strong>cross-validation</strong>, and the set we put aside is called the <strong>validation set.</strong></p>
<ol style="list-style-type: decimal">
<li>Partition data into disjoint sets (folds). Approximately 5 folds recommended.<br />
</li>
<li>Build a model using 4 of the 5 folds.<br />
</li>
<li>Use model to predict responses for remaining fold.</li>
<li>Calculate root mean square error <span class="math inline">\(RMSPE=\displaystyle\sqrt{\frac{\sum((\hat{y}_i-y_i)^2)}{n&#39;}}\)</span>.<br />
</li>
<li>Repeat for each of 5 folds.<br />
</li>
<li>Average RMSPE values across folds.</li>
</ol>
<p>If computational resources permit, it is often beneficial to perform CV multiple times, using different sets of folds.</p>
</div>
<div id="cross-validation-illustration" class="section level3 hasAnchor" number="7.2.6">
<h3><span class="header-section-number">7.2.6</span> Cross-Validation Illustration<a href="predictive-modeling.html#cross-validation-illustration" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-547"></span>
<img src="CV2.png" alt="https://www.researchgate.net/figure/A-schematic-illustration-of-K-fold-cross-validation-for-K-5-Original-dataset-shown_fig5_311668395" width="75%" />
<p class="caption">
Figure 7.1: <a href="https://www.researchgate.net/figure/A-schematic-illustration-of-K-fold-cross-validation-for-K-5-Original-dataset-shown_fig5_311668395" class="uri">https://www.researchgate.net/figure/A-schematic-illustration-of-K-fold-cross-validation-for-K-5-Original-dataset-shown_fig5_311668395</a>
</p>
</div>
</div>
<div id="cv-in-r" class="section level3 hasAnchor" number="7.2.7">
<h3><span class="header-section-number">7.2.7</span> CV in R<a href="predictive-modeling.html#cv-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <code>train</code> function in the <code>caret</code> R package performs cross validation automatically. Weâ€™ll use it to compare five different models for house prices among a dataset of 1,000 houses sold in Ames, IA between 2006 and 2010.</p>
<p>Weâ€™ll consider six different models of (mostly) increasing complexity.</p>
<div class="sourceCode" id="cb714"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb714-1"><a href="predictive-modeling.html#cb714-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb714-2"><a href="predictive-modeling.html#cb714-2" aria-hidden="true" tabindex="-1"></a>Train_Data <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;Ames_Train_Data.csv&quot;</span>)  <span class="co"># Load data</span></span>
<span id="cb714-3"><a href="predictive-modeling.html#cb714-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)   <span class="co"># load caret package</span></span></code></pre></div>
<div class="sourceCode" id="cb715"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb715-1"><a href="predictive-modeling.html#cb715-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set cross-validation settings - use 10 repeats of 10-fold CV</span></span>
<span id="cb715-2"><a href="predictive-modeling.html#cb715-2" aria-hidden="true" tabindex="-1"></a>control <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method=</span><span class="st">&quot;repeatedcv&quot;</span>, <span class="at">number=</span><span class="dv">10</span>, <span class="at">repeats=</span><span class="dv">10</span>, <span class="at">savePredictions =</span> <span class="st">&quot;all&quot;</span> )</span>
<span id="cb715-3"><a href="predictive-modeling.html#cb715-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb715-4"><a href="predictive-modeling.html#cb715-4" aria-hidden="true" tabindex="-1"></a><span class="co"># define models</span></span>
<span id="cb715-5"><a href="predictive-modeling.html#cb715-5" aria-hidden="true" tabindex="-1"></a><span class="co"># set same random seed before each model to ensure same partitions are used in CV, making them comparable</span></span>
<span id="cb715-6"><a href="predictive-modeling.html#cb715-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb715-7"><a href="predictive-modeling.html#cb715-7" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">10302023</span>)   </span>
<span id="cb715-8"><a href="predictive-modeling.html#cb715-8" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">data=</span>Train_Data, </span>
<span id="cb715-9"><a href="predictive-modeling.html#cb715-9" aria-hidden="true" tabindex="-1"></a>                SalePrice <span class="sc">~</span> <span class="st">`</span><span class="at">Overall Qual</span><span class="st">`</span> ,  </span>
<span id="cb715-10"><a href="predictive-modeling.html#cb715-10" aria-hidden="true" tabindex="-1"></a>                <span class="at">method=</span><span class="st">&quot;lm&quot;</span>, <span class="at">trControl=</span>control)</span>
<span id="cb715-11"><a href="predictive-modeling.html#cb715-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb715-12"><a href="predictive-modeling.html#cb715-12" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">10302023</span>) </span>
<span id="cb715-13"><a href="predictive-modeling.html#cb715-13" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">data=</span>Train_Data, </span>
<span id="cb715-14"><a href="predictive-modeling.html#cb715-14" aria-hidden="true" tabindex="-1"></a>                SalePrice <span class="sc">~</span> <span class="st">`</span><span class="at">Overall Qual</span><span class="st">`</span> <span class="sc">+</span>  <span class="st">`</span><span class="at">Gr Liv Area</span><span class="st">`</span> <span class="sc">+</span> <span class="st">`</span><span class="at">Garage Area</span><span class="st">`</span>,  </span>
<span id="cb715-15"><a href="predictive-modeling.html#cb715-15" aria-hidden="true" tabindex="-1"></a>                <span class="at">method=</span><span class="st">&quot;lm&quot;</span>, <span class="at">trControl=</span>control)</span>
<span id="cb715-16"><a href="predictive-modeling.html#cb715-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb715-17"><a href="predictive-modeling.html#cb715-17" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">10302023</span>) </span>
<span id="cb715-18"><a href="predictive-modeling.html#cb715-18" aria-hidden="true" tabindex="-1"></a>model3 <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">data=</span>Train_Data, SalePrice <span class="sc">~</span> <span class="st">`</span><span class="at">Overall Qual</span><span class="st">`</span> <span class="sc">+</span> </span>
<span id="cb715-19"><a href="predictive-modeling.html#cb715-19" aria-hidden="true" tabindex="-1"></a>                  <span class="st">`</span><span class="at">Gr Liv Area</span><span class="st">`</span> <span class="sc">+</span> <span class="st">`</span><span class="at">Garage Area</span><span class="st">`</span> <span class="sc">+</span> </span>
<span id="cb715-20"><a href="predictive-modeling.html#cb715-20" aria-hidden="true" tabindex="-1"></a>                  <span class="st">`</span><span class="at">Neighborhood</span><span class="st">`</span> <span class="sc">+</span> <span class="st">`</span><span class="at">Bldg Type</span><span class="st">`</span>,  </span>
<span id="cb715-21"><a href="predictive-modeling.html#cb715-21" aria-hidden="true" tabindex="-1"></a>                <span class="at">method=</span><span class="st">&quot;lm&quot;</span>, <span class="at">trControl=</span>control)</span>
<span id="cb715-22"><a href="predictive-modeling.html#cb715-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb715-23"><a href="predictive-modeling.html#cb715-23" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">10302023</span>) </span>
<span id="cb715-24"><a href="predictive-modeling.html#cb715-24" aria-hidden="true" tabindex="-1"></a>model4 <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">data=</span>Train_Data, SalePrice <span class="sc">~</span> <span class="st">`</span><span class="at">Overall Qual</span><span class="st">`</span> </span>
<span id="cb715-25"><a href="predictive-modeling.html#cb715-25" aria-hidden="true" tabindex="-1"></a>                <span class="sc">+</span> <span class="st">`</span><span class="at">Gr Liv Area</span><span class="st">`</span>  <span class="sc">+</span> <span class="st">`</span><span class="at">Garage Area</span><span class="st">`</span> </span>
<span id="cb715-26"><a href="predictive-modeling.html#cb715-26" aria-hidden="true" tabindex="-1"></a>                <span class="sc">+</span> <span class="st">`</span><span class="at">Neighborhood</span><span class="st">`</span> <span class="sc">+</span> <span class="st">`</span><span class="at">Bldg Type</span><span class="st">`</span> <span class="sc">+</span> <span class="st">`</span><span class="at">Year Built</span><span class="st">`</span>,  </span>
<span id="cb715-27"><a href="predictive-modeling.html#cb715-27" aria-hidden="true" tabindex="-1"></a>                <span class="at">method=</span><span class="st">&quot;lm&quot;</span>, <span class="at">trControl=</span>control)</span>
<span id="cb715-28"><a href="predictive-modeling.html#cb715-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb715-29"><a href="predictive-modeling.html#cb715-29" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">10302023</span>) </span>
<span id="cb715-30"><a href="predictive-modeling.html#cb715-30" aria-hidden="true" tabindex="-1"></a>model5 <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">data=</span>Train_Data, SalePrice <span class="sc">~</span> <span class="st">`</span><span class="at">Overall Qual</span><span class="st">`</span> <span class="sc">+</span> </span>
<span id="cb715-31"><a href="predictive-modeling.html#cb715-31" aria-hidden="true" tabindex="-1"></a>                  <span class="st">`</span><span class="at">Gr Liv Area</span><span class="st">`</span> <span class="sc">+</span> <span class="st">`</span><span class="at">Garage Area</span><span class="st">`</span> <span class="sc">+</span> <span class="st">`</span><span class="at">Neighborhood</span><span class="st">`</span> <span class="sc">+</span> </span>
<span id="cb715-32"><a href="predictive-modeling.html#cb715-32" aria-hidden="true" tabindex="-1"></a>                  <span class="st">`</span><span class="at">Bldg Type</span><span class="st">`</span> <span class="sc">+</span> <span class="st">`</span><span class="at">Year Built</span><span class="st">`</span> <span class="sc">+</span> <span class="fu">I</span>(<span class="st">`</span><span class="at">Overall Qual</span><span class="st">`</span><span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb715-33"><a href="predictive-modeling.html#cb715-33" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">I</span>(<span class="st">`</span><span class="at">Gr Liv Area</span><span class="st">`</span><span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">I</span>(<span class="st">`</span><span class="at">Garage Area</span><span class="st">`</span><span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb715-34"><a href="predictive-modeling.html#cb715-34" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">I</span>(<span class="st">`</span><span class="at">Year Built</span><span class="st">`</span><span class="sc">^</span><span class="dv">2</span>),  <span class="at">method=</span><span class="st">&quot;lm&quot;</span>, <span class="at">trControl=</span>control)</span>
<span id="cb715-35"><a href="predictive-modeling.html#cb715-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb715-36"><a href="predictive-modeling.html#cb715-36" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">10302023</span>) </span>
<span id="cb715-37"><a href="predictive-modeling.html#cb715-37" aria-hidden="true" tabindex="-1"></a>model6 <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">data=</span>Train_Data, SalePrice <span class="sc">~</span> .,  <span class="at">method=</span><span class="st">&quot;lm&quot;</span>, <span class="at">trControl=</span>control)  <span class="co"># include everything linearly</span></span>
<span id="cb715-38"><a href="predictive-modeling.html#cb715-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb715-39"><a href="predictive-modeling.html#cb715-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb715-40"><a href="predictive-modeling.html#cb715-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate RMSPE for each model</span></span>
<span id="cb715-41"><a href="predictive-modeling.html#cb715-41" aria-hidden="true" tabindex="-1"></a>RMSPE1 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((model1<span class="sc">$</span>pred<span class="sc">$</span>obs<span class="sc">-</span>model1<span class="sc">$</span>pred<span class="sc">$</span>pred)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb715-42"><a href="predictive-modeling.html#cb715-42" aria-hidden="true" tabindex="-1"></a>RMSPE2 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((model2<span class="sc">$</span>pred<span class="sc">$</span>obs<span class="sc">-</span>model2<span class="sc">$</span>pred<span class="sc">$</span>pred)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb715-43"><a href="predictive-modeling.html#cb715-43" aria-hidden="true" tabindex="-1"></a>RMSPE3 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((model3<span class="sc">$</span>pred<span class="sc">$</span>obs<span class="sc">-</span>model3<span class="sc">$</span>pred<span class="sc">$</span>pred)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb715-44"><a href="predictive-modeling.html#cb715-44" aria-hidden="true" tabindex="-1"></a>RMSPE4 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((model4<span class="sc">$</span>pred<span class="sc">$</span>obs<span class="sc">-</span>model4<span class="sc">$</span>pred<span class="sc">$</span>pred)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb715-45"><a href="predictive-modeling.html#cb715-45" aria-hidden="true" tabindex="-1"></a>RMSPE5 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((model5<span class="sc">$</span>pred<span class="sc">$</span>obs<span class="sc">-</span>model5<span class="sc">$</span>pred<span class="sc">$</span>pred)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb715-46"><a href="predictive-modeling.html#cb715-46" aria-hidden="true" tabindex="-1"></a>RMSPE6 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((model6<span class="sc">$</span>pred<span class="sc">$</span>obs<span class="sc">-</span>model6<span class="sc">$</span>pred<span class="sc">$</span>pred)<span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<div class="sourceCode" id="cb716"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb716-1"><a href="predictive-modeling.html#cb716-1" aria-hidden="true" tabindex="-1"></a>RMSPE1</span></code></pre></div>
<pre><code>## [1] 51710.58</code></pre>
<div class="sourceCode" id="cb718"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb718-1"><a href="predictive-modeling.html#cb718-1" aria-hidden="true" tabindex="-1"></a>RMSPE2</span></code></pre></div>
<pre><code>## [1] 44192.34</code></pre>
<div class="sourceCode" id="cb720"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb720-1"><a href="predictive-modeling.html#cb720-1" aria-hidden="true" tabindex="-1"></a>RMSPE3</span></code></pre></div>
<pre><code>## [1] 38212.96</code></pre>
<div class="sourceCode" id="cb722"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb722-1"><a href="predictive-modeling.html#cb722-1" aria-hidden="true" tabindex="-1"></a>RMSPE4</span></code></pre></div>
<pre><code>## [1] 38016.67</code></pre>
<div class="sourceCode" id="cb724"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb724-1"><a href="predictive-modeling.html#cb724-1" aria-hidden="true" tabindex="-1"></a>RMSPE5</span></code></pre></div>
<pre><code>## [1] 38245.46</code></pre>
<div class="sourceCode" id="cb726"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb726-1"><a href="predictive-modeling.html#cb726-1" aria-hidden="true" tabindex="-1"></a>RMSPE6</span></code></pre></div>
<pre><code>## [1] 40586.14</code></pre>
<p>We see that in this case, model M4 performed the best on the hold-out data. We should use Model 4 to make predictions on new data over the other models seen here. It is likely that there are better models out there than model 4, likely with complexity somewhere between that of model 4 and models 5 and 6. Perhaps you can find one.</p>
<p>Once we have our preferred model, we can read in our test data and make predictions, and create a csv file containing the predictions, using the code below.</p>
<div class="sourceCode" id="cb728"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb728-1"><a href="predictive-modeling.html#cb728-1" aria-hidden="true" tabindex="-1"></a>TestData <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;Ames_Test_Data.csv&quot;</span>)</span>
<span id="cb728-2"><a href="predictive-modeling.html#cb728-2" aria-hidden="true" tabindex="-1"></a>predictions <span class="ot">&lt;-</span> <span class="fu">predict</span>(model1, <span class="at">newdata=</span>TestData)  <span class="co"># substitute your best model</span></span>
<span id="cb728-3"><a href="predictive-modeling.html#cb728-3" aria-hidden="true" tabindex="-1"></a><span class="fu">write.csv</span>(predictions, <span class="at">file =</span> <span class="st">&quot;predictions.csv&quot;</span>)</span></code></pre></div>
</div>
</div>
<div id="ridge-regression" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Ridge Regression<a href="predictive-modeling.html#ridge-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="complexity-in-model-coefficients" class="section level3 hasAnchor" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> Complexity in Model Coefficients<a href="predictive-modeling.html#complexity-in-model-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Weâ€™ve thought about complexity in terms of the number of terms we include in a model, as well as whether we include quadratic terms and higher order terms and interactions. We can also think about model complexity in terms of the coefficients <span class="math inline">\(b_1, \ldots, b_p\)</span>. Larger values of <span class="math inline">\(b_1, \ldots, b_p\)</span> are associated with more complex models. Smaller values of <span class="math inline">\(b_1, \ldots, b_p\)</span> are associated with less complex models. When <span class="math inline">\(b_j=0\)</span>, this mean variable <span class="math inline">\(j\)</span> is not used in the model.</p>
<p>To illustrate, we fit a regression model to the Ames housing dataset, which includes 71 possible explanatory variables, in addition to price.</p>
<div class="sourceCode" id="cb729"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb729-1"><a href="predictive-modeling.html#cb729-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">10302021</span>)</span>
<span id="cb729-2"><a href="predictive-modeling.html#cb729-2" aria-hidden="true" tabindex="-1"></a>samp <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(ames_raw), <span class="dv">1000</span>)</span>
<span id="cb729-3"><a href="predictive-modeling.html#cb729-3" aria-hidden="true" tabindex="-1"></a>Train_Data <span class="ot">&lt;-</span> ames_raw[samp,]</span></code></pre></div>
<p>The full list of coefficient estimates is shown below.</p>
<div class="sourceCode" id="cb730"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb730-1"><a href="predictive-modeling.html#cb730-1" aria-hidden="true" tabindex="-1"></a>M_OLS <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data=</span>Train_Data, SalePrice <span class="sc">~</span> .)</span>
<span id="cb730-2"><a href="predictive-modeling.html#cb730-2" aria-hidden="true" tabindex="-1"></a>M_OLS<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##             (Intercept)          `Overall Qual`            `Year Built` 
##       -14136063.8944779            6975.9249580             494.5819845 
##          `Mas Vnr Area`          `Central Air`Y           `Gr Liv Area` 
##              33.3816934           -3745.3629147              37.2290416 
##          `Lot Frontage`            `1st Flr SF`         `Bedroom AbvGr` 
##             -14.9594204              13.3690338           -2353.0379012 
##         `TotRms AbvGrd`                   Order                     PID 
##             914.5594852               9.0730317               0.7938731 
##        `MS SubClass`030        `MS SubClass`040        `MS SubClass`045 
##            1747.3877587            5867.2885074            7182.7066393 
##        `MS SubClass`050        `MS SubClass`060        `MS SubClass`070 
##           -1330.0097462           -7400.8589386            -953.6999925 
##        `MS SubClass`075        `MS SubClass`080        `MS SubClass`085 
##          -10585.7402995           -6516.1314579           -6527.2337681 
##        `MS SubClass`090        `MS SubClass`120        `MS SubClass`160 
##          -21891.2260114          -20296.4699853          -34837.6663542 
##        `MS SubClass`180        `MS SubClass`190      `MS Zoning`C (all) 
##          -18822.1874094          -12421.4283083          -38785.6865317 
##           `MS Zoning`FV      `MS Zoning`I (all)           `MS Zoning`RH 
##          -23114.7182938          -17213.3331514          -13306.2219908 
##           `MS Zoning`RL           `MS Zoning`RM              `Lot Area` 
##          -17643.5478889          -24783.4465053               0.7407590 
##              StreetPave               AlleyPave                 AlleyNA 
##           37262.6393526            2527.1840221              29.0268338 
##          `Lot Shape`IR2          `Lot Shape`IR3          `Lot Shape`Reg 
##            8575.5874307           10397.8001841            2676.2691253 
##       `Land Contour`HLS       `Land Contour`Low       `Land Contour`Lvl 
##           11318.1617440          -22452.1871122           12230.7109977 
##     `Lot Config`CulDSac         `Lot Config`FR2         `Lot Config`FR3 
##           10427.6521788          -12317.5423512          -14557.2814204 
##      `Lot Config`Inside         `Land Slope`Mod         `Land Slope`Sev 
##           -1168.2308323           10550.3766984          -24213.7593573 
##     NeighborhoodBlueste      NeighborhoodBrDale     NeighborhoodBrkSide 
##           19001.1401867           20558.0541049           14314.5790198 
##     NeighborhoodClearCr     NeighborhoodCollgCr     NeighborhoodCrawfor 
##            7379.3465594             -46.6307558           28775.4384686 
##     NeighborhoodEdwards     NeighborhoodGilbert      NeighborhoodGreens 
##           -7491.2919144            1951.6205024            4667.7592928 
##      NeighborhoodIDOTRR     NeighborhoodMeadowV     NeighborhoodMitchel 
##           12473.1492830           18891.1264802           -9274.3206260 
##       NeighborhoodNAmes     NeighborhoodNoRidge     NeighborhoodNPkVill 
##            1781.9098984           32109.4882191           19601.4137971 
##     NeighborhoodNridgHt      NeighborhoodNWAmes     NeighborhoodOldTown 
##           34407.2004203           -3367.8475918            9857.0794600 
##      NeighborhoodSawyer     NeighborhoodSawyerW     NeighborhoodSomerst 
##            6761.6510470            3945.8508858           20453.1722509 
##     NeighborhoodStoneBr       NeighborhoodSWISU      NeighborhoodTimber 
##           44556.0799048            8518.9310629            1455.9163924 
##     NeighborhoodVeenker      `Condition 1`Feedr       `Condition 1`Norm 
##            5759.5517197            -315.5372848           10041.9749570 
##       `Condition 1`PosA       `Condition 1`PosN       `Condition 1`RRAe 
##           49498.8746234           15873.9313786          -11599.3284625 
##       `Condition 1`RRAn       `Condition 1`RRNn      `Condition 2`Feedr 
##            7894.0460905            6045.1253614            7639.7933280 
##       `Condition 2`Norm       `Condition 2`PosA       `Condition 2`PosN 
##            6804.6578647            1496.1296933         -224695.9129765 
##       `Condition 2`RRNn          `Overall Cond`        `Year Remod/Add` 
##           20826.2553462            5652.4500122             115.1029835 
##       `Roof Style`Gable     `Roof Style`Gambrel         `Roof Style`Hip 
##            -983.4965913           -3775.4699187           -1020.9768277 
##     `Roof Style`Mansard        `Roof Style`Shed      `Roof Matl`CompShg 
##           18711.8071355           -9552.0308918          671041.0670584 
##      `Roof Matl`Membran      `Roof Matl`Tar&amp;Grv      `Roof Matl`WdShngl 
##          738250.2085208          653237.7170668          757954.9462501 
##   `Mas Vnr Type`BrkFace      `Mas Vnr Type`None     `Mas Vnr Type`Stone 
##          -14297.1433973           -5178.2753573          -11764.3736400 
##          `Exter Qual`Fa          `Exter Qual`Gd          `Exter Qual`TA 
##          -21903.6214492          -37435.0319282          -40396.2768748 
##          `Exter Cond`Fa          `Exter Cond`Gd          `Exter Cond`Po 
##           -2499.1726078            9625.5181637           -9090.3355705 
##          `Exter Cond`TA        FoundationCBlock         FoundationPConc 
##           10976.5039644            2136.6596942            4341.3771422 
##          FoundationSlab         FoundationStone          FoundationWood 
##           -8011.2100540            5209.4079083          -21499.8677282 
##           `Bsmt Qual`Fa           `Bsmt Qual`Gd           `Bsmt Qual`Po 
##          -19118.1141774          -13338.1127108           58476.8071349 
##           `Bsmt Qual`TA           `Bsmt Qual`NA       `Bsmt Exposure`Gd 
##          -14904.6825425           29237.6694620            8445.8558434 
##       `Bsmt Exposure`Mn       `Bsmt Exposure`No       `Bsmt Exposure`NA 
##           -5636.1528491           -5726.8655933          -28286.6797423 
##          `BsmtFin SF 1`          `BsmtFin SF 2`           `Bsmt Unf SF` 
##              33.8127701              24.9524651              13.2017599 
##             HeatingGasW             HeatingWall          `Heating QC`Fa 
##            3554.5996405           16477.8270033           -7277.0668485 
##          `Heating QC`Gd          `Heating QC`Po          `Heating QC`TA 
##            -760.5324990          -17982.8274261           -5710.2836368 
##         ElectricalFuseF         ElectricalFuseP           ElectricalMix 
##             728.4351710           25345.9011297           51715.0815893 
##         ElectricalSBrkr            `2nd Flr SF`        `Bsmt Full Bath` 
##           -2872.3538220              12.4886691            -498.4665953 
##        `Bsmt Half Bath`             `Full Bath`             `Half Bath` 
##            3026.8853409            3781.8172133            3333.8342115 
##         `Kitchen AbvGr`        `Kitchen Qual`Fa        `Kitchen Qual`Gd 
##          -12956.8475513           -7789.8981158          -13554.1557391 
##        `Kitchen Qual`TA          FunctionalMaj2          FunctionalMin1 
##          -13298.5375890          -24570.4189413           -8664.3164291 
##          FunctionalMin2           FunctionalMod           FunctionalTyp 
##          -12709.0598893          -14394.7176699            4206.1036385 
##              Fireplaces        `Fireplace Qu`Fa        `Fireplace Qu`Gd 
##           11467.2013154          -12766.1232883          -14204.2309228 
##        `Fireplace Qu`Po        `Fireplace Qu`TA        `Fireplace Qu`NA 
##          -23140.9430388          -17684.2617792           -5555.0116338 
##     `Garage Type`Attchd    `Garage Type`Basment    `Garage Type`BuiltIn 
##           -1098.5463523            -698.4291295           -4596.8780051 
##    `Garage Type`CarPort     `Garage Type`Detchd         `Garage Yr Blt` 
##          -10478.5627774            -138.9775476             -61.6633788 
##      `Garage Finish`RFn      `Garage Finish`Unf           `Garage Cars` 
##           -4343.2684170           -1482.5096317            2990.4177680 
##           `Garage Area`         `Garage Qual`Fa         `Garage Qual`Gd 
##              24.3013657          -70420.9028312          -51114.4969380 
##         `Garage Qual`Po         `Garage Qual`TA         `Garage Cond`Fa 
##         -134052.9764560          -67768.2607876           73986.1047213 
##         `Garage Cond`Gd         `Garage Cond`Po         `Garage Cond`TA 
##           64104.4268776          112712.4021077           71259.2488661 
##          `Paved Drive`P          `Paved Drive`Y          `Wood Deck SF` 
##            5257.0317668            2077.1863821               7.6414867 
##         `Open Porch SF`        `Enclosed Porch`            `3Ssn Porch` 
##             -16.2882422             -11.3738346              16.1217441 
##          `Screen Porch`             `Pool Area`             `Pool QC`TA 
##              38.2573038            -106.8682816           24551.0371793 
##             `Pool QC`NA               FenceGdWo              FenceMnPrv 
##         -100661.2221692               3.7225602            1567.2382267 
##               FenceMnWw                 FenceNA      `Misc Feature`Gar2 
##            -880.4035636            1297.2148680          545161.5427966 
##      `Misc Feature`Othr      `Misc Feature`Shed        `Misc Feature`NA 
##          569599.2024080          551469.2569906          547892.2454607 
##              `Misc Val`               `Mo Sold`               `Yr Sold` 
##               1.6361319            -262.1972861            5937.2902795 
##          `Sale Type`Con        `Sale Type`ConLD        `Sale Type`ConLI 
##            7985.9754307           23972.1096774            9525.1931028 
##        `Sale Type`ConLw          `Sale Type`CWD          `Sale Type`New 
##           11056.3865076           -9551.0936247           23118.1311002 
##          `Sale Type`Oth          `Sale Type`VWD          `Sale Type`WD  
##           39733.0338449          -11729.6976216            4819.9397983 
## `Sale Condition`AdjLand  `Sale Condition`Alloca  `Sale Condition`Family 
##           41276.9513339           24461.6278440             404.8499384 
##  `Sale Condition`Normal `Sale Condition`Partial 
##            1592.2625631           -4641.7581345</code></pre>
<p>Letâ€™s focus on the first 10 rows.</p>
<div class="sourceCode" id="cb732"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb732-1"><a href="predictive-modeling.html#cb732-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">coef</span>(M_OLS),<span class="dv">10</span>) <span class="sc">%&gt;%</span> <span class="fu">round</span>(<span class="dv">3</span>)</span></code></pre></div>
<pre><code>##     (Intercept)  `Overall Qual`    `Year Built`  `Mas Vnr Area`  `Central Air`Y 
##   -14136063.894        6975.925         494.582          33.382       -3745.363 
##   `Gr Liv Area`  `Lot Frontage`    `1st Flr SF` `Bedroom AbvGr` `TotRms AbvGrd` 
##          37.229         -14.959          13.369       -2353.038         914.559</code></pre>
<p>If all coefficients in the model were 0, then we would be using the most simple constant model, and the prediction for the price of each house would be exactly the same as the overall mean. As <span class="math inline">\(b_j&#39;s\)</span> get farther from 0, predictions begin move away from the overall mean and depend more and more on the values or categories of the explanatory variable(s) associated with individual houses. This creates a risk, however, of overfitting.</p>
<p>A way to combat this, other than dropping variables from the model, is to shrink some or all of the regression coefficients closer to 0, pushing predictions closer to the overall mean.</p>
<p>A statistical technique for doing this is called <strong>ridge regression.</strong></p>
</div>
<div id="ridge-regression-penalty" class="section level3 hasAnchor" number="7.3.2">
<h3><span class="header-section-number">7.3.2</span> Ridge Regression Penalty<a href="predictive-modeling.html#ridge-regression-penalty" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Weâ€™ve seen that in ordinary least-squares regression, <span class="math inline">\(b_0, b_1, \ldots, b_p\)</span> are chosen in a way that to minimizes</p>
<p><span class="math display">\[ \displaystyle\sum_{i=1}^n (y_i -\hat{y}_i)^2  = \displaystyle\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2{x_i2} + \ldots +b_px_{ip}))^2 \]</span></p>
<p>When <span class="math inline">\(p\)</span> is large and we want to be careful of overfitting, a common approach is to add a â€œpenalty termâ€ to this function, to incentive choosing values of <span class="math inline">\(b_1, \ldots, b_p\)</span> that are closer to 0, thereby â€œshrinkingâ€ the predictions toward the overall mean house price.</p>
<p>Specifically, we minimize:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \displaystyle\sum_{i=1}^n (y_i -\hat{y}_i)^2  + \lambda\displaystyle\sum_{j=1}^pb_j^2\\ =  &amp; \displaystyle\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \ldots + b_px_{ip}))^2 + \lambda\displaystyle\sum_{j=1}^pb_j^2
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is a pre-determined positive constant.</p>
<p>Larger values of <span class="math inline">\(b_j\)</span> typically help the model better fit the training data, thereby making the first term smaller, but also make the second term larger. The idea is the find optimal values of <span class="math inline">\(b_0, b_1, \ldots, b_p\)</span> that are large enough to allow the model to fit the data well, thus keeping the first term (SSR) small, while also keeping the penalty term small as well.</p>
</div>
<div id="choosing-lambda" class="section level3 hasAnchor" number="7.3.3">
<h3><span class="header-section-number">7.3.3</span> Choosing <span class="math inline">\(\lambda\)</span><a href="predictive-modeling.html#choosing-lambda" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The value of <span class="math inline">\(\lambda\)</span> is predetermined by the user. The larger the value of <span class="math inline">\(\lambda\)</span>, the more heavily large <span class="math inline">\(b_j&#39;s\)</span> are penalized. A value of <span class="math inline">\(\lambda=0\)</span> corresponds to ordinary least-squares.</p>
<p><span class="math display">\[
\begin{aligned}
Q=&amp; \displaystyle\sum_{i=1}^n (y_i -\hat{y}_i)^2  + \lambda\displaystyle\sum_{j=1}^pb_j^2\\ =  &amp; \displaystyle\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \ldots + b_px_{ip}))^2 + \lambda\displaystyle\sum_{j=1}^pb_j^2
\end{aligned}
\]</span></p>
<ul>
<li>Small values of <span class="math inline">\(\lambda\)</span> lead to more complex models, with larger <span class="math inline">\(|b_j|\)</span>â€™s.<br />
</li>
<li>As <span class="math inline">\(\lambda\)</span> increases, <span class="math inline">\(|b_j|\)</span>â€™s shrink toward 0. The model becomes less complex, thus bias increases, but variance decreases.<br />
</li>
<li>We can use cross validation to determine the optimal value of <span class="math inline">\(\lambda\)</span></li>
</ul>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-556-1.png" width="960" /></p>
<p>When using ridge regression, it is important to standardize each explanatory variable (i.e.Â subtract the mean and divide by the standard deviation). This ensures each variable has mean 0 and standard deviation 1. Without standardizing the optimal choice of <span class="math inline">\(b_j\)</span>â€™s would depend on scale, with variables with larger absolute measurements having more influence. Weâ€™ll standardize the response variable too. Though this is not strictly necessary, it doesnâ€™t hurt. We can always transform back if necessary.</p>
<p>Standardization is performed using the <code>scale</code> command in R.</p>
<div class="sourceCode" id="cb734"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb734-1"><a href="predictive-modeling.html#cb734-1" aria-hidden="true" tabindex="-1"></a>Train_sc <span class="ot">&lt;-</span> Train_Data <span class="sc">%&gt;%</span> <span class="fu">mutate_if</span>(is.numeric, scale)</span></code></pre></div>
</div>
<div id="ridge-regression-on-housing-dataset" class="section level3 hasAnchor" number="7.3.4">
<h3><span class="header-section-number">7.3.4</span> Ridge Regression on Housing Dataset<a href="predictive-modeling.html#ridge-regression-on-housing-dataset" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Weâ€™ll use the <code>caret</code> package to perform cross validation in order to find the optimal value of <span class="math inline">\(\lambda\)</span>. To use ridge regression, we specify <code>method = "glmnet"</code>, and <code>tuneGrid=expand.grid(alpha=0, lambda=l_vals)</code>. Note the <code>alpha</code> value can be changed to use other types of penalized regression sometimes used in predictive modeling, such as lasso or elastic net.</p>
<div class="sourceCode" id="cb735"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb735-1"><a href="predictive-modeling.html#cb735-1" aria-hidden="true" tabindex="-1"></a>control <span class="ot">=</span> <span class="fu">trainControl</span>(<span class="st">&quot;repeatedcv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>, <span class="at">repeats=</span><span class="dv">10</span>)</span>
<span id="cb735-2"><a href="predictive-modeling.html#cb735-2" aria-hidden="true" tabindex="-1"></a>l_vals <span class="ot">=</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length =</span> <span class="dv">100</span>)  <span class="co"># test values between 1/1000 and 1000</span></span>
<span id="cb735-3"><a href="predictive-modeling.html#cb735-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb735-4"><a href="predictive-modeling.html#cb735-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">11162020</span>)</span>
<span id="cb735-5"><a href="predictive-modeling.html#cb735-5" aria-hidden="true" tabindex="-1"></a>Housing_ridge <span class="ot">&lt;-</span> <span class="fu">train</span>(SalePrice <span class="sc">~</span> .,</span>
<span id="cb735-6"><a href="predictive-modeling.html#cb735-6" aria-hidden="true" tabindex="-1"></a>                       <span class="at">data =</span> Train_sc, <span class="at">method =</span> <span class="st">&quot;glmnet&quot;</span>, <span class="at">trControl=</span>control , </span>
<span id="cb735-7"><a href="predictive-modeling.html#cb735-7" aria-hidden="true" tabindex="-1"></a>                      <span class="at">tuneGrid=</span><span class="fu">expand.grid</span>(<span class="at">alpha=</span><span class="dv">0</span>, <span class="at">lambda=</span>l_vals))</span></code></pre></div>
<p>Value of <span class="math inline">\(\lambda\)</span> minimizing RMSPE:</p>
<div class="sourceCode" id="cb736"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb736-1"><a href="predictive-modeling.html#cb736-1" aria-hidden="true" tabindex="-1"></a>Housing_ridge<span class="sc">$</span>bestTune<span class="sc">$</span>lambda</span></code></pre></div>
<pre><code>## [1] 0.6135907</code></pre>
<p>We examine RMSPE on the withheld data as a function of <span class="math inline">\(\lambda\)</span>.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-560-1.png" width="672" /></p>
<p>Using <span class="math inline">\(\lambda\)</span> = 0.6135907, obtain the following set of ridge regression coefficients. Notice how the ridge coefficients are typically closer to 0 than the ordinary least squares coefficients, indicating a less complex model.</p>
<div class="sourceCode" id="cb738"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb738-1"><a href="predictive-modeling.html#cb738-1" aria-hidden="true" tabindex="-1"></a>M_OLS_sc <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data=</span>Train_sc, SalePrice <span class="sc">~</span> .)</span>
<span id="cb738-2"><a href="predictive-modeling.html#cb738-2" aria-hidden="true" tabindex="-1"></a>OLS_coef <span class="ot">&lt;-</span> M_OLS_sc<span class="sc">$</span>coefficients</span>
<span id="cb738-3"><a href="predictive-modeling.html#cb738-3" aria-hidden="true" tabindex="-1"></a>Ridge_coef <span class="ot">&lt;-</span> <span class="fu">coef</span>(Housing_ridge<span class="sc">$</span>finalModel, Housing_ridge<span class="sc">$</span>bestTune<span class="sc">$</span>lambda)[,<span class="dv">1</span>]</span>
<span id="cb738-4"><a href="predictive-modeling.html#cb738-4" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(OLS_coef[<span class="dv">2</span><span class="sc">:</span><span class="dv">10</span>], Ridge_coef[<span class="dv">2</span><span class="sc">:</span><span class="dv">10</span>])</span>
<span id="cb738-5"><a href="predictive-modeling.html#cb738-5" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(df) <span class="ot">&lt;-</span><span class="fu">c</span>(<span class="st">&quot;OLS Coeff&quot;</span>, <span class="st">&quot;Ridge Coeff&quot;</span>)</span>
<span id="cb738-6"><a href="predictive-modeling.html#cb738-6" aria-hidden="true" tabindex="-1"></a>df</span></code></pre></div>
<pre><code>##                    OLS Coeff Ridge Coeff
## `Overall Qual`   0.121728754  0.10435284
## `Year Built`     0.187102422  0.03451303
## `Mas Vnr Area`   0.080212607  0.06202880
## `Central Air`Y  -0.046191694  0.04289126
## `Gr Liv Area`    0.237623291  0.00000000
## `Lot Frontage`  -0.004290945  0.07967743
## `1st Flr SF`     0.069910650  0.01020597
## `Bedroom AbvGr` -0.022457937  0.07194208
## `TotRms AbvGrd`  0.017574153  0.01342224</code></pre>
<p>Predictions and residuals for the first six houses in the traning data, using ordinary least squares and ridge regression, are shown below.</p>
<div class="sourceCode" id="cb740"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb740-1"><a href="predictive-modeling.html#cb740-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb740-2"><a href="predictive-modeling.html#cb740-2" aria-hidden="true" tabindex="-1"></a>MAT <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(SalePrice<span class="sc">~</span>., <span class="at">data=</span>Train_sc)</span>
<span id="cb740-3"><a href="predictive-modeling.html#cb740-3" aria-hidden="true" tabindex="-1"></a>ridge_mod <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(<span class="at">x=</span>MAT, <span class="at">y=</span>Train_sc<span class="sc">$</span>SalePrice, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda=</span>Housing_ridge<span class="sc">$</span>bestTune<span class="sc">$</span>lambda )</span></code></pre></div>
<div class="sourceCode" id="cb741"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb741-1"><a href="predictive-modeling.html#cb741-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> Train_sc<span class="sc">$</span>SalePrice</span>
<span id="cb741-2"><a href="predictive-modeling.html#cb741-2" aria-hidden="true" tabindex="-1"></a>Pred_OLS <span class="ot">&lt;-</span> <span class="fu">predict</span>(M_OLS_sc)</span>
<span id="cb741-3"><a href="predictive-modeling.html#cb741-3" aria-hidden="true" tabindex="-1"></a>Pred_Ridge <span class="ot">&lt;-</span> <span class="fu">predict</span>(ridge_mod, <span class="at">newx=</span>MAT)</span>
<span id="cb741-4"><a href="predictive-modeling.html#cb741-4" aria-hidden="true" tabindex="-1"></a>OLS_Resid <span class="ot">&lt;-</span> y <span class="sc">-</span> Pred_OLS</span>
<span id="cb741-5"><a href="predictive-modeling.html#cb741-5" aria-hidden="true" tabindex="-1"></a>Ridge_Resid <span class="ot">&lt;-</span> y <span class="sc">-</span> Pred_Ridge</span>
<span id="cb741-6"><a href="predictive-modeling.html#cb741-6" aria-hidden="true" tabindex="-1"></a>Resdf <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, Pred_OLS, Pred_Ridge, OLS_Resid, Ridge_Resid)</span>
<span id="cb741-7"><a href="predictive-modeling.html#cb741-7" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(Resdf) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;y&quot;</span>, <span class="st">&quot;OLS Pred&quot;</span>, <span class="st">&quot;Ridge Pred&quot;</span>, <span class="st">&quot;OLS Resid&quot;</span>, <span class="st">&quot;Ridge Resid&quot;</span>)</span>
<span id="cb741-8"><a href="predictive-modeling.html#cb741-8" aria-hidden="true" tabindex="-1"></a><span class="fu">kable</span>(<span class="fu">head</span>(Resdf))</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">y</th>
<th align="right">OLS Pred</th>
<th align="right">Ridge Pred</th>
<th align="right">OLS Resid</th>
<th align="right">Ridge Resid</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">859</td>
<td align="right">-0.6210832</td>
<td align="right">-0.4637429</td>
<td align="right">-0.4651589</td>
<td align="right">-0.1573403</td>
<td align="right">-0.1559243</td>
</tr>
<tr class="even">
<td align="left">1850</td>
<td align="right">0.6800520</td>
<td align="right">1.1897467</td>
<td align="right">1.0528536</td>
<td align="right">-0.5096947</td>
<td align="right">-0.3728016</td>
</tr>
<tr class="odd">
<td align="left">1301</td>
<td align="right">-0.4545873</td>
<td align="right">-0.4527781</td>
<td align="right">-0.4958630</td>
<td align="right">-0.0018092</td>
<td align="right">0.0412758</td>
</tr>
<tr class="even">
<td align="left">981</td>
<td align="right">-0.6408161</td>
<td align="right">-0.6626212</td>
<td align="right">-0.7711186</td>
<td align="right">0.0218051</td>
<td align="right">0.1303025</td>
</tr>
<tr class="odd">
<td align="left">2694</td>
<td align="right">-0.7937457</td>
<td align="right">-0.8679455</td>
<td align="right">-0.7543093</td>
<td align="right">0.0741997</td>
<td align="right">-0.0394365</td>
</tr>
<tr class="even">
<td align="left">2209</td>
<td align="right">-0.7906625</td>
<td align="right">-0.6955254</td>
<td align="right">-0.6449779</td>
<td align="right">-0.0951370</td>
<td align="right">-0.1456845</td>
</tr>
</tbody>
</table>
</div>
<div id="ridge-vs-ols" class="section level3 hasAnchor" number="7.3.5">
<h3><span class="header-section-number">7.3.5</span> Ridge vs OLS<a href="predictive-modeling.html#ridge-vs-ols" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In OLS, we choose <span class="math inline">\(b_0, b_1, \ldots, b_p\)</span> are chosen in a way that minimizes</p>
<p><span class="math display">\[ \displaystyle\sum_{i=1}^n (y_i -\hat{y}_i)^2  = \displaystyle\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \ldots + b_px_{ip}))^2 \]</span></p>
<p>OLS: <span class="math inline">\(\displaystyle\sum_{i=1}^n (y_i -\hat{y}_i)^2\)</span></p>
<div class="sourceCode" id="cb742"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb742-1"><a href="predictive-modeling.html#cb742-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>((y<span class="sc">-</span>Pred_OLS)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 56.94383</code></pre>
<p>Ridge: <span class="math inline">\(\displaystyle\sum_{i=1}^n (y_i -\hat{y}_i)^2\)</span></p>
<div class="sourceCode" id="cb744"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb744-1"><a href="predictive-modeling.html#cb744-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>((y<span class="sc">-</span>Pred_Ridge)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 127.1331</code></pre>
<p>Not surprisingly the OLS model achieves smaller <span class="math inline">\(\displaystyle\sum_{i=1}^n (y_i -\hat{y}_i)^2\)</span>. This has to be true, since the OLS coefficients are chosen to minimize this quantity.</p>
<p>In ridge regression, <span class="math inline">\(b_0, b_1, \ldots, b_p\)</span> are chosen in a way that minimizes</p>
<p><span class="math display">\[
\begin{aligned}
Q=&amp; \displaystyle\sum_{i=1}^n (y_i -\hat{y}_i)^2  + \lambda\displaystyle\sum_{j=1}^pb_j^2\\ =  &amp; \displaystyle\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \ldots + b_px_{ip}))^2 + \lambda\displaystyle\sum_{j=1}^pb_j^2
\end{aligned}
\]</span></p>
<p>OLS: <span class="math inline">\(\displaystyle\sum_{i=1}^n (y_i -\hat{y}_i)^2 + \lambda\displaystyle\sum_{j=1}^pb_j^2\)</span></p>
<div class="sourceCode" id="cb746"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb746-1"><a href="predictive-modeling.html#cb746-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>((y<span class="sc">-</span>Pred_OLS)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fl">0.6136</span><span class="sc">*</span><span class="fu">sum</span>(<span class="fu">coef</span>(M_OLS_sc)[<span class="sc">-</span><span class="dv">1</span>]<span class="sc">^</span><span class="dv">2</span>) </span></code></pre></div>
<pre><code>## [1] 373.1205</code></pre>
<p>Ridge: <span class="math inline">\(\displaystyle\sum_{i=1}^n (y_i -\hat{y}_i)^2 + \lambda\displaystyle\sum_{j=1}^pb_j^2\)</span></p>
<div class="sourceCode" id="cb748"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb748-1"><a href="predictive-modeling.html#cb748-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>((y<span class="sc">-</span>Pred_Ridge)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fl">0.6136</span><span class="sc">*</span><span class="fu">sum</span>((Ridge_coef)[<span class="sc">-</span><span class="dv">1</span>]<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 130.3375</code></pre>
<p>We see that the ridge coefficients achieve a lower value of Q than the OLS ones.</p>
</div>
<div id="lasso-and-elastic-net" class="section level3 hasAnchor" number="7.3.6">
<h3><span class="header-section-number">7.3.6</span> Lasso and Elastic Net<a href="predictive-modeling.html#lasso-and-elastic-net" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Two other techniques that are similar to ridge regression are lasso and elastic net. Both also aim to avoid overfitting by shrinking regression coefficients toward 0 in a manner similar to ridge regression.</p>
<p>Lasso regression is very similar to ridge regression. Coefficients <span class="math inline">\(b_0, b_1, \ldots, b_p\)</span> are chosen in a way that to minimizes</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \displaystyle\sum_{i=1}^n (y_i -\hat{y}_i)^2  + \lambda\displaystyle\sum_{j=1}^p|b_j|\\ =  &amp; \displaystyle\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \ldots + b_px_{ip}))^2 + \lambda\displaystyle\sum_{j=1}^p|b_j|
\end{aligned}
\]</span>
Regression with an elastic net uses both ridge and lasso penalty terms and determines the values of <span class="math inline">\(b_0, b_1, \ldots, b_p\)</span> by minimizing</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \displaystyle\sum_{i=1}^n (y_i -\hat{y}_i)^2  + \lambda\displaystyle\sum_{j=1}^p|b_j|\\ =  &amp; \displaystyle\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \ldots + b_px_{ip}))^2 + \lambda_1\displaystyle\sum_{j=1}^pb_j^2+ \lambda_2\displaystyle\sum_{j=1}^p|b_j|
\end{aligned}
\]</span></p>
</div>
</div>
<div id="decision-trees" class="section level2 hasAnchor" number="7.4">
<h2><span class="header-section-number">7.4</span> Decision Trees<a href="predictive-modeling.html#decision-trees" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="basics-of-decision-trees" class="section level3 hasAnchor" number="7.4.1">
<h3><span class="header-section-number">7.4.1</span> Basics of Decision Trees<a href="predictive-modeling.html#basics-of-decision-trees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A decision tree is a flexible alternative to a regression model. It is said to be <strong>nonparametric</strong> because it does not involve parameters like <span class="math inline">\(\beta_0, \beta_1, \ldots \beta_p\)</span>. A tree makes no assumption about the nature of the relationship between the response and explanatory variables, and instead allows us to learn this relationship from the data. A tree makes prediction by repeatedly grouping together like observations in the training data. We can make predictions for a new case, by tracing it through the tree, and averaging responses of training cases in the same terminal node.</p>
<p><strong>Decision Tree Example:</strong></p>
<p>We fit a decision tree to the Ames Housing dataset, using the <code>rpart</code> function in a package by the same name.</p>
<div class="sourceCode" id="cb750"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb750-1"><a href="predictive-modeling.html#cb750-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb750-2"><a href="predictive-modeling.html#cb750-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb750-3"><a href="predictive-modeling.html#cb750-3" aria-hidden="true" tabindex="-1"></a>tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(SalePrice<span class="sc">~</span>., <span class="at">data=</span>Train_Data, <span class="at">cp=</span><span class="fl">0.04</span>)</span>
<span id="cb750-4"><a href="predictive-modeling.html#cb750-4" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(tree, <span class="at">box.palette=</span><span class="st">&quot;RdBu&quot;</span>, <span class="at">shadow.col=</span><span class="st">&quot;gray&quot;</span>, <span class="at">nn=</span><span class="cn">TRUE</span>, <span class="at">cex=</span><span class="dv">1</span>, <span class="at">extra=</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-568-1.png" width="1056" /></p>
<p>We see that the houses are first split based on whether or not their overall quality rating was less than 8. Each of the resulting nodes are then split again, using information from other explanatory variables. Each split partitions the data further, so that houses in the same node can be thought of as being similar to one another.</p>
<ul>
<li><p>The predicted price of a House with overall quality 7, and was built in 1995 is $200,000.</p></li>
<li><p>The predicted price of a House overall quality 8 and 1,750 sq. ft. on the first floor is $370,000.</p></li>
</ul>
</div>
<div id="partitioning-in-a-decision-tree" class="section level3 hasAnchor" number="7.4.2">
<h3><span class="header-section-number">7.4.2</span> Partitioning in A Decision Tree<a href="predictive-modeling.html#partitioning-in-a-decision-tree" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For a quantitative response variable, data are split into two nodes so that responses in the same node are as similar as possible, while responses in the different nodes are as different as possible.</p>
<p>Let L and R represent the left and right nodes from a possible split. Let <span class="math inline">\(n_L\)</span> and <span class="math inline">\(n_R\)</span> represent the number of observations in each node, and <span class="math inline">\(\bar{y}_L\)</span> and <span class="math inline">\(\bar{y}_R\)</span> represent the mean of the training data responses in each node.</p>
<p>For each possible split, involving an explanatory variable, we calculate:</p>
<p><span class="math display">\[ \displaystyle\sum_{i=1}^{n_L} (y_i -\bar{y}_L)^2 + \displaystyle\sum_{i=1}^{n_R} (y_i -\bar{y}_R)^2
\]</span></p>
<p>We choose the split that minimizes this quantity.</p>
<p><strong>Partitioning Example</strong></p>
<p>Consider a dataset with two explanatory variables, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, and a response variable <span class="math inline">\(y\)</span>, whose values are shown numerically in the graph.</p>
<pre><code>##    [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]
## x1    8    2    8    1    8    6    2    5    1     8     4    10     9     8
## x2    5    3    1    1    4    3    8    1   10     8     6     5     0     2
## y   253   64  258   21  257  203  246  114  331   256   213   406   326   273
##    [,15]
## x1     6
## x2     1
## y    155</code></pre>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-570-1.png" width="672" /></p>
<p>The goal is to split up the data, using information about <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> in a way that makes the <span class="math inline">\(y\)</span> values grouped together as similar as possible.</p>
<p><strong>1. One Possible Split (<span class="math inline">\(x_1 &lt; 5.5\)</span>)</strong></p>
<p>We could split the data into 2 groups depending on whether <span class="math inline">\(x_1 &lt; 5.5\)</span>.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-571-1.png" width="768" /></p>
<p>We calcuate the mean y-value in each resulting node:</p>
<ul>
<li><span class="math inline">\(\bar{y}_L = (331+246+213+21+64+114)/6 \approx 164.84\)</span><br />
</li>
<li><span class="math inline">\(\bar{y}_R = (203+155+256+253+257+273+258+326+406)/9 \approx 265.22\)</span></li>
</ul>
<p>To measure measure the amount of deviation in the node, we calculate the sum of the squared difference between each individual value and the overall mean in each node.</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \displaystyle\sum_{i=1}^{n_L} (y_i -\bar{y}_L)^2  \\
&amp; =(331-164.83)^2+(246-164.33)^2 + \ldots+(114-164.33)^2 \\
&amp; =69958.83
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\displaystyle\sum_{i=1}^{n_R} (y_i -\bar{y}_R)^2 \\
&amp; =(203-265.22)^2+(155-265.22)^2 + \ldots+(406-265.22)^2 \\
&amp; =39947.56
\end{aligned}
\]</span></p>
<p>Adding together these two quantities, we obtain an overall measure of the squared deviations between observations in the same node.</p>
<ul>
<li>69958.83 + 39947.56 = 109906.4</li>
</ul>
<p><strong>2.Second Possible Split (<span class="math inline">\(x_1 &lt; 6.5\)</span>)</strong></p>
<p>We could alternatively split the data into 2 groups depending on whether <span class="math inline">\(x_1 &lt; 6.5\)</span>.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-572-1.png" width="768" /></p>
<p>Using this split,</p>
<ul>
<li><span class="math inline">\(\bar{y}_L = (331+246+213+21+64+114 + 203+155)/8 \approx 168.375\)</span><br />
</li>
<li><span class="math inline">\(\bar{y}_R = (256+253+257+273+258+326+406)/7 \approx 289.857\)</span></li>
</ul>
<p><span class="math display">\[
\begin{aligned}
&amp; \displaystyle\sum_{i=1}^{n_L} (y_i -\bar{y}_L)^2  \\
&amp; =(331-168.375)^2+(246-168.375)^2 + \ldots+(203-168.375)^2 \\
&amp; =71411.88
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\displaystyle\sum_{i=1}^{n_R} (y_i -\bar{y}_R)^2 \\
&amp; =(203-289.857)^2+(155-289.857)^2 + \ldots+(406-289.857)^2 \\
&amp; =19678.86
\end{aligned}
\]</span></p>
<p>The total squared deviation is:</p>
<ul>
<li>71411.88 + 19678.86 = 91090.74</li>
</ul>
<p>The split at <span class="math inline">\(x1 &lt; 6.5\)</span> is better than <span class="math inline">\(x_1&lt;5.5\)</span></p>
<p><strong>3. Third Possible Split (<span class="math inline">\(x_2 &lt; 5.5\)</span>)</strong></p>
<p>We could also split the data into 2 groups depending on whether <span class="math inline">\(x_2 &lt; 5.5\)</span>.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-573-1.png" width="768" /></p>
<p>Using this split,</p>
<ul>
<li><span class="math inline">\(\bar{y}_L = (331+246+213+256)/4 \approx 261.5\)</span><br />
</li>
<li><span class="math inline">\(\bar{y}_R = (21 + 64 + \ldots + 406)/11 \approx 211.82\)</span></li>
</ul>
<p><span class="math display">\[
\begin{aligned}
&amp; \displaystyle\sum_{i=1}^{n_L} (y_i -\bar{y}_L)^2  \\
&amp; =(331-261.5)^2+(246-261.5)^2 + (213-261.5)^2+(256-261.5)^2 \\
&amp; =7453
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\displaystyle\sum_{i=1}^{n_R} (y_i -\bar{y}_R)^2 \\
&amp; =(21-211.82)^2+(64-211.82)^2 + \ldots+(406-211.82)^2 \\
&amp; =131493.6
\end{aligned}
\]</span></p>
<p>The sum of squared deviations is:</p>
<ul>
<li>7453 + 131493.6 = 138946.6</li>
</ul>
<p><strong>Comparison of Splits</strong></p>
<ul>
<li><p>Of the three splitâ€™s weâ€™ve calculated, <span class="math inline">\(\displaystyle\sum_{i=1}^{n_L} (y_i -\bar{y}_L)^2 + \displaystyle\sum_{i=1}^{n_R} (y_i -\bar{y}_R)^2\)</span> is minimized using <span class="math inline">\(x_1 &lt; 6.5\)</span>.</p></li>
<li><p>In fact, if we calculate all possible splits over <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, <span class="math inline">\(\displaystyle\sum_{i=1}^{n_L} (y_i -\bar{y}_L)^2 + \displaystyle\sum_{i=1}^{n_R} (y_i -\bar{y}_R)^2\)</span> is minimized by splitting on <span class="math inline">\(x_1 &lt; 6.5\)</span></p></li>
</ul>
<p>Thus, we perform the first split in the tree, using <span class="math inline">\(x_1 &lt; 6.5\)</span>.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-574-1.png" width="768" /></p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-575-1.png" width="1056" /></p>
</div>
<div id="next-splits" class="section level3 hasAnchor" number="7.4.3">
<h3><span class="header-section-number">7.4.3</span> Next Splits<a href="predictive-modeling.html#next-splits" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Next, we find the best splits on the resulting two nodes. It turns out that the left node is best split on <span class="math inline">\(x_2 &lt; 4.5\)</span>, and the right node is best split on <span class="math inline">\(x_1 &lt; 8.5\)</span>.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-576-1.png" width="768" /></p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-577-1.png" width="1056" /></p>
</div>
<div id="recursive-partitioning" class="section level3 hasAnchor" number="7.4.4">
<h3><span class="header-section-number">7.4.4</span> Recursive Partitioning<a href="predictive-modeling.html#recursive-partitioning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Splitting continues until nodes reach a certain predetermined minimal size, or until change improvement in model fit drops below a predetermined value</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-578-1.png" width="1056" /></p>
</div>
<div id="model-complexity-in-trees" class="section level3 hasAnchor" number="7.4.5">
<h3><span class="header-section-number">7.4.5</span> Model Complexity in Trees<a href="predictive-modeling.html#model-complexity-in-trees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The more we partition data into smaller nodes, the more complex the model becomes. As we continue to partition, bias decreases, as cases are grouped with those that are more similar to themselves. On the other hand, variance increases, as there are fewer cases in each node to be averaged, putting more weight on each individual observation.</p>
<p>Splitting into too small of nodes can lead to drastic overfitting. In the extreme case, if we split all the way to nodes of size 1, we would get RMSE of 0 on the training data, but should certainly not expect RMSPE of 0 on the test data.</p>
<p>The optimal depth of the tree, or minimal size for terminal nodes can be determined using cross-validation. The <code>rpart</code> package uses a complexity parameter <code>cp</code>, which determines how much a split must improve model fit in order to be made. Smaller values of <code>cp</code> are associated with more complex tree models, since they allow splits even when model fit only improves by a little.</p>
</div>
<div id="cross-validation-on-housing-data" class="section level3 hasAnchor" number="7.4.6">
<h3><span class="header-section-number">7.4.6</span> Cross-Validation on Housing Data<a href="predictive-modeling.html#cross-validation-on-housing-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Weâ€™ll use <code>caret</code> to determine the optimal value of the <code>cp</code> parameter. We use <code>method="rpart"</code> to grow decision trees.</p>
<div class="sourceCode" id="cb752"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb752-1"><a href="predictive-modeling.html#cb752-1" aria-hidden="true" tabindex="-1"></a>cp_vals <span class="ot">=</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">8</span>, <span class="dv">1</span>, <span class="at">length =</span> <span class="dv">100</span>) <span class="co"># test values between 1/10^8 and 1</span></span>
<span id="cb752-2"><a href="predictive-modeling.html#cb752-2" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(Train_sc) <span class="ot">&lt;-</span> <span class="fu">make.names</span>(<span class="fu">colnames</span>(Train_sc))</span>
<span id="cb752-3"><a href="predictive-modeling.html#cb752-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb752-4"><a href="predictive-modeling.html#cb752-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">11162020</span>)</span>
<span id="cb752-5"><a href="predictive-modeling.html#cb752-5" aria-hidden="true" tabindex="-1"></a>Housing_Tree <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">data=</span>Train_sc, SalePrice <span class="sc">~</span> .,  <span class="at">method=</span><span class="st">&quot;rpart&quot;</span>, <span class="at">trControl=</span>control, </span>
<span id="cb752-6"><a href="predictive-modeling.html#cb752-6" aria-hidden="true" tabindex="-1"></a>                     <span class="at">tuneGrid=</span><span class="fu">expand.grid</span>(<span class="at">cp=</span>cp_vals))</span></code></pre></div>
<p>The optimal value of <code>cp</code> is:</p>
<div class="sourceCode" id="cb753"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb753-1"><a href="predictive-modeling.html#cb753-1" aria-hidden="true" tabindex="-1"></a>Housing_Tree<span class="sc">$</span>bestTune</span></code></pre></div>
<pre><code>##              cp
## 52 0.0004328761</code></pre>
<p>We plot RMSPE on the holdout data as a function of <code>cp</code>.</p>
<div class="sourceCode" id="cb755"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb755-1"><a href="predictive-modeling.html#cb755-1" aria-hidden="true" tabindex="-1"></a>cp <span class="ot">&lt;-</span> Housing_Tree<span class="sc">$</span>results<span class="sc">$</span>cp</span>
<span id="cb755-2"><a href="predictive-modeling.html#cb755-2" aria-hidden="true" tabindex="-1"></a>RMSPE <span class="ot">&lt;-</span> Housing_Tree<span class="sc">$</span>results<span class="sc">$</span>RMSE</span>
<span id="cb755-3"><a href="predictive-modeling.html#cb755-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span><span class="fu">data.frame</span>(cp, RMSPE), <span class="fu">aes</span>(<span class="at">x=</span>cp, <span class="at">y=</span>RMSPE))<span class="sc">+</span><span class="fu">geom_line</span>() <span class="sc">+</span> <span class="fu">xlim</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="fl">0.001</span>)) <span class="sc">+</span> <span class="fu">ylim</span>(<span class="fu">c</span>(<span class="fl">0.475</span>,<span class="fl">0.485</span>))  <span class="sc">+</span> </span>
<span id="cb755-4"><a href="predictive-modeling.html#cb755-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Regression Tree Cross Validation Results&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-581-1.png" width="672" /></p>
</div>
<div id="comparing-ols-lasso-ridge-and-tree" class="section level3 hasAnchor" number="7.4.7">
<h3><span class="header-section-number">7.4.7</span> Comparing OLS, Lasso, Ridge, and Tree<a href="predictive-modeling.html#comparing-ols-lasso-ridge-and-tree" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb756"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb756-1"><a href="predictive-modeling.html#cb756-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">11162020</span>)</span>
<span id="cb756-2"><a href="predictive-modeling.html#cb756-2" aria-hidden="true" tabindex="-1"></a>Housing_OLS <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">data=</span>Train_sc, SalePrice <span class="sc">~</span> .,  <span class="at">method=</span><span class="st">&quot;lm&quot;</span>, <span class="at">trControl=</span>control)</span>
<span id="cb756-3"><a href="predictive-modeling.html#cb756-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">11162020</span>)</span>
<span id="cb756-4"><a href="predictive-modeling.html#cb756-4" aria-hidden="true" tabindex="-1"></a>Housing_lasso <span class="ot">&lt;-</span> <span class="fu">train</span>(SalePrice <span class="sc">~</span>., <span class="at">data =</span> Train_sc, <span class="at">method =</span> <span class="st">&quot;glmnet&quot;</span>, <span class="at">trControl=</span>control, </span>
<span id="cb756-5"><a href="predictive-modeling.html#cb756-5" aria-hidden="true" tabindex="-1"></a>                      <span class="at">tuneGrid=</span><span class="fu">expand.grid</span>(<span class="at">alpha=</span><span class="dv">1</span>, <span class="at">lambda=</span>l_vals))</span></code></pre></div>
<p>RMSPE on the standardized version of the response variable is displayed below for ordinary least squares, ridge regression, lasso regression, and a decistion tree.</p>
<div class="sourceCode" id="cb757"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb757-1"><a href="predictive-modeling.html#cb757-1" aria-hidden="true" tabindex="-1"></a><span class="fu">min</span>(Housing_OLS <span class="sc">$</span>results<span class="sc">$</span>RMSE)</span></code></pre></div>
<pre><code>## [1] 0.5634392</code></pre>
<div class="sourceCode" id="cb759"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb759-1"><a href="predictive-modeling.html#cb759-1" aria-hidden="true" tabindex="-1"></a><span class="fu">min</span>(Housing_ridge<span class="sc">$</span>results<span class="sc">$</span>RMSE)</span></code></pre></div>
<pre><code>## [1] 0.4570054</code></pre>
<div class="sourceCode" id="cb761"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb761-1"><a href="predictive-modeling.html#cb761-1" aria-hidden="true" tabindex="-1"></a><span class="fu">min</span>(Housing_lasso<span class="sc">$</span>results<span class="sc">$</span>RMSE)</span></code></pre></div>
<pre><code>## [1] 0.4730672</code></pre>
<div class="sourceCode" id="cb763"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb763-1"><a href="predictive-modeling.html#cb763-1" aria-hidden="true" tabindex="-1"></a><span class="fu">min</span>(Housing_Tree<span class="sc">$</span>results<span class="sc">$</span>RMSE)</span></code></pre></div>
<pre><code>## [1] 0.477414</code></pre>
<p>In this situation, the tree outperforms OLS, but does not do as well as lasso or ridge. The best model will vary depending on the nature of the data. We can use cross-validation to determine which model is likely to perform best in prediction.</p>
</div>
<div id="random-forest" class="section level3 hasAnchor" number="7.4.8">
<h3><span class="header-section-number">7.4.8</span> Random Forest<a href="predictive-modeling.html#random-forest" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A popular extension of a decision tree is a random forest. A random forest consists of many (often ~10,000) trees. Predictions are made by averaging predictions from individual trees.</p>
<ul>
<li>In order to ensure the trees are different from each other:
<ol style="list-style-type: decimal">
<li>each tree is grown from a different bootstrap sample of the training data.<br />
</li>
<li>when deciding on a split, only a random subset of explanatory variables are considered.</li>
</ol></li>
</ul>
<p>Growing deep trees ensures low bias. In a random forest, averaging across many deep trees decreases variance, while maintaining low bias.</p>
</div>
</div>
<div id="regression-splines" class="section level2 hasAnchor" number="7.5">
<h2><span class="header-section-number">7.5</span> Regression Splines<a href="predictive-modeling.html#regression-splines" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="regression-splines-1" class="section level3 hasAnchor" number="7.5.1">
<h3><span class="header-section-number">7.5.1</span> Regression Splines<a href="predictive-modeling.html#regression-splines-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Weâ€™ve seen that we can use polynomial regression to capture nonlinear trends in data.</p>
<ul>
<li>A <strong>regression spline</strong> is a piecewise function of polynomials.</li>
</ul>
<p>Here weâ€™ll keep thing simple by focusing on a spline with a single explanatory variable. Splines can also be used for multivariate data.</p>
<p>Weâ€™ll examine the use of splines on the car price prediction dataset.</p>
<p>We divide the data into a set of 75 cars, which weâ€™ll use to train the model, and 35 cars, on which weâ€™ll make and evaluate predictions.</p>
<p>The 75 cars in the training set are shown below.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-585-1.png" width="960" /></p>
</div>
<div id="two-models-with-high-bias" class="section level3 hasAnchor" number="7.5.2">
<h3><span class="header-section-number">7.5.2</span> Two Models with High Bias<a href="predictive-modeling.html#two-models-with-high-bias" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-586-1.png" width="1152" /></p>
<p>The constant and linear models have high bias, as they are not complex enough to capture the apparent curvature in the relationship between price and acceleration time.</p>
<p>A cubic model, on the other hand might better capture the trend.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-587-1.png" width="672" /></p>
</div>
<div id="cubic-splines" class="section level3 hasAnchor" number="7.5.3">
<h3><span class="header-section-number">7.5.3</span> Cubic Splines<a href="predictive-modeling.html#cubic-splines" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Itâ€™s possible that the behavior of the response variable might differ in different regions of the x-axis. A cubic spline allows us to fit different models in different regions of the x-axis.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-588-1.png" width="960" /></p>
<p>The region boundaries are called <strong>knots</strong></p>
<p><strong>Cubic Spline with 5 Knots</strong></p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-589-1.png" width="960" /></p>
<p><strong>Cubic Spline with 10 Knots</strong></p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-590-1.png" width="960" /></p>
<p><strong>Cubic Spline with 20 Knots</strong></p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-591-1.png" width="960" /></p>
<p>Notice that as the number of knots increases, the model becomes more and more complex. We would not expect the relationship between price and acceleration time to look like it does in these more complicated pictures. It is likely that as the number of knots gets big, the model overfits the training data.</p>
</div>
<div id="predicting-test-data" class="section level3 hasAnchor" number="7.5.4">
<h3><span class="header-section-number">7.5.4</span> Predicting Test Data<a href="predictive-modeling.html#predicting-test-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Shown below is a plot of RMSPE when predictions are made on the new test data.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-592-1.png" width="960" /></p>
<p>We see that RMSPE is minimized using the model with three knots.</p>
</div>
<div id="implementation-of-splines" class="section level3 hasAnchor" number="7.5.5">
<h3><span class="header-section-number">7.5.5</span> Implementation of Splines<a href="predictive-modeling.html#implementation-of-splines" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Important Considerations:</p>
<ul>
<li>how many knots<br />
</li>
<li>where to place knots<br />
</li>
<li>degree of polynomial</li>
</ul>
<p>The best choices for all of these will vary between datasets and can be assessed through cross-validation.</p>
</div>
</div>
<div id="summary-and-comparision" class="section level2 hasAnchor" number="7.6">
<h2><span class="header-section-number">7.6</span> Summary and Comparision<a href="predictive-modeling.html#summary-and-comparision" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous sections, weâ€™ve applied various predictive modeling techniques to predict house prices in Ames, IA. In each section, weâ€™ve focused on an individual predictive technique (OLS, ridge/lasso regression, trees, splines), but in practice, we often test out these techniques together to find which is likely to perform best on a set of data. Here, weâ€™ll go through the steps to test out and evaluate these techniques on the Ames Housing dataset.</p>
<p>There are no new statistical ideas presented in this section, just a synthesis of the preceding material. We leave out splines, since we did not discuss using splines in a multivariate setting, but we compare OLS, ridge and decision trees.</p>
<p>We use a subset of variables for illustrative purposes.</p>
<div class="sourceCode" id="cb765"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb765-1"><a href="predictive-modeling.html#cb765-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">10302021</span>)</span>
<span id="cb765-2"><a href="predictive-modeling.html#cb765-2" aria-hidden="true" tabindex="-1"></a>samp <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(ames_raw), <span class="dv">1000</span>)</span>
<span id="cb765-3"><a href="predictive-modeling.html#cb765-3" aria-hidden="true" tabindex="-1"></a>Houses <span class="ot">&lt;-</span> ames_raw[samp,]</span></code></pre></div>
<div class="sourceCode" id="cb766"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb766-1"><a href="predictive-modeling.html#cb766-1" aria-hidden="true" tabindex="-1"></a>New_Houses <span class="ot">&lt;-</span> ames_raw <span class="ot">&lt;-</span> ames_raw[<span class="sc">-</span>samp,]</span>
<span id="cb766-2"><a href="predictive-modeling.html#cb766-2" aria-hidden="true" tabindex="-1"></a>New_Houses <span class="ot">&lt;-</span> New_Houses[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, ]</span></code></pre></div>
<p>Weâ€™ll begin by doing some data preparation.</p>
<p>We standardize all explanatory variables in the training and new data. We do not standardize the response variable, price, so we can interpret predicted values more easily.</p>
<div class="sourceCode" id="cb767"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb767-1"><a href="predictive-modeling.html#cb767-1" aria-hidden="true" tabindex="-1"></a>Houses_Combined <span class="ot">&lt;-</span> <span class="fu">rbind</span>(Houses, New_Houses)</span>
<span id="cb767-2"><a href="predictive-modeling.html#cb767-2" aria-hidden="true" tabindex="-1"></a>Houses_sc <span class="ot">&lt;-</span> Houses_Combined <span class="sc">%&gt;%</span> <span class="fu">mutate_if</span>(is.numeric, scale)</span>
<span id="cb767-3"><a href="predictive-modeling.html#cb767-3" aria-hidden="true" tabindex="-1"></a>Houses_sc<span class="sc">$</span>SalePrice <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(Houses_Combined<span class="sc">$</span>SalePrice)</span>
<span id="cb767-4"><a href="predictive-modeling.html#cb767-4" aria-hidden="true" tabindex="-1"></a>Houses_sc_Train <span class="ot">&lt;-</span> Houses_sc[<span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>, ]</span>
<span id="cb767-5"><a href="predictive-modeling.html#cb767-5" aria-hidden="true" tabindex="-1"></a>Houses_sc_New <span class="ot">&lt;-</span> Houses_sc[<span class="dv">1001</span><span class="sc">:</span><span class="dv">1005</span>, ]</span></code></pre></div>
<p>The <code>Houses_sc_Train</code> dataset contains standardized values for the 1000 houses in the training data. The first six rows are shown below.</p>
<div class="sourceCode" id="cb768"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb768-1"><a href="predictive-modeling.html#cb768-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Houses_sc_Train)</span></code></pre></div>
<pre><code>##      Overall.Qual  Year.Built Central.Air  Gr.Liv.Area X1st.Flr.SF
## 2771  -0.07619084  0.79212207           Y -0.003620472  -0.9340706
## 2909  -0.77229808  0.18658251           Y -0.392399656  -1.3891530
## 2368  -0.07619084  0.01837707           Y -0.938684253  -1.6993209
## 2604  -2.16451257 -1.89916487           Y -0.571836202  -1.1908489
## 669   -0.07619084 -0.14982836           Y -0.918746859  -0.3111924
## 1427   2.01213088  1.22945620           Y  0.528707949   1.5345608
##      Bedroom.AbvGr TotRms.AbvGrd    Lot.Area Lot.Shape Land.Contour
## 2771     0.1632018    -0.2910546  0.28570669       IR1          Lvl
## 2909     0.1632018    -0.9160758 -0.88703104       Reg          Lvl
## 2368     0.1632018    -0.2910546 -0.98002927       Reg          Lvl
## 2604     0.1632018    -0.2910546  0.04235132       Reg          Lvl
## 669      0.1632018    -0.2910546  0.17314883       IR1          Lvl
## 1427     0.1632018     0.9589877  0.20650820       Reg          Lvl
##      Overall.Cond Exter.Qual Heating.QC Paved.Drive SalePrice
## 2771   -0.4680319         Gd         Ex           Y    187000
## 2909    0.4279149         TA         TA           Y    104500
## 2368    1.3238618         TA         Ex           Y    116000
## 2604   -1.3639788         TA         TA           Y    105000
## 669    -1.3639788         Fa         Gd           Y    163000
## 1427   -0.4680319         Ex         Ex           Y    395039</code></pre>
<p>The <code>Houses_sc_New</code> displays standardized values for the new houses that weâ€™re trying to predict.</p>
<div class="sourceCode" id="cb770"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb770-1"><a href="predictive-modeling.html#cb770-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Houses_sc_New)</span></code></pre></div>
<pre><code>##   Overall.Qual Year.Built Central.Air Gr.Liv.Area X1st.Flr.SF Bedroom.AbvGr
## 2  -0.77229808 -0.3516749           Y  -1.2058453  -0.6772922    -1.0608118
## 4   0.61991640 -0.1161873           Y   1.2145543   2.4091326     0.1632018
## 6  -0.07619084  0.8930453           Y   0.2057222  -0.6010214     0.1632018
## 7   1.31602364  0.9939686           Y  -0.3246125   0.4464308    -1.0608118
## 8   1.31602364  0.6911988           Y  -0.4402494   0.2989739    -1.0608118
##   TotRms.AbvGrd   Lot.Area Lot.Shape Land.Contour Overall.Cond Exter.Qual
## 2    -0.9160758  0.1877886       Reg          Lvl    0.4279149         TA
## 4     0.9589877  0.1323496       Reg          Lvl   -0.4680319         Gd
## 6     0.3339665 -0.0094877       IR1          Lvl    0.4279149         TA
## 7    -0.2910546 -0.6164362       Reg          Lvl   -0.4680319         Gd
## 8    -0.9160758 -0.6062364       IR1          HLS   -0.4680319         Gd
##   Heating.QC Paved.Drive SalePrice
## 2         TA           Y    105000
## 4         Ex           Y    244000
## 6         Ex           Y    195500
## 7         Ex           Y    213500
## 8         Ex           Y    191500</code></pre>
<p>Since the <code>glmnet</code> command requires training data to be entered as a matrix, we create versions of the datasets in matrix form.</p>
<div class="sourceCode" id="cb772"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb772-1"><a href="predictive-modeling.html#cb772-1" aria-hidden="true" tabindex="-1"></a>Houses_sc<span class="sc">$</span>SalePrice[<span class="fu">is.na</span>(Houses<span class="sc">$</span>SalePrice)] <span class="ot">&lt;-</span> <span class="dv">0</span> <span class="co">#can&#39;t take NA&#39;s when fitting model matrix, doesn&#39;t matter since only need x-coeffs</span></span>
<span id="cb772-2"><a href="predictive-modeling.html#cb772-2" aria-hidden="true" tabindex="-1"></a>Houses_sc_Combined_MAT <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(SalePrice<span class="sc">~</span>., <span class="at">data=</span><span class="fu">rbind</span>(Houses_sc))</span>
<span id="cb772-3"><a href="predictive-modeling.html#cb772-3" aria-hidden="true" tabindex="-1"></a>Houses_sc_Train_MAT <span class="ot">&lt;-</span> Houses_sc_Combined_MAT[<span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>, ]</span>
<span id="cb772-4"><a href="predictive-modeling.html#cb772-4" aria-hidden="true" tabindex="-1"></a>Houses_sc_New_MAT <span class="ot">&lt;-</span> Houses_sc_Combined_MAT[<span class="dv">1001</span><span class="sc">:</span><span class="dv">1005</span>, ]</span></code></pre></div>
<div id="modeling-with-ols" class="section level3 hasAnchor" number="7.6.1">
<h3><span class="header-section-number">7.6.1</span> Modeling with OLS<a href="predictive-modeling.html#modeling-with-ols" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We first fit an ordinary least squares regression model to the data.</p>
<div class="sourceCode" id="cb773"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb773-1"><a href="predictive-modeling.html#cb773-1" aria-hidden="true" tabindex="-1"></a>Housing_OLS <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data=</span>Houses_sc_Train, SalePrice<span class="sc">~</span> .)</span>
<span id="cb773-2"><a href="predictive-modeling.html#cb773-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(Housing_OLS)</span></code></pre></div>
<pre><code>##     (Intercept)    Overall.Qual      Year.Built    Central.AirY     Gr.Liv.Area 
##     238908.0614      23368.4152      14036.2549      -4497.1153      29640.4606 
##     X1st.Flr.SF   Bedroom.AbvGr   TotRms.AbvGrd        Lot.Area    Lot.ShapeIR2 
##       8320.1472      -5011.0485       1554.7785       7566.9377       1570.1676 
##    Lot.ShapeIR3    Lot.ShapeReg Land.ContourHLS Land.ContourLow Land.ContourLvl 
##      19082.7508      -4566.5111      44704.8906      22406.5959      19096.4163 
##    Overall.Cond    Exter.QualFa    Exter.QualGd    Exter.QualTA    Heating.QCFa 
##       7965.6704     -68750.2773     -62800.5804     -74028.3841      -3972.4036 
##    Heating.QCGd    Heating.QCPo    Heating.QCTA    Paved.DriveP    Paved.DriveY 
##      -4478.6876     -23000.4394      -5272.7136      -1901.9235        709.1532</code></pre>
</div>
<div id="ridge-regression-with-housing-data" class="section level3 hasAnchor" number="7.6.2">
<h3><span class="header-section-number">7.6.2</span> Ridge Regression with Housing Data<a href="predictive-modeling.html#ridge-regression-with-housing-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now, weâ€™ll use ridge regression to predict insurance costs.</p>
<p>We use cross validation to determine the optimal value of lamba. We perform 10 repeats of 10-fold cross-validation. We test 100 lambda-values ranging from <span class="math inline">\(10^-5\)</span> to <span class="math inline">\(10^5\)</span>.</p>
<div class="sourceCode" id="cb775"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb775-1"><a href="predictive-modeling.html#cb775-1" aria-hidden="true" tabindex="-1"></a>control <span class="ot">=</span> <span class="fu">trainControl</span>(<span class="st">&quot;repeatedcv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>, <span class="at">repeats=</span><span class="dv">10</span>)</span>
<span id="cb775-2"><a href="predictive-modeling.html#cb775-2" aria-hidden="true" tabindex="-1"></a>l_vals <span class="ot">=</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="at">length =</span> <span class="dv">100</span>)</span>
<span id="cb775-3"><a href="predictive-modeling.html#cb775-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb775-4"><a href="predictive-modeling.html#cb775-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2022</span>)</span>
<span id="cb775-5"><a href="predictive-modeling.html#cb775-5" aria-hidden="true" tabindex="-1"></a>Housing_ridge <span class="ot">&lt;-</span> <span class="fu">train</span>( SalePrice <span class="sc">~</span> ., <span class="at">data =</span> Houses_sc_Train, <span class="at">method =</span> <span class="st">&quot;glmnet&quot;</span>, <span class="at">trControl=</span>control , <span class="at">tuneGrid=</span><span class="fu">expand.grid</span>(<span class="at">alpha=</span><span class="dv">0</span>, <span class="at">lambda=</span>l_vals))</span></code></pre></div>
<div class="sourceCode" id="cb776"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb776-1"><a href="predictive-modeling.html#cb776-1" aria-hidden="true" tabindex="-1"></a>Housing_ridge<span class="sc">$</span>bestTune<span class="sc">$</span>lambda</span></code></pre></div>
<pre><code>## [1] 6135.907</code></pre>
<p>We fit a model to the full training dataset using the optimal value of <span class="math inline">\(lambda\)</span> .</p>
<div class="sourceCode" id="cb778"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb778-1"><a href="predictive-modeling.html#cb778-1" aria-hidden="true" tabindex="-1"></a>ridge_mod <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(<span class="at">x=</span>Houses_sc_Train_MAT, <span class="at">y=</span>Houses_sc_Train<span class="sc">$</span>SalePrice, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda=</span>Housing_ridge<span class="sc">$</span>bestTune<span class="sc">$</span>lambda )</span>
<span id="cb778-2"><a href="predictive-modeling.html#cb778-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(ridge_mod)</span></code></pre></div>
<pre><code>## 26 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                          s0
## (Intercept)     206079.5442
## (Intercept)          .     
## Overall.Qual     24716.6502
## Year.Built       12909.8224
## Central.AirY     -1693.4832
## Gr.Liv.Area      24137.8185
## X1st.Flr.SF      10707.5001
## Bedroom.AbvGr    -5034.2266
## TotRms.AbvGrd     5394.5170
## Lot.Area          7086.7613
## Lot.ShapeIR2      3322.3720
## Lot.ShapeIR3     18987.3176
## Lot.ShapeReg     -5345.7478
## Land.ContourHLS  41239.7682
## Land.ContourLow  15011.2269
## Land.ContourLvl  12784.0351
## Overall.Cond      6560.4987
## Exter.QualFa    -29042.5581
## Exter.QualGd    -24942.6445
## Exter.QualTA    -35102.6069
## Heating.QCFa     -8118.6371
## Heating.QCGd     -6380.1279
## Heating.QCPo    -19693.6611
## Heating.QCTA     -7645.0855
## Paved.DriveP      -613.3798
## Paved.DriveY      1324.2704</code></pre>
<p>The regression coefficients are displayed together with the OLS coefficients in a data.frame.</p>
<div class="sourceCode" id="cb780"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb780-1"><a href="predictive-modeling.html#cb780-1" aria-hidden="true" tabindex="-1"></a>Ridge_coef <span class="ot">&lt;-</span> <span class="fu">as.vector</span>(ridge_mod<span class="sc">$</span>beta)[<span class="sc">-</span><span class="dv">1</span>] <span class="co">#leave off intercept using [-1]</span></span>
<span id="cb780-2"><a href="predictive-modeling.html#cb780-2" aria-hidden="true" tabindex="-1"></a>OLS_coef <span class="ot">&lt;-</span> <span class="fu">coef</span>(Housing_OLS)[<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb780-3"><a href="predictive-modeling.html#cb780-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(OLS_coef, Ridge_coef)</span></code></pre></div>
<pre><code>##                    OLS_coef  Ridge_coef
## Overall.Qual     23368.4152  24716.6502
## Year.Built       14036.2549  12909.8224
## Central.AirY     -4497.1153  -1693.4832
## Gr.Liv.Area      29640.4606  24137.8185
## X1st.Flr.SF       8320.1472  10707.5001
## Bedroom.AbvGr    -5011.0485  -5034.2266
## TotRms.AbvGrd     1554.7785   5394.5170
## Lot.Area          7566.9377   7086.7613
## Lot.ShapeIR2      1570.1676   3322.3720
## Lot.ShapeIR3     19082.7508  18987.3176
## Lot.ShapeReg     -4566.5111  -5345.7478
## Land.ContourHLS  44704.8906  41239.7682
## Land.ContourLow  22406.5959  15011.2269
## Land.ContourLvl  19096.4163  12784.0351
## Overall.Cond      7965.6704   6560.4987
## Exter.QualFa    -68750.2773 -29042.5581
## Exter.QualGd    -62800.5804 -24942.6445
## Exter.QualTA    -74028.3841 -35102.6069
## Heating.QCFa     -3972.4036  -8118.6371
## Heating.QCGd     -4478.6876  -6380.1279
## Heating.QCPo    -23000.4394 -19693.6611
## Heating.QCTA     -5272.7136  -7645.0855
## Paved.DriveP     -1901.9235   -613.3798
## Paved.DriveY       709.1532   1324.2704</code></pre>
</div>
<div id="decision-tree" class="section level3 hasAnchor" number="7.6.3">
<h3><span class="header-section-number">7.6.3</span> Decision Tree<a href="predictive-modeling.html#decision-tree" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now, weâ€™ll predict house prices using using a decision tree.</p>
<p>First, we grow and display a small decision tree, by setting the <code>cp</code> parameter equal to 0.05.</p>
<div class="sourceCode" id="cb782"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb782-1"><a href="predictive-modeling.html#cb782-1" aria-hidden="true" tabindex="-1"></a>tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(SalePrice<span class="sc">~</span>., <span class="at">data=</span>Houses_sc_Train, <span class="at">cp=</span><span class="fl">0.05</span>)</span>
<span id="cb782-2"><a href="predictive-modeling.html#cb782-2" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(tree, <span class="at">box.palette=</span><span class="st">&quot;RdBu&quot;</span>, <span class="at">shadow.col=</span><span class="st">&quot;gray&quot;</span>, <span class="at">nn=</span><span class="cn">TRUE</span>, <span class="at">cex=</span><span class="dv">1</span>, <span class="at">extra=</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-605-1.png" width="672" /></p>
<p>Now we use cross-validation to determine the optimal value of the <code>cp</code> parameter. We use 10 repeats of 10-fold cross-validation. We test 1000 cp-values ranging from <span class="math inline">\(10^-5\)</span> to <span class="math inline">\(10^5\)</span>.</p>
<div class="sourceCode" id="cb783"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb783-1"><a href="predictive-modeling.html#cb783-1" aria-hidden="true" tabindex="-1"></a>cp_vals <span class="ot">=</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="at">length =</span> <span class="dv">100</span>)</span>
<span id="cb783-2"><a href="predictive-modeling.html#cb783-2" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(Houses_sc_Train) <span class="ot">&lt;-</span> <span class="fu">make.names</span>(<span class="fu">colnames</span>(Houses_sc_Train))</span>
<span id="cb783-3"><a href="predictive-modeling.html#cb783-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb783-4"><a href="predictive-modeling.html#cb783-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2022</span>)</span>
<span id="cb783-5"><a href="predictive-modeling.html#cb783-5" aria-hidden="true" tabindex="-1"></a>Housing_Tree <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">data=</span>Houses_sc_Train, SalePrice <span class="sc">~</span> .,  <span class="at">method=</span><span class="st">&quot;rpart&quot;</span>, <span class="at">trControl=</span>control,<span class="at">tuneGrid=</span><span class="fu">expand.grid</span>(<span class="at">cp=</span>cp_vals))</span>
<span id="cb783-6"><a href="predictive-modeling.html#cb783-6" aria-hidden="true" tabindex="-1"></a>Housing_Tree<span class="sc">$</span>bestTune</span></code></pre></div>
<pre><code>##              cp
## 4 0.00002009233</code></pre>
<p>We grow a full tree using the optimal <code>cp</code> value.</p>
<div class="sourceCode" id="cb785"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb785-1"><a href="predictive-modeling.html#cb785-1" aria-hidden="true" tabindex="-1"></a>Housing_Best_Tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(SalePrice<span class="sc">~</span>., <span class="at">data=</span>Houses_sc_Train, <span class="at">cp=</span>Housing_Tree<span class="sc">$</span>bestTune)</span></code></pre></div>
</div>
<div id="comparing-performance" class="section level3 hasAnchor" number="7.6.4">
<h3><span class="header-section-number">7.6.4</span> Comparing Performance<a href="predictive-modeling.html#comparing-performance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We use cross-validation to compare the performance of the linear model, ridge regression model, and decision tree.</p>
<div class="sourceCode" id="cb786"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb786-1"><a href="predictive-modeling.html#cb786-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2022</span>)</span>
<span id="cb786-2"><a href="predictive-modeling.html#cb786-2" aria-hidden="true" tabindex="-1"></a>Housing_OLS <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">data=</span>Houses_sc_Train, SalePrice <span class="sc">~</span> .,  <span class="at">method=</span><span class="st">&quot;lm&quot;</span>, <span class="at">trControl=</span>control)</span></code></pre></div>
<div class="sourceCode" id="cb787"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb787-1"><a href="predictive-modeling.html#cb787-1" aria-hidden="true" tabindex="-1"></a><span class="fu">min</span>(Housing_OLS<span class="sc">$</span>results<span class="sc">$</span>RMSE)</span></code></pre></div>
<pre><code>## [1] 35313.3</code></pre>
<div class="sourceCode" id="cb789"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb789-1"><a href="predictive-modeling.html#cb789-1" aria-hidden="true" tabindex="-1"></a><span class="fu">min</span>(Housing_ridge<span class="sc">$</span>results<span class="sc">$</span>RMSE)</span></code></pre></div>
<pre><code>## [1] 35747.22</code></pre>
<div class="sourceCode" id="cb791"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb791-1"><a href="predictive-modeling.html#cb791-1" aria-hidden="true" tabindex="-1"></a><span class="fu">min</span>(Housing_Tree<span class="sc">$</span>results<span class="sc">$</span>RMSE)</span></code></pre></div>
<pre><code>## [1] 33675.46</code></pre>
<p>The tree predictions give slightly lower RMSPE.</p>
</div>
<div id="predictions-on-new-data" class="section level3 hasAnchor" number="7.6.5">
<h3><span class="header-section-number">7.6.5</span> Predictions on New Data<a href="predictive-modeling.html#predictions-on-new-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now predict the sale price of the five new houses using each technique.</p>
<p>Ordinary Least-Squares model:</p>
<div class="sourceCode" id="cb793"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb793-1"><a href="predictive-modeling.html#cb793-1" aria-hidden="true" tabindex="-1"></a>OLS_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(Housing_OLS, <span class="at">newdata=</span>Houses_sc_New)</span>
<span id="cb793-2"><a href="predictive-modeling.html#cb793-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(OLS_pred)</span></code></pre></div>
<pre><code>##        2        4        6        7        8 
## 114709.4 253695.8 195078.1 222117.6 242493.9</code></pre>
<p>Ridge regression model:</p>
<p>We use the <code>Customers_sc_New_MAT</code> dataset, since the <code>glmnet</code> package requires inputs in matrix form.</p>
<div class="sourceCode" id="cb795"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb795-1"><a href="predictive-modeling.html#cb795-1" aria-hidden="true" tabindex="-1"></a>ridge_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(ridge_mod, <span class="at">newx=</span>Houses_sc_New_MAT)</span>
<span id="cb795-2"><a href="predictive-modeling.html#cb795-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(ridge_pred)</span></code></pre></div>
<pre><code>##         s0
## 2 114950.4
## 4 259359.8
## 6 195288.0
## 7 226841.6
## 8 249064.8</code></pre>
<p>Decision tree:</p>
<div class="sourceCode" id="cb797"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb797-1"><a href="predictive-modeling.html#cb797-1" aria-hidden="true" tabindex="-1"></a>tree_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(Housing_Best_Tree, <span class="at">newdata=</span>Houses_sc_New)</span>
<span id="cb797-2"><a href="predictive-modeling.html#cb797-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(tree_pred)</span></code></pre></div>
<pre><code>##        2        4        6        7        8 
## 132926.5 287045.1 183978.6 207079.4 207079.4</code></pre>
</div>
</div>
<div id="ethical-considerations-in-predictive-modeling" class="section level2 hasAnchor" number="7.7">
<h2><span class="header-section-number">7.7</span> Ethical Considerations in Predictive Modeling<a href="predictive-modeling.html#ethical-considerations-in-predictive-modeling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="assumptions-in-predictive-models" class="section level3 hasAnchor" number="7.7.1">
<h3><span class="header-section-number">7.7.1</span> Assumptions in Predictive Models<a href="predictive-modeling.html#assumptions-in-predictive-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Like any other statistical technique, predictive inference (sometimes done through machine learning algorithms) depends on the validity of assumptions.</p>
<ol style="list-style-type: decimal">
<li><p>The response variable observed in the data is actually the thing we want to predict<br />
</p></li>
<li><p>Training/Test data representative of population of interest</p></li>
<li><p>Prediction accuracy is appropriate metric</p></li>
</ol>
<p>Below are some examples of real uses of predictive inference in which some of these assumptions were violated, leading to inappropriate and unethical conclusions.</p>
</div>
<div id="amazon-hiring-algorithm" class="section level3 hasAnchor" number="7.7.2">
<h3><span class="header-section-number">7.7.2</span> Amazon Hiring Algorithm<a href="predictive-modeling.html#amazon-hiring-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In 2014, Amazon began working on an algorithm to predict whether a job applicant would be suitable for hire for software developer positions, based on characteristics of their job application.</p>
<p>response variable: rating of candidateâ€™s strength (1-5)
explanatory variables: many variables based on information included on the resume (e.g.Â highest degree, major, GPA, college/university, prior job experiences, internships, frequency of certain words on resume, etc.)</p>
<p>The algorithm was trained using data from past applications, rated by humans, over the past 10 years. It could then be used to predict ratings of future job applicants.</p>
<p>According to [Reuters])(<a href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G" class="uri">https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G</a>),</p>
<p>â€œIn effect, Amazonâ€™s system taught itself that male candidates were preferable. It penalized resumes that included the word â€œwomenâ€™s,â€ as in â€œwomenâ€™s chess club captain.â€ And it downgraded graduates of two all-womenâ€™s colleges, according to people familiar with the matter.â€</p>
<p>While the algorithm was intended to predict candidate quality, the response variable on the training data actually reflected biases in past hiring decisions, leading the algorithm to do the same.</p>
</div>
<div id="facial-recognition" class="section level3 hasAnchor" number="7.7.3">
<h3><span class="header-section-number">7.7.3</span> Facial Recognition<a href="predictive-modeling.html#facial-recognition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Facial recognition technology is used by law enforcement surveillance, airport passenger screening, and employment and housing decisions. It has, however, been banned for use by police in some cities, including San Francisco and Boston, due to concerns about inequity and privacy.</p>
<p><a href="https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/">Research</a> has shown that although certain facial recognition algorithms achieve over 90% accuracy overall, accuracy rate is lower among subjects who are female, Black, or 18-30 years old.</p>
<p>This is likely due, at least in part, to the algorithms being trained primarily on data an images of people who are not members of these groups.</p>
<p>Although the algorithms might attain strong accuracy overall, it is inappropriate to evaluate them on this basis, without accounting for performance on subgroups in the population.</p>
</div>
<div id="comments" class="section level3 hasAnchor" number="7.7.4">
<h3><span class="header-section-number">7.7.4</span> Comments<a href="predictive-modeling.html#comments" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The biases and assumptions noted above are not reasons to abandon predictive modeling, but rather flaws to be aware of and work to correct.</p>
<p>Predictive algorithms, are only as good as the data on which they are trained and the societies in which they are developed, and will reflect inherent biases. Thus, they should be used cautiously and with with human judgment, just like any other statistical technique.</p>
<p>Beware of statements like:</p>
<p>â€œThe data say this!â€</p>
<p>â€œThe algorithm is objective.â€</p>
<p>â€œThe numbers donâ€™t lie.â€</p>
<p>Any data-driven analysis depends on assumptions, and sound judgment and awareness of context are required when assessing the validaty of conclusions drawn.</p>
</div>
<div id="modeling-for-prediction-2" class="section level3 hasAnchor" number="7.7.5">
<h3><span class="header-section-number">7.7.5</span> Modeling for Prediction<a href="predictive-modeling.html#modeling-for-prediction-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Goal is to make the most accurate predictions possible.<br />
</li>
<li>Not concerned with understanding relationships between variables. Not worried model being to complicated to interpret, as long as it yields good predictions.<br />
</li>
<li>Aim for a model that best captures the signal in the data, without being thrown off by noise.<br />
- Large number of predictors is ok<br />
- Donâ€™t make model so complicated that it overfits the data.<br />
</li>
<li>Be sure that model is predicting what you intend it to<br />
</li>
<li>Reflective of biases inherent in the data on which it was trained</li>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="building-models-for-interpretation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="logistic-regression-and-classification.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/Stat255-LU/Notes/edit/master/07-Predictive_Modeling.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/Stat255-LU/Notes/blob/master/07-Predictive_Modeling.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
